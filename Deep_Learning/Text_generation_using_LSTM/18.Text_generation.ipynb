{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
      "120000 120000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "from agi_news import AGI_News\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "nepochs = 20\n",
    "batch_size = 32\n",
    "max_len = 128\n",
    "data_set_root = \"datasets\"\n",
    "\n",
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"AG_NEWS\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "dataset_train = AG_NEWS(root=data_set_root, split=\"train\")\n",
    "print(next(iter(dataset_train)))\n",
    "\n",
    "\n",
    "main_dir = os.getcwd()\n",
    "\n",
    "# Change the working directory to the main directory\n",
    "os.chdir(main_dir)\n",
    "\n",
    "train_data = AGI_News(data_set_root, data_split=\"train\")\n",
    "test_data = AGI_News(data_set_root, data_split=\"test\")\n",
    "\n",
    "print(len(train_data), len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=datasets/datasets/AG_NEWS/data.txt --model_prefix=spm_ag_news --vocab_size=20000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: datasets/datasets/AG_NEWS/data.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_ag_news\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: datasets/datasets/AG_NEWS/data.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 120000 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=28222889\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9756% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999756\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 120000 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 152602 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 120000\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 120631\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 120631 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=62443 obj=11.2717 num_tokens=276733 num_tokens/piece=4.43177\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=49790 obj=9.0969 num_tokens=277388 num_tokens/piece=5.57116\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=37337 obj=9.0515 num_tokens=288412 num_tokens/piece=7.72456\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=37324 obj=9.03806 num_tokens=288579 num_tokens/piece=7.73173\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=27993 obj=9.07602 num_tokens=309208 num_tokens/piece=11.0459\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=27993 obj=9.06489 num_tokens=309177 num_tokens/piece=11.0448\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22000 obj=9.12194 num_tokens=328649 num_tokens/piece=14.9386\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=21999 obj=9.10707 num_tokens=328625 num_tokens/piece=14.9382\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: spm_ag_news.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: spm_ag_news.vocab\n"
     ]
    }
   ],
   "source": [
    "# Un-Comment to train sentence-piece model for tokenizer and vocab!\n",
    "\n",
    "from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "with open(os.path.join(data_set_root, \"datasets/AG_NEWS/train.csv\")) as f:\n",
    "    with open(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \"w\") as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            text_only = \"\".join(line.split(\",\")[1:])\n",
    "            filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "            f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "generate_sp_model(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \n",
    "                  vocab_size=20000, model_prefix='spm_ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "# DataLoader for testing dataset\n",
    "data_loader_test = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁i', '▁am', '▁creat', 'ing']\n"
     ]
    }
   ],
   "source": [
    "sp_model = load_sp_model(\"spm_ag_news.model\")\n",
    "\n",
    "# Create a tokenizer using the loaded model\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "# Iterate over tokens generated by the tokenizer\n",
    "for token in tokenizer([\"i am creating\"]):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line.split(\"\\t\")[0]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(\"spm_ag_news.vocab\"),\n",
    "    # Define special tokens with special_first=True to place them at the beginning of the vocabulary\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set default index for out-of-vocabulary tokens\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    def __init__(self, prob =0.1, pad_token = 0, num_special =4):\n",
    "\n",
    "        self.prob = prob\n",
    "        self.pad_token = pad_token\n",
    "        self.num_special = num_special\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        mask = torch.bernoulli(self.prob*torch.ones_like(sample)).long()\n",
    "\n",
    "        can_drop = (sample>= self.num_special).long()\n",
    "\n",
    "        mask = mask*can_drop\n",
    "\n",
    "        replace_with = (self.pad_token*torch.ones_like(sample)).long()\n",
    "\n",
    "        sample_out = (1-mask) * sample +mak *replace_with\n",
    "\n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentence if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    # Add <eos> token at the end of each sentence (index 2 in vocabulary)\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "\n",
    "# Define a transformation pipeline for generation (without truncation)\n",
    "gen_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\n",
      "after arafat : in his press conference last thursday, president bush said people who don #39;t believe in the applicability of democracy to the arab world cannot really believe in a two-state solution to the palestinian conflict with israel.\n",
      "Tokens\n",
      "['<sos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 't', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '-', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "text = next(iter(data_loader_train))\n",
    "\n",
    "index =0\n",
    "input_tokens = train_transform(text)\n",
    "print(\"Sentence\")\n",
    "print(text[index])\n",
    "print(\"Tokens\")\n",
    "print(vocab.lookup_tokens(input_tokens[index].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens back to Sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>t<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>-<unk><unk><unk><unk><unk><unk><unk><unk>.<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tokens back to Sentence\")\n",
    "\n",
    "pred_text = \"\".join(vocab.lookup_tokens(input_tokens[index].numpy()))\n",
    "pred_text.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, num_layers, emb_size = 128, hidden_size = 128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_emb, emb_size)\n",
    "\n",
    "        self.mlp_emb = nn.Sequential(nn.Linear(emb_size, emb_size),\n",
    "                                        nn.LayerNorm(emb_size),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Linear(emb_size, emb_size))\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = emb_size, hidden_size = hidden_size,\n",
    "        num_layers = num_layers, batch_first = True, dropout = 0.1)\n",
    "\n",
    "        self.mlp_out = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n",
    "        nn.LayerNorm(hidden_size//2),\n",
    "        nn.ELU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(hidden_size//2, num_emb))\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        input_emb = self.embedding(input_seq)\n",
    "        input_embs = self.mlp_emb(input_embs)\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "        return self.mlp_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "emb_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "lstm_generator = LSTM(num_emb= len(vocab), num_layers = num_layers,\n",
    "emb_size = emb_size, hidden_size = hidden_size).to(device)\n",
    "\n",
    "optimizer = optim.Adam(lstm_generator.parameters(), lr = learning_rate, weight_decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.1)\n",
    "\n",
    "# List to store training loss during each epoch\n",
    "training_loss_logger = []\n",
    "\n",
    "# List to store entropy during training (for monitoring)\n",
    "entropy_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 14347318 (Approximately 14 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "num_model_params = 0\n",
    "for param in lstm_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8063cc51aa2a4447b7948ff19cb26765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f202c61c25e4d6199e9722f863a64d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'bernouli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text_tokens \u001b[39m=\u001b[39m train_transform(\u001b[39mlist\u001b[39m(text))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m bs \u001b[39m=\u001b[39m text_tokens\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m input_text \u001b[39m=\u001b[39m td(text_tokens[:,\u001b[39m0\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m output_text \u001b[39m=\u001b[39m (text_tokens[:,\u001b[39m1\u001b[39m:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m hidden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(num_layers, bs, hidden_size, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[1;32m/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, sample):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbernouli(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mones_like(sample))\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     can_drop \u001b[39m=\u001b[39m (sample\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_special)\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshayarravarapu/Documents/GitHub/Machine_Learning/Deep_Learning/Text_generation_using_LSTM/18.Text_generation.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     mask \u001b[39m=\u001b[39m mask\u001b[39m*\u001b[39mcan_drop\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/__init__.py:1938\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mimportlib\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m__name__\u001b[39m)\n\u001b[0;32m-> 1938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'bernouli'"
     ]
    }
   ],
   "source": [
    "for epoch in trange(0,nepochs, leave = False, desc = \"Epoch\"):\n",
    "    lstm_generator.train()\n",
    "    steps =0\n",
    "\n",
    "    for text in tqdm(data_loader_train, desc =\"Training\", leave = False):\n",
    "        text_tokens = train_transform(list(text)).to(device)\n",
    "\n",
    "        bs = text_tokens.shape[0]\n",
    "\n",
    "        input_text = td(text_tokens[:,0:-1])\n",
    "        output_text = (text_tokens[:,1:])\n",
    "\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        \n",
    "        # Forward pass through the LSTM generator\n",
    "        pred, hidden, memory = lstm_generator(input_text, hidden, memory)\n",
    "\n",
    "\n",
    "        loss = loss_fn(pred.transpose(1, 2), output_text)\n",
    "        \n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        # Log entropy during training (for monitoring)\n",
    "        with torch.no_grad():\n",
    "            dist = Categorical(logits=pred)\n",
    "            entropy_logger.append(dist.entropy().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
