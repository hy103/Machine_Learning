{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torchvision.transforms as trsansforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 \n",
    "epochs = 2\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                        train = True,\n",
    "                                        transform=trsansforms.ToTensor(),\n",
    "                                        download = True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                        train = False,\n",
    "                                        transform=trsansforms.ToTensor(),\n",
    "                                        download = True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuralnet(\n",
      "  (layer1): Linear(in_features=784, out_features=500, bias=True)\n",
      "  (Relu): ReLU()\n",
      "  (layer2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Model definition\n",
    "class Neuralnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Neuralnet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Neuralnet(input_size, hidden_size, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 , Step 1/600, Loss : 2.3101\n",
      "Epoch 1/2 , Step 2/600, Loss : 2.2265\n",
      "Epoch 1/2 , Step 3/600, Loss : 2.1364\n",
      "Epoch 1/2 , Step 4/600, Loss : 2.0386\n",
      "Epoch 1/2 , Step 5/600, Loss : 1.8903\n",
      "Epoch 1/2 , Step 6/600, Loss : 1.8770\n",
      "Epoch 1/2 , Step 7/600, Loss : 1.8283\n",
      "Epoch 1/2 , Step 8/600, Loss : 1.6356\n",
      "Epoch 1/2 , Step 9/600, Loss : 1.5979\n",
      "Epoch 1/2 , Step 10/600, Loss : 1.4006\n",
      "Epoch 1/2 , Step 11/600, Loss : 1.3493\n",
      "Epoch 1/2 , Step 12/600, Loss : 1.2594\n",
      "Epoch 1/2 , Step 13/600, Loss : 1.1910\n",
      "Epoch 1/2 , Step 14/600, Loss : 1.2110\n",
      "Epoch 1/2 , Step 15/600, Loss : 1.1386\n",
      "Epoch 1/2 , Step 16/600, Loss : 1.0107\n",
      "Epoch 1/2 , Step 17/600, Loss : 1.0028\n",
      "Epoch 1/2 , Step 18/600, Loss : 0.8886\n",
      "Epoch 1/2 , Step 19/600, Loss : 0.8525\n",
      "Epoch 1/2 , Step 20/600, Loss : 0.8353\n",
      "Epoch 1/2 , Step 21/600, Loss : 0.7662\n",
      "Epoch 1/2 , Step 22/600, Loss : 0.7378\n",
      "Epoch 1/2 , Step 23/600, Loss : 0.6958\n",
      "Epoch 1/2 , Step 24/600, Loss : 0.6170\n",
      "Epoch 1/2 , Step 25/600, Loss : 0.6807\n",
      "Epoch 1/2 , Step 26/600, Loss : 0.5697\n",
      "Epoch 1/2 , Step 27/600, Loss : 0.5385\n",
      "Epoch 1/2 , Step 28/600, Loss : 0.6593\n",
      "Epoch 1/2 , Step 29/600, Loss : 0.6728\n",
      "Epoch 1/2 , Step 30/600, Loss : 0.6317\n",
      "Epoch 1/2 , Step 31/600, Loss : 0.5368\n",
      "Epoch 1/2 , Step 32/600, Loss : 0.6139\n",
      "Epoch 1/2 , Step 33/600, Loss : 0.5950\n",
      "Epoch 1/2 , Step 34/600, Loss : 0.4870\n",
      "Epoch 1/2 , Step 35/600, Loss : 0.4350\n",
      "Epoch 1/2 , Step 36/600, Loss : 0.4630\n",
      "Epoch 1/2 , Step 37/600, Loss : 0.5106\n",
      "Epoch 1/2 , Step 38/600, Loss : 0.5072\n",
      "Epoch 1/2 , Step 39/600, Loss : 0.4365\n",
      "Epoch 1/2 , Step 40/600, Loss : 0.4759\n",
      "Epoch 1/2 , Step 41/600, Loss : 0.4976\n",
      "Epoch 1/2 , Step 42/600, Loss : 0.5270\n",
      "Epoch 1/2 , Step 43/600, Loss : 0.6090\n",
      "Epoch 1/2 , Step 44/600, Loss : 0.4513\n",
      "Epoch 1/2 , Step 45/600, Loss : 0.4689\n",
      "Epoch 1/2 , Step 46/600, Loss : 0.3601\n",
      "Epoch 1/2 , Step 47/600, Loss : 0.5136\n",
      "Epoch 1/2 , Step 48/600, Loss : 0.3938\n",
      "Epoch 1/2 , Step 49/600, Loss : 0.3907\n",
      "Epoch 1/2 , Step 50/600, Loss : 0.5317\n",
      "Epoch 1/2 , Step 51/600, Loss : 0.4665\n",
      "Epoch 1/2 , Step 52/600, Loss : 0.4160\n",
      "Epoch 1/2 , Step 53/600, Loss : 0.4531\n",
      "Epoch 1/2 , Step 54/600, Loss : 0.4499\n",
      "Epoch 1/2 , Step 55/600, Loss : 0.3598\n",
      "Epoch 1/2 , Step 56/600, Loss : 0.4610\n",
      "Epoch 1/2 , Step 57/600, Loss : 0.3025\n",
      "Epoch 1/2 , Step 58/600, Loss : 0.4359\n",
      "Epoch 1/2 , Step 59/600, Loss : 0.4651\n",
      "Epoch 1/2 , Step 60/600, Loss : 0.2892\n",
      "Epoch 1/2 , Step 61/600, Loss : 0.4774\n",
      "Epoch 1/2 , Step 62/600, Loss : 0.3657\n",
      "Epoch 1/2 , Step 63/600, Loss : 0.3479\n",
      "Epoch 1/2 , Step 64/600, Loss : 0.5984\n",
      "Epoch 1/2 , Step 65/600, Loss : 0.2868\n",
      "Epoch 1/2 , Step 66/600, Loss : 0.4498\n",
      "Epoch 1/2 , Step 67/600, Loss : 0.4050\n",
      "Epoch 1/2 , Step 68/600, Loss : 0.3695\n",
      "Epoch 1/2 , Step 69/600, Loss : 0.3626\n",
      "Epoch 1/2 , Step 70/600, Loss : 0.4770\n",
      "Epoch 1/2 , Step 71/600, Loss : 0.3751\n",
      "Epoch 1/2 , Step 72/600, Loss : 0.3916\n",
      "Epoch 1/2 , Step 73/600, Loss : 0.4085\n",
      "Epoch 1/2 , Step 74/600, Loss : 0.3038\n",
      "Epoch 1/2 , Step 75/600, Loss : 0.4037\n",
      "Epoch 1/2 , Step 76/600, Loss : 0.2751\n",
      "Epoch 1/2 , Step 77/600, Loss : 0.1873\n",
      "Epoch 1/2 , Step 78/600, Loss : 0.4283\n",
      "Epoch 1/2 , Step 79/600, Loss : 0.3036\n",
      "Epoch 1/2 , Step 80/600, Loss : 0.3542\n",
      "Epoch 1/2 , Step 81/600, Loss : 0.3958\n",
      "Epoch 1/2 , Step 82/600, Loss : 0.3660\n",
      "Epoch 1/2 , Step 83/600, Loss : 0.2554\n",
      "Epoch 1/2 , Step 84/600, Loss : 0.2895\n",
      "Epoch 1/2 , Step 85/600, Loss : 0.2688\n",
      "Epoch 1/2 , Step 86/600, Loss : 0.3577\n",
      "Epoch 1/2 , Step 87/600, Loss : 0.3405\n",
      "Epoch 1/2 , Step 88/600, Loss : 0.5237\n",
      "Epoch 1/2 , Step 89/600, Loss : 0.3944\n",
      "Epoch 1/2 , Step 90/600, Loss : 0.4090\n",
      "Epoch 1/2 , Step 91/600, Loss : 0.2987\n",
      "Epoch 1/2 , Step 92/600, Loss : 0.2814\n",
      "Epoch 1/2 , Step 93/600, Loss : 0.2210\n",
      "Epoch 1/2 , Step 94/600, Loss : 0.3473\n",
      "Epoch 1/2 , Step 95/600, Loss : 0.3299\n",
      "Epoch 1/2 , Step 96/600, Loss : 0.3929\n",
      "Epoch 1/2 , Step 97/600, Loss : 0.2495\n",
      "Epoch 1/2 , Step 98/600, Loss : 0.3096\n",
      "Epoch 1/2 , Step 99/600, Loss : 0.4738\n",
      "Epoch 1/2 , Step 100/600, Loss : 0.2843\n",
      "Epoch 1/2 , Step 101/600, Loss : 0.4363\n",
      "Epoch 1/2 , Step 102/600, Loss : 0.4561\n",
      "Epoch 1/2 , Step 103/600, Loss : 0.2727\n",
      "Epoch 1/2 , Step 104/600, Loss : 0.4847\n",
      "Epoch 1/2 , Step 105/600, Loss : 0.4443\n",
      "Epoch 1/2 , Step 106/600, Loss : 0.2579\n",
      "Epoch 1/2 , Step 107/600, Loss : 0.2586\n",
      "Epoch 1/2 , Step 108/600, Loss : 0.1969\n",
      "Epoch 1/2 , Step 109/600, Loss : 0.3998\n",
      "Epoch 1/2 , Step 110/600, Loss : 0.3347\n",
      "Epoch 1/2 , Step 111/600, Loss : 0.3905\n",
      "Epoch 1/2 , Step 112/600, Loss : 0.4221\n",
      "Epoch 1/2 , Step 113/600, Loss : 0.2190\n",
      "Epoch 1/2 , Step 114/600, Loss : 0.2795\n",
      "Epoch 1/2 , Step 115/600, Loss : 0.2637\n",
      "Epoch 1/2 , Step 116/600, Loss : 0.4506\n",
      "Epoch 1/2 , Step 117/600, Loss : 0.3698\n",
      "Epoch 1/2 , Step 118/600, Loss : 0.4385\n",
      "Epoch 1/2 , Step 119/600, Loss : 0.2294\n",
      "Epoch 1/2 , Step 120/600, Loss : 0.3593\n",
      "Epoch 1/2 , Step 121/600, Loss : 0.4098\n",
      "Epoch 1/2 , Step 122/600, Loss : 0.2905\n",
      "Epoch 1/2 , Step 123/600, Loss : 0.4263\n",
      "Epoch 1/2 , Step 124/600, Loss : 0.3418\n",
      "Epoch 1/2 , Step 125/600, Loss : 0.3659\n",
      "Epoch 1/2 , Step 126/600, Loss : 0.2781\n",
      "Epoch 1/2 , Step 127/600, Loss : 0.3344\n",
      "Epoch 1/2 , Step 128/600, Loss : 0.2333\n",
      "Epoch 1/2 , Step 129/600, Loss : 0.1987\n",
      "Epoch 1/2 , Step 130/600, Loss : 0.4684\n",
      "Epoch 1/2 , Step 131/600, Loss : 0.3551\n",
      "Epoch 1/2 , Step 132/600, Loss : 0.2504\n",
      "Epoch 1/2 , Step 133/600, Loss : 0.3673\n",
      "Epoch 1/2 , Step 134/600, Loss : 0.2386\n",
      "Epoch 1/2 , Step 135/600, Loss : 0.3329\n",
      "Epoch 1/2 , Step 136/600, Loss : 0.4128\n",
      "Epoch 1/2 , Step 137/600, Loss : 0.3129\n",
      "Epoch 1/2 , Step 138/600, Loss : 0.2730\n",
      "Epoch 1/2 , Step 139/600, Loss : 0.2659\n",
      "Epoch 1/2 , Step 140/600, Loss : 0.4232\n",
      "Epoch 1/2 , Step 141/600, Loss : 0.4235\n",
      "Epoch 1/2 , Step 142/600, Loss : 0.3307\n",
      "Epoch 1/2 , Step 143/600, Loss : 0.2675\n",
      "Epoch 1/2 , Step 144/600, Loss : 0.2489\n",
      "Epoch 1/2 , Step 145/600, Loss : 0.3097\n",
      "Epoch 1/2 , Step 146/600, Loss : 0.2480\n",
      "Epoch 1/2 , Step 147/600, Loss : 0.3934\n",
      "Epoch 1/2 , Step 148/600, Loss : 0.2470\n",
      "Epoch 1/2 , Step 149/600, Loss : 0.2141\n",
      "Epoch 1/2 , Step 150/600, Loss : 0.3507\n",
      "Epoch 1/2 , Step 151/600, Loss : 0.5140\n",
      "Epoch 1/2 , Step 152/600, Loss : 0.2692\n",
      "Epoch 1/2 , Step 153/600, Loss : 0.1964\n",
      "Epoch 1/2 , Step 154/600, Loss : 0.2759\n",
      "Epoch 1/2 , Step 155/600, Loss : 0.2150\n",
      "Epoch 1/2 , Step 156/600, Loss : 0.2857\n",
      "Epoch 1/2 , Step 157/600, Loss : 0.1922\n",
      "Epoch 1/2 , Step 158/600, Loss : 0.4518\n",
      "Epoch 1/2 , Step 159/600, Loss : 0.3200\n",
      "Epoch 1/2 , Step 160/600, Loss : 0.2034\n",
      "Epoch 1/2 , Step 161/600, Loss : 0.3122\n",
      "Epoch 1/2 , Step 162/600, Loss : 0.1809\n",
      "Epoch 1/2 , Step 163/600, Loss : 0.2094\n",
      "Epoch 1/2 , Step 164/600, Loss : 0.2685\n",
      "Epoch 1/2 , Step 165/600, Loss : 0.1950\n",
      "Epoch 1/2 , Step 166/600, Loss : 0.2928\n",
      "Epoch 1/2 , Step 167/600, Loss : 0.3095\n",
      "Epoch 1/2 , Step 168/600, Loss : 0.1639\n",
      "Epoch 1/2 , Step 169/600, Loss : 0.2607\n",
      "Epoch 1/2 , Step 170/600, Loss : 0.2862\n",
      "Epoch 1/2 , Step 171/600, Loss : 0.3644\n",
      "Epoch 1/2 , Step 172/600, Loss : 0.1897\n",
      "Epoch 1/2 , Step 173/600, Loss : 0.2465\n",
      "Epoch 1/2 , Step 174/600, Loss : 0.2242\n",
      "Epoch 1/2 , Step 175/600, Loss : 0.3174\n",
      "Epoch 1/2 , Step 176/600, Loss : 0.2778\n",
      "Epoch 1/2 , Step 177/600, Loss : 0.3018\n",
      "Epoch 1/2 , Step 178/600, Loss : 0.4740\n",
      "Epoch 1/2 , Step 179/600, Loss : 0.4445\n",
      "Epoch 1/2 , Step 180/600, Loss : 0.1439\n",
      "Epoch 1/2 , Step 181/600, Loss : 0.2715\n",
      "Epoch 1/2 , Step 182/600, Loss : 0.2163\n",
      "Epoch 1/2 , Step 183/600, Loss : 0.3203\n",
      "Epoch 1/2 , Step 184/600, Loss : 0.2556\n",
      "Epoch 1/2 , Step 185/600, Loss : 0.1919\n",
      "Epoch 1/2 , Step 186/600, Loss : 0.3403\n",
      "Epoch 1/2 , Step 187/600, Loss : 0.2443\n",
      "Epoch 1/2 , Step 188/600, Loss : 0.3140\n",
      "Epoch 1/2 , Step 189/600, Loss : 0.3845\n",
      "Epoch 1/2 , Step 190/600, Loss : 0.2166\n",
      "Epoch 1/2 , Step 191/600, Loss : 0.3184\n",
      "Epoch 1/2 , Step 192/600, Loss : 0.2125\n",
      "Epoch 1/2 , Step 193/600, Loss : 0.3043\n",
      "Epoch 1/2 , Step 194/600, Loss : 0.2417\n",
      "Epoch 1/2 , Step 195/600, Loss : 0.2673\n",
      "Epoch 1/2 , Step 196/600, Loss : 0.3235\n",
      "Epoch 1/2 , Step 197/600, Loss : 0.2448\n",
      "Epoch 1/2 , Step 198/600, Loss : 0.2849\n",
      "Epoch 1/2 , Step 199/600, Loss : 0.2212\n",
      "Epoch 1/2 , Step 200/600, Loss : 0.2544\n",
      "Epoch 1/2 , Step 201/600, Loss : 0.1830\n",
      "Epoch 1/2 , Step 202/600, Loss : 0.3532\n",
      "Epoch 1/2 , Step 203/600, Loss : 0.1885\n",
      "Epoch 1/2 , Step 204/600, Loss : 0.2894\n",
      "Epoch 1/2 , Step 205/600, Loss : 0.2641\n",
      "Epoch 1/2 , Step 206/600, Loss : 0.3449\n",
      "Epoch 1/2 , Step 207/600, Loss : 0.2462\n",
      "Epoch 1/2 , Step 208/600, Loss : 0.1384\n",
      "Epoch 1/2 , Step 209/600, Loss : 0.1913\n",
      "Epoch 1/2 , Step 210/600, Loss : 0.2551\n",
      "Epoch 1/2 , Step 211/600, Loss : 0.3026\n",
      "Epoch 1/2 , Step 212/600, Loss : 0.2603\n",
      "Epoch 1/2 , Step 213/600, Loss : 0.3212\n",
      "Epoch 1/2 , Step 214/600, Loss : 0.2636\n",
      "Epoch 1/2 , Step 215/600, Loss : 0.1054\n",
      "Epoch 1/2 , Step 216/600, Loss : 0.1746\n",
      "Epoch 1/2 , Step 217/600, Loss : 0.2210\n",
      "Epoch 1/2 , Step 218/600, Loss : 0.4643\n",
      "Epoch 1/2 , Step 219/600, Loss : 0.2856\n",
      "Epoch 1/2 , Step 220/600, Loss : 0.2551\n",
      "Epoch 1/2 , Step 221/600, Loss : 0.2269\n",
      "Epoch 1/2 , Step 222/600, Loss : 0.2893\n",
      "Epoch 1/2 , Step 223/600, Loss : 0.2525\n",
      "Epoch 1/2 , Step 224/600, Loss : 0.1318\n",
      "Epoch 1/2 , Step 225/600, Loss : 0.2701\n",
      "Epoch 1/2 , Step 226/600, Loss : 0.1919\n",
      "Epoch 1/2 , Step 227/600, Loss : 0.3284\n",
      "Epoch 1/2 , Step 228/600, Loss : 0.2915\n",
      "Epoch 1/2 , Step 229/600, Loss : 0.4706\n",
      "Epoch 1/2 , Step 230/600, Loss : 0.3496\n",
      "Epoch 1/2 , Step 231/600, Loss : 0.3094\n",
      "Epoch 1/2 , Step 232/600, Loss : 0.3289\n",
      "Epoch 1/2 , Step 233/600, Loss : 0.1335\n",
      "Epoch 1/2 , Step 234/600, Loss : 0.2085\n",
      "Epoch 1/2 , Step 235/600, Loss : 0.2554\n",
      "Epoch 1/2 , Step 236/600, Loss : 0.2002\n",
      "Epoch 1/2 , Step 237/600, Loss : 0.3795\n",
      "Epoch 1/2 , Step 238/600, Loss : 0.2748\n",
      "Epoch 1/2 , Step 239/600, Loss : 0.2444\n",
      "Epoch 1/2 , Step 240/600, Loss : 0.2156\n",
      "Epoch 1/2 , Step 241/600, Loss : 0.2017\n",
      "Epoch 1/2 , Step 242/600, Loss : 0.1715\n",
      "Epoch 1/2 , Step 243/600, Loss : 0.1974\n",
      "Epoch 1/2 , Step 244/600, Loss : 0.3198\n",
      "Epoch 1/2 , Step 245/600, Loss : 0.2733\n",
      "Epoch 1/2 , Step 246/600, Loss : 0.2874\n",
      "Epoch 1/2 , Step 247/600, Loss : 0.1543\n",
      "Epoch 1/2 , Step 248/600, Loss : 0.3480\n",
      "Epoch 1/2 , Step 249/600, Loss : 0.2488\n",
      "Epoch 1/2 , Step 250/600, Loss : 0.1470\n",
      "Epoch 1/2 , Step 251/600, Loss : 0.1608\n",
      "Epoch 1/2 , Step 252/600, Loss : 0.2609\n",
      "Epoch 1/2 , Step 253/600, Loss : 0.3786\n",
      "Epoch 1/2 , Step 254/600, Loss : 0.1779\n",
      "Epoch 1/2 , Step 255/600, Loss : 0.2177\n",
      "Epoch 1/2 , Step 256/600, Loss : 0.2570\n",
      "Epoch 1/2 , Step 257/600, Loss : 0.2698\n",
      "Epoch 1/2 , Step 258/600, Loss : 0.1652\n",
      "Epoch 1/2 , Step 259/600, Loss : 0.1911\n",
      "Epoch 1/2 , Step 260/600, Loss : 0.1927\n",
      "Epoch 1/2 , Step 261/600, Loss : 0.4261\n",
      "Epoch 1/2 , Step 262/600, Loss : 0.2555\n",
      "Epoch 1/2 , Step 263/600, Loss : 0.2056\n",
      "Epoch 1/2 , Step 264/600, Loss : 0.1817\n",
      "Epoch 1/2 , Step 265/600, Loss : 0.3328\n",
      "Epoch 1/2 , Step 266/600, Loss : 0.1341\n",
      "Epoch 1/2 , Step 267/600, Loss : 0.1560\n",
      "Epoch 1/2 , Step 268/600, Loss : 0.1839\n",
      "Epoch 1/2 , Step 269/600, Loss : 0.3021\n",
      "Epoch 1/2 , Step 270/600, Loss : 0.2600\n",
      "Epoch 1/2 , Step 271/600, Loss : 0.2903\n",
      "Epoch 1/2 , Step 272/600, Loss : 0.1198\n",
      "Epoch 1/2 , Step 273/600, Loss : 0.1616\n",
      "Epoch 1/2 , Step 274/600, Loss : 0.2861\n",
      "Epoch 1/2 , Step 275/600, Loss : 0.2897\n",
      "Epoch 1/2 , Step 276/600, Loss : 0.3492\n",
      "Epoch 1/2 , Step 277/600, Loss : 0.2694\n",
      "Epoch 1/2 , Step 278/600, Loss : 0.1575\n",
      "Epoch 1/2 , Step 279/600, Loss : 0.2686\n",
      "Epoch 1/2 , Step 280/600, Loss : 0.2541\n",
      "Epoch 1/2 , Step 281/600, Loss : 0.3358\n",
      "Epoch 1/2 , Step 282/600, Loss : 0.2785\n",
      "Epoch 1/2 , Step 283/600, Loss : 0.2090\n",
      "Epoch 1/2 , Step 284/600, Loss : 0.2179\n",
      "Epoch 1/2 , Step 285/600, Loss : 0.3504\n",
      "Epoch 1/2 , Step 286/600, Loss : 0.2095\n",
      "Epoch 1/2 , Step 287/600, Loss : 0.1405\n",
      "Epoch 1/2 , Step 288/600, Loss : 0.2695\n",
      "Epoch 1/2 , Step 289/600, Loss : 0.1342\n",
      "Epoch 1/2 , Step 290/600, Loss : 0.2407\n",
      "Epoch 1/2 , Step 291/600, Loss : 0.1352\n",
      "Epoch 1/2 , Step 292/600, Loss : 0.4248\n",
      "Epoch 1/2 , Step 293/600, Loss : 0.4042\n",
      "Epoch 1/2 , Step 294/600, Loss : 0.0677\n",
      "Epoch 1/2 , Step 295/600, Loss : 0.1700\n",
      "Epoch 1/2 , Step 296/600, Loss : 0.2028\n",
      "Epoch 1/2 , Step 297/600, Loss : 0.1660\n",
      "Epoch 1/2 , Step 298/600, Loss : 0.1500\n",
      "Epoch 1/2 , Step 299/600, Loss : 0.3023\n",
      "Epoch 1/2 , Step 300/600, Loss : 0.2190\n",
      "Epoch 1/2 , Step 301/600, Loss : 0.1296\n",
      "Epoch 1/2 , Step 302/600, Loss : 0.2511\n",
      "Epoch 1/2 , Step 303/600, Loss : 0.2296\n",
      "Epoch 1/2 , Step 304/600, Loss : 0.3773\n",
      "Epoch 1/2 , Step 305/600, Loss : 0.2441\n",
      "Epoch 1/2 , Step 306/600, Loss : 0.2371\n",
      "Epoch 1/2 , Step 307/600, Loss : 0.2910\n",
      "Epoch 1/2 , Step 308/600, Loss : 0.2664\n",
      "Epoch 1/2 , Step 309/600, Loss : 0.3242\n",
      "Epoch 1/2 , Step 310/600, Loss : 0.2321\n",
      "Epoch 1/2 , Step 311/600, Loss : 0.2274\n",
      "Epoch 1/2 , Step 312/600, Loss : 0.3204\n",
      "Epoch 1/2 , Step 313/600, Loss : 0.2179\n",
      "Epoch 1/2 , Step 314/600, Loss : 0.1947\n",
      "Epoch 1/2 , Step 315/600, Loss : 0.1299\n",
      "Epoch 1/2 , Step 316/600, Loss : 0.2681\n",
      "Epoch 1/2 , Step 317/600, Loss : 0.3021\n",
      "Epoch 1/2 , Step 318/600, Loss : 0.1316\n",
      "Epoch 1/2 , Step 319/600, Loss : 0.2892\n",
      "Epoch 1/2 , Step 320/600, Loss : 0.1719\n",
      "Epoch 1/2 , Step 321/600, Loss : 0.2439\n",
      "Epoch 1/2 , Step 322/600, Loss : 0.1606\n",
      "Epoch 1/2 , Step 323/600, Loss : 0.1789\n",
      "Epoch 1/2 , Step 324/600, Loss : 0.2128\n",
      "Epoch 1/2 , Step 325/600, Loss : 0.2561\n",
      "Epoch 1/2 , Step 326/600, Loss : 0.3110\n",
      "Epoch 1/2 , Step 327/600, Loss : 0.2244\n",
      "Epoch 1/2 , Step 328/600, Loss : 0.0998\n",
      "Epoch 1/2 , Step 329/600, Loss : 0.1991\n",
      "Epoch 1/2 , Step 330/600, Loss : 0.2093\n",
      "Epoch 1/2 , Step 331/600, Loss : 0.2192\n",
      "Epoch 1/2 , Step 332/600, Loss : 0.2894\n",
      "Epoch 1/2 , Step 333/600, Loss : 0.1253\n",
      "Epoch 1/2 , Step 334/600, Loss : 0.1930\n",
      "Epoch 1/2 , Step 335/600, Loss : 0.1000\n",
      "Epoch 1/2 , Step 336/600, Loss : 0.2948\n",
      "Epoch 1/2 , Step 337/600, Loss : 0.1686\n",
      "Epoch 1/2 , Step 338/600, Loss : 0.1663\n",
      "Epoch 1/2 , Step 339/600, Loss : 0.2126\n",
      "Epoch 1/2 , Step 340/600, Loss : 0.3538\n",
      "Epoch 1/2 , Step 341/600, Loss : 0.2025\n",
      "Epoch 1/2 , Step 342/600, Loss : 0.2359\n",
      "Epoch 1/2 , Step 343/600, Loss : 0.2908\n",
      "Epoch 1/2 , Step 344/600, Loss : 0.1571\n",
      "Epoch 1/2 , Step 345/600, Loss : 0.1218\n",
      "Epoch 1/2 , Step 346/600, Loss : 0.3424\n",
      "Epoch 1/2 , Step 347/600, Loss : 0.1610\n",
      "Epoch 1/2 , Step 348/600, Loss : 0.1137\n",
      "Epoch 1/2 , Step 349/600, Loss : 0.0669\n",
      "Epoch 1/2 , Step 350/600, Loss : 0.2045\n",
      "Epoch 1/2 , Step 351/600, Loss : 0.3232\n",
      "Epoch 1/2 , Step 352/600, Loss : 0.1877\n",
      "Epoch 1/2 , Step 353/600, Loss : 0.3447\n",
      "Epoch 1/2 , Step 354/600, Loss : 0.1542\n",
      "Epoch 1/2 , Step 355/600, Loss : 0.0983\n",
      "Epoch 1/2 , Step 356/600, Loss : 0.1264\n",
      "Epoch 1/2 , Step 357/600, Loss : 0.2785\n",
      "Epoch 1/2 , Step 358/600, Loss : 0.0988\n",
      "Epoch 1/2 , Step 359/600, Loss : 0.2729\n",
      "Epoch 1/2 , Step 360/600, Loss : 0.3141\n",
      "Epoch 1/2 , Step 361/600, Loss : 0.2094\n",
      "Epoch 1/2 , Step 362/600, Loss : 0.3704\n",
      "Epoch 1/2 , Step 363/600, Loss : 0.1894\n",
      "Epoch 1/2 , Step 364/600, Loss : 0.3113\n",
      "Epoch 1/2 , Step 365/600, Loss : 0.1464\n",
      "Epoch 1/2 , Step 366/600, Loss : 0.2190\n",
      "Epoch 1/2 , Step 367/600, Loss : 0.2007\n",
      "Epoch 1/2 , Step 368/600, Loss : 0.1979\n",
      "Epoch 1/2 , Step 369/600, Loss : 0.1897\n",
      "Epoch 1/2 , Step 370/600, Loss : 0.2370\n",
      "Epoch 1/2 , Step 371/600, Loss : 0.2559\n",
      "Epoch 1/2 , Step 372/600, Loss : 0.2190\n",
      "Epoch 1/2 , Step 373/600, Loss : 0.1548\n",
      "Epoch 1/2 , Step 374/600, Loss : 0.2414\n",
      "Epoch 1/2 , Step 375/600, Loss : 0.1841\n",
      "Epoch 1/2 , Step 376/600, Loss : 0.2763\n",
      "Epoch 1/2 , Step 377/600, Loss : 0.1897\n",
      "Epoch 1/2 , Step 378/600, Loss : 0.1743\n",
      "Epoch 1/2 , Step 379/600, Loss : 0.2332\n",
      "Epoch 1/2 , Step 380/600, Loss : 0.2930\n",
      "Epoch 1/2 , Step 381/600, Loss : 0.3846\n",
      "Epoch 1/2 , Step 382/600, Loss : 0.2369\n",
      "Epoch 1/2 , Step 383/600, Loss : 0.1767\n",
      "Epoch 1/2 , Step 384/600, Loss : 0.1267\n",
      "Epoch 1/2 , Step 385/600, Loss : 0.1741\n",
      "Epoch 1/2 , Step 386/600, Loss : 0.1251\n",
      "Epoch 1/2 , Step 387/600, Loss : 0.2126\n",
      "Epoch 1/2 , Step 388/600, Loss : 0.2119\n",
      "Epoch 1/2 , Step 389/600, Loss : 0.2420\n",
      "Epoch 1/2 , Step 390/600, Loss : 0.2624\n",
      "Epoch 1/2 , Step 391/600, Loss : 0.1836\n",
      "Epoch 1/2 , Step 392/600, Loss : 0.2159\n",
      "Epoch 1/2 , Step 393/600, Loss : 0.3919\n",
      "Epoch 1/2 , Step 394/600, Loss : 0.1747\n",
      "Epoch 1/2 , Step 395/600, Loss : 0.1614\n",
      "Epoch 1/2 , Step 396/600, Loss : 0.1899\n",
      "Epoch 1/2 , Step 397/600, Loss : 0.1311\n",
      "Epoch 1/2 , Step 398/600, Loss : 0.2930\n",
      "Epoch 1/2 , Step 399/600, Loss : 0.1766\n",
      "Epoch 1/2 , Step 400/600, Loss : 0.2149\n",
      "Epoch 1/2 , Step 401/600, Loss : 0.1340\n",
      "Epoch 1/2 , Step 402/600, Loss : 0.3112\n",
      "Epoch 1/2 , Step 403/600, Loss : 0.1032\n",
      "Epoch 1/2 , Step 404/600, Loss : 0.2183\n",
      "Epoch 1/2 , Step 405/600, Loss : 0.1538\n",
      "Epoch 1/2 , Step 406/600, Loss : 0.1802\n",
      "Epoch 1/2 , Step 407/600, Loss : 0.1434\n",
      "Epoch 1/2 , Step 408/600, Loss : 0.2720\n",
      "Epoch 1/2 , Step 409/600, Loss : 0.2192\n",
      "Epoch 1/2 , Step 410/600, Loss : 0.3164\n",
      "Epoch 1/2 , Step 411/600, Loss : 0.1231\n",
      "Epoch 1/2 , Step 412/600, Loss : 0.1784\n",
      "Epoch 1/2 , Step 413/600, Loss : 0.1749\n",
      "Epoch 1/2 , Step 414/600, Loss : 0.1755\n",
      "Epoch 1/2 , Step 415/600, Loss : 0.1366\n",
      "Epoch 1/2 , Step 416/600, Loss : 0.2700\n",
      "Epoch 1/2 , Step 417/600, Loss : 0.1501\n",
      "Epoch 1/2 , Step 418/600, Loss : 0.2475\n",
      "Epoch 1/2 , Step 419/600, Loss : 0.1234\n",
      "Epoch 1/2 , Step 420/600, Loss : 0.1505\n",
      "Epoch 1/2 , Step 421/600, Loss : 0.3180\n",
      "Epoch 1/2 , Step 422/600, Loss : 0.1112\n",
      "Epoch 1/2 , Step 423/600, Loss : 0.1041\n",
      "Epoch 1/2 , Step 424/600, Loss : 0.1137\n",
      "Epoch 1/2 , Step 425/600, Loss : 0.1596\n",
      "Epoch 1/2 , Step 426/600, Loss : 0.2051\n",
      "Epoch 1/2 , Step 427/600, Loss : 0.3248\n",
      "Epoch 1/2 , Step 428/600, Loss : 0.1614\n",
      "Epoch 1/2 , Step 429/600, Loss : 0.1315\n",
      "Epoch 1/2 , Step 430/600, Loss : 0.0745\n",
      "Epoch 1/2 , Step 431/600, Loss : 0.1323\n",
      "Epoch 1/2 , Step 432/600, Loss : 0.1293\n",
      "Epoch 1/2 , Step 433/600, Loss : 0.2176\n",
      "Epoch 1/2 , Step 434/600, Loss : 0.1394\n",
      "Epoch 1/2 , Step 435/600, Loss : 0.2756\n",
      "Epoch 1/2 , Step 436/600, Loss : 0.2359\n",
      "Epoch 1/2 , Step 437/600, Loss : 0.1740\n",
      "Epoch 1/2 , Step 438/600, Loss : 0.0652\n",
      "Epoch 1/2 , Step 439/600, Loss : 0.1397\n",
      "Epoch 1/2 , Step 440/600, Loss : 0.4534\n",
      "Epoch 1/2 , Step 441/600, Loss : 0.1893\n",
      "Epoch 1/2 , Step 442/600, Loss : 0.1359\n",
      "Epoch 1/2 , Step 443/600, Loss : 0.1840\n",
      "Epoch 1/2 , Step 444/600, Loss : 0.2635\n",
      "Epoch 1/2 , Step 445/600, Loss : 0.2507\n",
      "Epoch 1/2 , Step 446/600, Loss : 0.2360\n",
      "Epoch 1/2 , Step 447/600, Loss : 0.2535\n",
      "Epoch 1/2 , Step 448/600, Loss : 0.2011\n",
      "Epoch 1/2 , Step 449/600, Loss : 0.1411\n",
      "Epoch 1/2 , Step 450/600, Loss : 0.1157\n",
      "Epoch 1/2 , Step 451/600, Loss : 0.1840\n",
      "Epoch 1/2 , Step 452/600, Loss : 0.1008\n",
      "Epoch 1/2 , Step 453/600, Loss : 0.1477\n",
      "Epoch 1/2 , Step 454/600, Loss : 0.2187\n",
      "Epoch 1/2 , Step 455/600, Loss : 0.3433\n",
      "Epoch 1/2 , Step 456/600, Loss : 0.2100\n",
      "Epoch 1/2 , Step 457/600, Loss : 0.2033\n",
      "Epoch 1/2 , Step 458/600, Loss : 0.1492\n",
      "Epoch 1/2 , Step 459/600, Loss : 0.1222\n",
      "Epoch 1/2 , Step 460/600, Loss : 0.2579\n",
      "Epoch 1/2 , Step 461/600, Loss : 0.1210\n",
      "Epoch 1/2 , Step 462/600, Loss : 0.1498\n",
      "Epoch 1/2 , Step 463/600, Loss : 0.1199\n",
      "Epoch 1/2 , Step 464/600, Loss : 0.4206\n",
      "Epoch 1/2 , Step 465/600, Loss : 0.1319\n",
      "Epoch 1/2 , Step 466/600, Loss : 0.1813\n",
      "Epoch 1/2 , Step 467/600, Loss : 0.1315\n",
      "Epoch 1/2 , Step 468/600, Loss : 0.0513\n",
      "Epoch 1/2 , Step 469/600, Loss : 0.2047\n",
      "Epoch 1/2 , Step 470/600, Loss : 0.2338\n",
      "Epoch 1/2 , Step 471/600, Loss : 0.0780\n",
      "Epoch 1/2 , Step 472/600, Loss : 0.1030\n",
      "Epoch 1/2 , Step 473/600, Loss : 0.1675\n",
      "Epoch 1/2 , Step 474/600, Loss : 0.1406\n",
      "Epoch 1/2 , Step 475/600, Loss : 0.1044\n",
      "Epoch 1/2 , Step 476/600, Loss : 0.1856\n",
      "Epoch 1/2 , Step 477/600, Loss : 0.2830\n",
      "Epoch 1/2 , Step 478/600, Loss : 0.1903\n",
      "Epoch 1/2 , Step 479/600, Loss : 0.1537\n",
      "Epoch 1/2 , Step 480/600, Loss : 0.2100\n",
      "Epoch 1/2 , Step 481/600, Loss : 0.2210\n",
      "Epoch 1/2 , Step 482/600, Loss : 0.2080\n",
      "Epoch 1/2 , Step 483/600, Loss : 0.1572\n",
      "Epoch 1/2 , Step 484/600, Loss : 0.1088\n",
      "Epoch 1/2 , Step 485/600, Loss : 0.0882\n",
      "Epoch 1/2 , Step 486/600, Loss : 0.2304\n",
      "Epoch 1/2 , Step 487/600, Loss : 0.1882\n",
      "Epoch 1/2 , Step 488/600, Loss : 0.1458\n",
      "Epoch 1/2 , Step 489/600, Loss : 0.2411\n",
      "Epoch 1/2 , Step 490/600, Loss : 0.1375\n",
      "Epoch 1/2 , Step 491/600, Loss : 0.1284\n",
      "Epoch 1/2 , Step 492/600, Loss : 0.1910\n",
      "Epoch 1/2 , Step 493/600, Loss : 0.2005\n",
      "Epoch 1/2 , Step 494/600, Loss : 0.0796\n",
      "Epoch 1/2 , Step 495/600, Loss : 0.2188\n",
      "Epoch 1/2 , Step 496/600, Loss : 0.1735\n",
      "Epoch 1/2 , Step 497/600, Loss : 0.2467\n",
      "Epoch 1/2 , Step 498/600, Loss : 0.1529\n",
      "Epoch 1/2 , Step 499/600, Loss : 0.1039\n",
      "Epoch 1/2 , Step 500/600, Loss : 0.1597\n",
      "Epoch 1/2 , Step 501/600, Loss : 0.1590\n",
      "Epoch 1/2 , Step 502/600, Loss : 0.2458\n",
      "Epoch 1/2 , Step 503/600, Loss : 0.1218\n",
      "Epoch 1/2 , Step 504/600, Loss : 0.1735\n",
      "Epoch 1/2 , Step 505/600, Loss : 0.1780\n",
      "Epoch 1/2 , Step 506/600, Loss : 0.1125\n",
      "Epoch 1/2 , Step 507/600, Loss : 0.1893\n",
      "Epoch 1/2 , Step 508/600, Loss : 0.2621\n",
      "Epoch 1/2 , Step 509/600, Loss : 0.1698\n",
      "Epoch 1/2 , Step 510/600, Loss : 0.1680\n",
      "Epoch 1/2 , Step 511/600, Loss : 0.1051\n",
      "Epoch 1/2 , Step 512/600, Loss : 0.0771\n",
      "Epoch 1/2 , Step 513/600, Loss : 0.3700\n",
      "Epoch 1/2 , Step 514/600, Loss : 0.1220\n",
      "Epoch 1/2 , Step 515/600, Loss : 0.1843\n",
      "Epoch 1/2 , Step 516/600, Loss : 0.1898\n",
      "Epoch 1/2 , Step 517/600, Loss : 0.2126\n",
      "Epoch 1/2 , Step 518/600, Loss : 0.1377\n",
      "Epoch 1/2 , Step 519/600, Loss : 0.1764\n",
      "Epoch 1/2 , Step 520/600, Loss : 0.1419\n",
      "Epoch 1/2 , Step 521/600, Loss : 0.2778\n",
      "Epoch 1/2 , Step 522/600, Loss : 0.1324\n",
      "Epoch 1/2 , Step 523/600, Loss : 0.1214\n",
      "Epoch 1/2 , Step 524/600, Loss : 0.3146\n",
      "Epoch 1/2 , Step 525/600, Loss : 0.1774\n",
      "Epoch 1/2 , Step 526/600, Loss : 0.1451\n",
      "Epoch 1/2 , Step 527/600, Loss : 0.1219\n",
      "Epoch 1/2 , Step 528/600, Loss : 0.2123\n",
      "Epoch 1/2 , Step 529/600, Loss : 0.0950\n",
      "Epoch 1/2 , Step 530/600, Loss : 0.1933\n",
      "Epoch 1/2 , Step 531/600, Loss : 0.2361\n",
      "Epoch 1/2 , Step 532/600, Loss : 0.1644\n",
      "Epoch 1/2 , Step 533/600, Loss : 0.2173\n",
      "Epoch 1/2 , Step 534/600, Loss : 0.2390\n",
      "Epoch 1/2 , Step 535/600, Loss : 0.1629\n",
      "Epoch 1/2 , Step 536/600, Loss : 0.1299\n",
      "Epoch 1/2 , Step 537/600, Loss : 0.1544\n",
      "Epoch 1/2 , Step 538/600, Loss : 0.1080\n",
      "Epoch 1/2 , Step 539/600, Loss : 0.1759\n",
      "Epoch 1/2 , Step 540/600, Loss : 0.1477\n",
      "Epoch 1/2 , Step 541/600, Loss : 0.1495\n",
      "Epoch 1/2 , Step 542/600, Loss : 0.1507\n",
      "Epoch 1/2 , Step 543/600, Loss : 0.1704\n",
      "Epoch 1/2 , Step 544/600, Loss : 0.2619\n",
      "Epoch 1/2 , Step 545/600, Loss : 0.0950\n",
      "Epoch 1/2 , Step 546/600, Loss : 0.1262\n",
      "Epoch 1/2 , Step 547/600, Loss : 0.1644\n",
      "Epoch 1/2 , Step 548/600, Loss : 0.1161\n",
      "Epoch 1/2 , Step 549/600, Loss : 0.2150\n",
      "Epoch 1/2 , Step 550/600, Loss : 0.2102\n",
      "Epoch 1/2 , Step 551/600, Loss : 0.2112\n",
      "Epoch 1/2 , Step 552/600, Loss : 0.0786\n",
      "Epoch 1/2 , Step 553/600, Loss : 0.2367\n",
      "Epoch 1/2 , Step 554/600, Loss : 0.2291\n",
      "Epoch 1/2 , Step 555/600, Loss : 0.1414\n",
      "Epoch 1/2 , Step 556/600, Loss : 0.1059\n",
      "Epoch 1/2 , Step 557/600, Loss : 0.2435\n",
      "Epoch 1/2 , Step 558/600, Loss : 0.0670\n",
      "Epoch 1/2 , Step 559/600, Loss : 0.2376\n",
      "Epoch 1/2 , Step 560/600, Loss : 0.1633\n",
      "Epoch 1/2 , Step 561/600, Loss : 0.1750\n",
      "Epoch 1/2 , Step 562/600, Loss : 0.1148\n",
      "Epoch 1/2 , Step 563/600, Loss : 0.1649\n",
      "Epoch 1/2 , Step 564/600, Loss : 0.1676\n",
      "Epoch 1/2 , Step 565/600, Loss : 0.1511\n",
      "Epoch 1/2 , Step 566/600, Loss : 0.2454\n",
      "Epoch 1/2 , Step 567/600, Loss : 0.2144\n",
      "Epoch 1/2 , Step 568/600, Loss : 0.1861\n",
      "Epoch 1/2 , Step 569/600, Loss : 0.1649\n",
      "Epoch 1/2 , Step 570/600, Loss : 0.1282\n",
      "Epoch 1/2 , Step 571/600, Loss : 0.0927\n",
      "Epoch 1/2 , Step 572/600, Loss : 0.2228\n",
      "Epoch 1/2 , Step 573/600, Loss : 0.1143\n",
      "Epoch 1/2 , Step 574/600, Loss : 0.1030\n",
      "Epoch 1/2 , Step 575/600, Loss : 0.1899\n",
      "Epoch 1/2 , Step 576/600, Loss : 0.1239\n",
      "Epoch 1/2 , Step 577/600, Loss : 0.1229\n",
      "Epoch 1/2 , Step 578/600, Loss : 0.1956\n",
      "Epoch 1/2 , Step 579/600, Loss : 0.1529\n",
      "Epoch 1/2 , Step 580/600, Loss : 0.1804\n",
      "Epoch 1/2 , Step 581/600, Loss : 0.0638\n",
      "Epoch 1/2 , Step 582/600, Loss : 0.2890\n",
      "Epoch 1/2 , Step 583/600, Loss : 0.1845\n",
      "Epoch 1/2 , Step 584/600, Loss : 0.2295\n",
      "Epoch 1/2 , Step 585/600, Loss : 0.1417\n",
      "Epoch 1/2 , Step 586/600, Loss : 0.0842\n",
      "Epoch 1/2 , Step 587/600, Loss : 0.1193\n",
      "Epoch 1/2 , Step 588/600, Loss : 0.1554\n",
      "Epoch 1/2 , Step 589/600, Loss : 0.1746\n",
      "Epoch 1/2 , Step 590/600, Loss : 0.1555\n",
      "Epoch 1/2 , Step 591/600, Loss : 0.0879\n",
      "Epoch 1/2 , Step 592/600, Loss : 0.1188\n",
      "Epoch 1/2 , Step 593/600, Loss : 0.0579\n",
      "Epoch 1/2 , Step 594/600, Loss : 0.1956\n",
      "Epoch 1/2 , Step 595/600, Loss : 0.1949\n",
      "Epoch 1/2 , Step 596/600, Loss : 0.2182\n",
      "Epoch 1/2 , Step 597/600, Loss : 0.1428\n",
      "Epoch 1/2 , Step 598/600, Loss : 0.1046\n",
      "Epoch 1/2 , Step 599/600, Loss : 0.1493\n",
      "Epoch 1/2 , Step 600/600, Loss : 0.1145\n",
      "Epoch 2/2 , Step 1/600, Loss : 0.0841\n",
      "Epoch 2/2 , Step 2/600, Loss : 0.1548\n",
      "Epoch 2/2 , Step 3/600, Loss : 0.2452\n",
      "Epoch 2/2 , Step 4/600, Loss : 0.1362\n",
      "Epoch 2/2 , Step 5/600, Loss : 0.1472\n",
      "Epoch 2/2 , Step 6/600, Loss : 0.1117\n",
      "Epoch 2/2 , Step 7/600, Loss : 0.0869\n",
      "Epoch 2/2 , Step 8/600, Loss : 0.1007\n",
      "Epoch 2/2 , Step 9/600, Loss : 0.1582\n",
      "Epoch 2/2 , Step 10/600, Loss : 0.1036\n",
      "Epoch 2/2 , Step 11/600, Loss : 0.1941\n",
      "Epoch 2/2 , Step 12/600, Loss : 0.1889\n",
      "Epoch 2/2 , Step 13/600, Loss : 0.1881\n",
      "Epoch 2/2 , Step 14/600, Loss : 0.1581\n",
      "Epoch 2/2 , Step 15/600, Loss : 0.1147\n",
      "Epoch 2/2 , Step 16/600, Loss : 0.1041\n",
      "Epoch 2/2 , Step 17/600, Loss : 0.1006\n",
      "Epoch 2/2 , Step 18/600, Loss : 0.0973\n",
      "Epoch 2/2 , Step 19/600, Loss : 0.2212\n",
      "Epoch 2/2 , Step 20/600, Loss : 0.0870\n",
      "Epoch 2/2 , Step 21/600, Loss : 0.1862\n",
      "Epoch 2/2 , Step 22/600, Loss : 0.2405\n",
      "Epoch 2/2 , Step 23/600, Loss : 0.2323\n",
      "Epoch 2/2 , Step 24/600, Loss : 0.0758\n",
      "Epoch 2/2 , Step 25/600, Loss : 0.1000\n",
      "Epoch 2/2 , Step 26/600, Loss : 0.1747\n",
      "Epoch 2/2 , Step 27/600, Loss : 0.1059\n",
      "Epoch 2/2 , Step 28/600, Loss : 0.2014\n",
      "Epoch 2/2 , Step 29/600, Loss : 0.1689\n",
      "Epoch 2/2 , Step 30/600, Loss : 0.1728\n",
      "Epoch 2/2 , Step 31/600, Loss : 0.1109\n",
      "Epoch 2/2 , Step 32/600, Loss : 0.0874\n",
      "Epoch 2/2 , Step 33/600, Loss : 0.1397\n",
      "Epoch 2/2 , Step 34/600, Loss : 0.1847\n",
      "Epoch 2/2 , Step 35/600, Loss : 0.1448\n",
      "Epoch 2/2 , Step 36/600, Loss : 0.1088\n",
      "Epoch 2/2 , Step 37/600, Loss : 0.1834\n",
      "Epoch 2/2 , Step 38/600, Loss : 0.0882\n",
      "Epoch 2/2 , Step 39/600, Loss : 0.0798\n",
      "Epoch 2/2 , Step 40/600, Loss : 0.1464\n",
      "Epoch 2/2 , Step 41/600, Loss : 0.1538\n",
      "Epoch 2/2 , Step 42/600, Loss : 0.1435\n",
      "Epoch 2/2 , Step 43/600, Loss : 0.1009\n",
      "Epoch 2/2 , Step 44/600, Loss : 0.1926\n",
      "Epoch 2/2 , Step 45/600, Loss : 0.0794\n",
      "Epoch 2/2 , Step 46/600, Loss : 0.0788\n",
      "Epoch 2/2 , Step 47/600, Loss : 0.0731\n",
      "Epoch 2/2 , Step 48/600, Loss : 0.0880\n",
      "Epoch 2/2 , Step 49/600, Loss : 0.0849\n",
      "Epoch 2/2 , Step 50/600, Loss : 0.2223\n",
      "Epoch 2/2 , Step 51/600, Loss : 0.1637\n",
      "Epoch 2/2 , Step 52/600, Loss : 0.0927\n",
      "Epoch 2/2 , Step 53/600, Loss : 0.0660\n",
      "Epoch 2/2 , Step 54/600, Loss : 0.1473\n",
      "Epoch 2/2 , Step 55/600, Loss : 0.1913\n",
      "Epoch 2/2 , Step 56/600, Loss : 0.1223\n",
      "Epoch 2/2 , Step 57/600, Loss : 0.0904\n",
      "Epoch 2/2 , Step 58/600, Loss : 0.0826\n",
      "Epoch 2/2 , Step 59/600, Loss : 0.2399\n",
      "Epoch 2/2 , Step 60/600, Loss : 0.1891\n",
      "Epoch 2/2 , Step 61/600, Loss : 0.0968\n",
      "Epoch 2/2 , Step 62/600, Loss : 0.1783\n",
      "Epoch 2/2 , Step 63/600, Loss : 0.0890\n",
      "Epoch 2/2 , Step 64/600, Loss : 0.1889\n",
      "Epoch 2/2 , Step 65/600, Loss : 0.2566\n",
      "Epoch 2/2 , Step 66/600, Loss : 0.1528\n",
      "Epoch 2/2 , Step 67/600, Loss : 0.0877\n",
      "Epoch 2/2 , Step 68/600, Loss : 0.1129\n",
      "Epoch 2/2 , Step 69/600, Loss : 0.1477\n",
      "Epoch 2/2 , Step 70/600, Loss : 0.1353\n",
      "Epoch 2/2 , Step 71/600, Loss : 0.0970\n",
      "Epoch 2/2 , Step 72/600, Loss : 0.1342\n",
      "Epoch 2/2 , Step 73/600, Loss : 0.1792\n",
      "Epoch 2/2 , Step 74/600, Loss : 0.0814\n",
      "Epoch 2/2 , Step 75/600, Loss : 0.1824\n",
      "Epoch 2/2 , Step 76/600, Loss : 0.0676\n",
      "Epoch 2/2 , Step 77/600, Loss : 0.1437\n",
      "Epoch 2/2 , Step 78/600, Loss : 0.1691\n",
      "Epoch 2/2 , Step 79/600, Loss : 0.0949\n",
      "Epoch 2/2 , Step 80/600, Loss : 0.1794\n",
      "Epoch 2/2 , Step 81/600, Loss : 0.0892\n",
      "Epoch 2/2 , Step 82/600, Loss : 0.1571\n",
      "Epoch 2/2 , Step 83/600, Loss : 0.0931\n",
      "Epoch 2/2 , Step 84/600, Loss : 0.1786\n",
      "Epoch 2/2 , Step 85/600, Loss : 0.1271\n",
      "Epoch 2/2 , Step 86/600, Loss : 0.1085\n",
      "Epoch 2/2 , Step 87/600, Loss : 0.0726\n",
      "Epoch 2/2 , Step 88/600, Loss : 0.2004\n",
      "Epoch 2/2 , Step 89/600, Loss : 0.1915\n",
      "Epoch 2/2 , Step 90/600, Loss : 0.0819\n",
      "Epoch 2/2 , Step 91/600, Loss : 0.1590\n",
      "Epoch 2/2 , Step 92/600, Loss : 0.1388\n",
      "Epoch 2/2 , Step 93/600, Loss : 0.2085\n",
      "Epoch 2/2 , Step 94/600, Loss : 0.2263\n",
      "Epoch 2/2 , Step 95/600, Loss : 0.2063\n",
      "Epoch 2/2 , Step 96/600, Loss : 0.0839\n",
      "Epoch 2/2 , Step 97/600, Loss : 0.1352\n",
      "Epoch 2/2 , Step 98/600, Loss : 0.2214\n",
      "Epoch 2/2 , Step 99/600, Loss : 0.3158\n",
      "Epoch 2/2 , Step 100/600, Loss : 0.1579\n",
      "Epoch 2/2 , Step 101/600, Loss : 0.1132\n",
      "Epoch 2/2 , Step 102/600, Loss : 0.1003\n",
      "Epoch 2/2 , Step 103/600, Loss : 0.1739\n",
      "Epoch 2/2 , Step 104/600, Loss : 0.1524\n",
      "Epoch 2/2 , Step 105/600, Loss : 0.0710\n",
      "Epoch 2/2 , Step 106/600, Loss : 0.1023\n",
      "Epoch 2/2 , Step 107/600, Loss : 0.0775\n",
      "Epoch 2/2 , Step 108/600, Loss : 0.1325\n",
      "Epoch 2/2 , Step 109/600, Loss : 0.1106\n",
      "Epoch 2/2 , Step 110/600, Loss : 0.2592\n",
      "Epoch 2/2 , Step 111/600, Loss : 0.1336\n",
      "Epoch 2/2 , Step 112/600, Loss : 0.0806\n",
      "Epoch 2/2 , Step 113/600, Loss : 0.1173\n",
      "Epoch 2/2 , Step 114/600, Loss : 0.1734\n",
      "Epoch 2/2 , Step 115/600, Loss : 0.1287\n",
      "Epoch 2/2 , Step 116/600, Loss : 0.1812\n",
      "Epoch 2/2 , Step 117/600, Loss : 0.1024\n",
      "Epoch 2/2 , Step 118/600, Loss : 0.1181\n",
      "Epoch 2/2 , Step 119/600, Loss : 0.1144\n",
      "Epoch 2/2 , Step 120/600, Loss : 0.1677\n",
      "Epoch 2/2 , Step 121/600, Loss : 0.1665\n",
      "Epoch 2/2 , Step 122/600, Loss : 0.1085\n",
      "Epoch 2/2 , Step 123/600, Loss : 0.1322\n",
      "Epoch 2/2 , Step 124/600, Loss : 0.1596\n",
      "Epoch 2/2 , Step 125/600, Loss : 0.1368\n",
      "Epoch 2/2 , Step 126/600, Loss : 0.0912\n",
      "Epoch 2/2 , Step 127/600, Loss : 0.0431\n",
      "Epoch 2/2 , Step 128/600, Loss : 0.1085\n",
      "Epoch 2/2 , Step 129/600, Loss : 0.0743\n",
      "Epoch 2/2 , Step 130/600, Loss : 0.1489\n",
      "Epoch 2/2 , Step 131/600, Loss : 0.1382\n",
      "Epoch 2/2 , Step 132/600, Loss : 0.0775\n",
      "Epoch 2/2 , Step 133/600, Loss : 0.1051\n",
      "Epoch 2/2 , Step 134/600, Loss : 0.1375\n",
      "Epoch 2/2 , Step 135/600, Loss : 0.0936\n",
      "Epoch 2/2 , Step 136/600, Loss : 0.1026\n",
      "Epoch 2/2 , Step 137/600, Loss : 0.0713\n",
      "Epoch 2/2 , Step 138/600, Loss : 0.1204\n",
      "Epoch 2/2 , Step 139/600, Loss : 0.0561\n",
      "Epoch 2/2 , Step 140/600, Loss : 0.1303\n",
      "Epoch 2/2 , Step 141/600, Loss : 0.1583\n",
      "Epoch 2/2 , Step 142/600, Loss : 0.0818\n",
      "Epoch 2/2 , Step 143/600, Loss : 0.0979\n",
      "Epoch 2/2 , Step 144/600, Loss : 0.0547\n",
      "Epoch 2/2 , Step 145/600, Loss : 0.0565\n",
      "Epoch 2/2 , Step 146/600, Loss : 0.1317\n",
      "Epoch 2/2 , Step 147/600, Loss : 0.1992\n",
      "Epoch 2/2 , Step 148/600, Loss : 0.1880\n",
      "Epoch 2/2 , Step 149/600, Loss : 0.0885\n",
      "Epoch 2/2 , Step 150/600, Loss : 0.1615\n",
      "Epoch 2/2 , Step 151/600, Loss : 0.1058\n",
      "Epoch 2/2 , Step 152/600, Loss : 0.0420\n",
      "Epoch 2/2 , Step 153/600, Loss : 0.1100\n",
      "Epoch 2/2 , Step 154/600, Loss : 0.0865\n",
      "Epoch 2/2 , Step 155/600, Loss : 0.1262\n",
      "Epoch 2/2 , Step 156/600, Loss : 0.0880\n",
      "Epoch 2/2 , Step 157/600, Loss : 0.2727\n",
      "Epoch 2/2 , Step 158/600, Loss : 0.0559\n",
      "Epoch 2/2 , Step 159/600, Loss : 0.0868\n",
      "Epoch 2/2 , Step 160/600, Loss : 0.2105\n",
      "Epoch 2/2 , Step 161/600, Loss : 0.1240\n",
      "Epoch 2/2 , Step 162/600, Loss : 0.0329\n",
      "Epoch 2/2 , Step 163/600, Loss : 0.0721\n",
      "Epoch 2/2 , Step 164/600, Loss : 0.1694\n",
      "Epoch 2/2 , Step 165/600, Loss : 0.1319\n",
      "Epoch 2/2 , Step 166/600, Loss : 0.1717\n",
      "Epoch 2/2 , Step 167/600, Loss : 0.1217\n",
      "Epoch 2/2 , Step 168/600, Loss : 0.0750\n",
      "Epoch 2/2 , Step 169/600, Loss : 0.1441\n",
      "Epoch 2/2 , Step 170/600, Loss : 0.1561\n",
      "Epoch 2/2 , Step 171/600, Loss : 0.1390\n",
      "Epoch 2/2 , Step 172/600, Loss : 0.0942\n",
      "Epoch 2/2 , Step 173/600, Loss : 0.1071\n",
      "Epoch 2/2 , Step 174/600, Loss : 0.1535\n",
      "Epoch 2/2 , Step 175/600, Loss : 0.0914\n",
      "Epoch 2/2 , Step 176/600, Loss : 0.1654\n",
      "Epoch 2/2 , Step 177/600, Loss : 0.0998\n",
      "Epoch 2/2 , Step 178/600, Loss : 0.0963\n",
      "Epoch 2/2 , Step 179/600, Loss : 0.0658\n",
      "Epoch 2/2 , Step 180/600, Loss : 0.0659\n",
      "Epoch 2/2 , Step 181/600, Loss : 0.0839\n",
      "Epoch 2/2 , Step 182/600, Loss : 0.0823\n",
      "Epoch 2/2 , Step 183/600, Loss : 0.0589\n",
      "Epoch 2/2 , Step 184/600, Loss : 0.0594\n",
      "Epoch 2/2 , Step 185/600, Loss : 0.0696\n",
      "Epoch 2/2 , Step 186/600, Loss : 0.1405\n",
      "Epoch 2/2 , Step 187/600, Loss : 0.0432\n",
      "Epoch 2/2 , Step 188/600, Loss : 0.1161\n",
      "Epoch 2/2 , Step 189/600, Loss : 0.1138\n",
      "Epoch 2/2 , Step 190/600, Loss : 0.2246\n",
      "Epoch 2/2 , Step 191/600, Loss : 0.1707\n",
      "Epoch 2/2 , Step 192/600, Loss : 0.0821\n",
      "Epoch 2/2 , Step 193/600, Loss : 0.2939\n",
      "Epoch 2/2 , Step 194/600, Loss : 0.1090\n",
      "Epoch 2/2 , Step 195/600, Loss : 0.1362\n",
      "Epoch 2/2 , Step 196/600, Loss : 0.1145\n",
      "Epoch 2/2 , Step 197/600, Loss : 0.0759\n",
      "Epoch 2/2 , Step 198/600, Loss : 0.1284\n",
      "Epoch 2/2 , Step 199/600, Loss : 0.2227\n",
      "Epoch 2/2 , Step 200/600, Loss : 0.0727\n",
      "Epoch 2/2 , Step 201/600, Loss : 0.1237\n",
      "Epoch 2/2 , Step 202/600, Loss : 0.1293\n",
      "Epoch 2/2 , Step 203/600, Loss : 0.0702\n",
      "Epoch 2/2 , Step 204/600, Loss : 0.0575\n",
      "Epoch 2/2 , Step 205/600, Loss : 0.1116\n",
      "Epoch 2/2 , Step 206/600, Loss : 0.1368\n",
      "Epoch 2/2 , Step 207/600, Loss : 0.1166\n",
      "Epoch 2/2 , Step 208/600, Loss : 0.1056\n",
      "Epoch 2/2 , Step 209/600, Loss : 0.0898\n",
      "Epoch 2/2 , Step 210/600, Loss : 0.1504\n",
      "Epoch 2/2 , Step 211/600, Loss : 0.1679\n",
      "Epoch 2/2 , Step 212/600, Loss : 0.0828\n",
      "Epoch 2/2 , Step 213/600, Loss : 0.1726\n",
      "Epoch 2/2 , Step 214/600, Loss : 0.0792\n",
      "Epoch 2/2 , Step 215/600, Loss : 0.0696\n",
      "Epoch 2/2 , Step 216/600, Loss : 0.1328\n",
      "Epoch 2/2 , Step 217/600, Loss : 0.0815\n",
      "Epoch 2/2 , Step 218/600, Loss : 0.1413\n",
      "Epoch 2/2 , Step 219/600, Loss : 0.0748\n",
      "Epoch 2/2 , Step 220/600, Loss : 0.1370\n",
      "Epoch 2/2 , Step 221/600, Loss : 0.1024\n",
      "Epoch 2/2 , Step 222/600, Loss : 0.1395\n",
      "Epoch 2/2 , Step 223/600, Loss : 0.0715\n",
      "Epoch 2/2 , Step 224/600, Loss : 0.0511\n",
      "Epoch 2/2 , Step 225/600, Loss : 0.1827\n",
      "Epoch 2/2 , Step 226/600, Loss : 0.1270\n",
      "Epoch 2/2 , Step 227/600, Loss : 0.1032\n",
      "Epoch 2/2 , Step 228/600, Loss : 0.1334\n",
      "Epoch 2/2 , Step 229/600, Loss : 0.0609\n",
      "Epoch 2/2 , Step 230/600, Loss : 0.0686\n",
      "Epoch 2/2 , Step 231/600, Loss : 0.0802\n",
      "Epoch 2/2 , Step 232/600, Loss : 0.1368\n",
      "Epoch 2/2 , Step 233/600, Loss : 0.1162\n",
      "Epoch 2/2 , Step 234/600, Loss : 0.0879\n",
      "Epoch 2/2 , Step 235/600, Loss : 0.1174\n",
      "Epoch 2/2 , Step 236/600, Loss : 0.1572\n",
      "Epoch 2/2 , Step 237/600, Loss : 0.1062\n",
      "Epoch 2/2 , Step 238/600, Loss : 0.1059\n",
      "Epoch 2/2 , Step 239/600, Loss : 0.1378\n",
      "Epoch 2/2 , Step 240/600, Loss : 0.0727\n",
      "Epoch 2/2 , Step 241/600, Loss : 0.0710\n",
      "Epoch 2/2 , Step 242/600, Loss : 0.1928\n",
      "Epoch 2/2 , Step 243/600, Loss : 0.1480\n",
      "Epoch 2/2 , Step 244/600, Loss : 0.1274\n",
      "Epoch 2/2 , Step 245/600, Loss : 0.2159\n",
      "Epoch 2/2 , Step 246/600, Loss : 0.0248\n",
      "Epoch 2/2 , Step 247/600, Loss : 0.0935\n",
      "Epoch 2/2 , Step 248/600, Loss : 0.1863\n",
      "Epoch 2/2 , Step 249/600, Loss : 0.0948\n",
      "Epoch 2/2 , Step 250/600, Loss : 0.0719\n",
      "Epoch 2/2 , Step 251/600, Loss : 0.2254\n",
      "Epoch 2/2 , Step 252/600, Loss : 0.1270\n",
      "Epoch 2/2 , Step 253/600, Loss : 0.0801\n",
      "Epoch 2/2 , Step 254/600, Loss : 0.1539\n",
      "Epoch 2/2 , Step 255/600, Loss : 0.1116\n",
      "Epoch 2/2 , Step 256/600, Loss : 0.1721\n",
      "Epoch 2/2 , Step 257/600, Loss : 0.1220\n",
      "Epoch 2/2 , Step 258/600, Loss : 0.1064\n",
      "Epoch 2/2 , Step 259/600, Loss : 0.1371\n",
      "Epoch 2/2 , Step 260/600, Loss : 0.1571\n",
      "Epoch 2/2 , Step 261/600, Loss : 0.0727\n",
      "Epoch 2/2 , Step 262/600, Loss : 0.1508\n",
      "Epoch 2/2 , Step 263/600, Loss : 0.1441\n",
      "Epoch 2/2 , Step 264/600, Loss : 0.0626\n",
      "Epoch 2/2 , Step 265/600, Loss : 0.1964\n",
      "Epoch 2/2 , Step 266/600, Loss : 0.1983\n",
      "Epoch 2/2 , Step 267/600, Loss : 0.0912\n",
      "Epoch 2/2 , Step 268/600, Loss : 0.0785\n",
      "Epoch 2/2 , Step 269/600, Loss : 0.0557\n",
      "Epoch 2/2 , Step 270/600, Loss : 0.0619\n",
      "Epoch 2/2 , Step 271/600, Loss : 0.1313\n",
      "Epoch 2/2 , Step 272/600, Loss : 0.1519\n",
      "Epoch 2/2 , Step 273/600, Loss : 0.1627\n",
      "Epoch 2/2 , Step 274/600, Loss : 0.0683\n",
      "Epoch 2/2 , Step 275/600, Loss : 0.0815\n",
      "Epoch 2/2 , Step 276/600, Loss : 0.1225\n",
      "Epoch 2/2 , Step 277/600, Loss : 0.0934\n",
      "Epoch 2/2 , Step 278/600, Loss : 0.1467\n",
      "Epoch 2/2 , Step 279/600, Loss : 0.0730\n",
      "Epoch 2/2 , Step 280/600, Loss : 0.1690\n",
      "Epoch 2/2 , Step 281/600, Loss : 0.0887\n",
      "Epoch 2/2 , Step 282/600, Loss : 0.1484\n",
      "Epoch 2/2 , Step 283/600, Loss : 0.0630\n",
      "Epoch 2/2 , Step 284/600, Loss : 0.1088\n",
      "Epoch 2/2 , Step 285/600, Loss : 0.0649\n",
      "Epoch 2/2 , Step 286/600, Loss : 0.1430\n",
      "Epoch 2/2 , Step 287/600, Loss : 0.2171\n",
      "Epoch 2/2 , Step 288/600, Loss : 0.0949\n",
      "Epoch 2/2 , Step 289/600, Loss : 0.1416\n",
      "Epoch 2/2 , Step 290/600, Loss : 0.1585\n",
      "Epoch 2/2 , Step 291/600, Loss : 0.0629\n",
      "Epoch 2/2 , Step 292/600, Loss : 0.0981\n",
      "Epoch 2/2 , Step 293/600, Loss : 0.1037\n",
      "Epoch 2/2 , Step 294/600, Loss : 0.0976\n",
      "Epoch 2/2 , Step 295/600, Loss : 0.0836\n",
      "Epoch 2/2 , Step 296/600, Loss : 0.1529\n",
      "Epoch 2/2 , Step 297/600, Loss : 0.0885\n",
      "Epoch 2/2 , Step 298/600, Loss : 0.0825\n",
      "Epoch 2/2 , Step 299/600, Loss : 0.1598\n",
      "Epoch 2/2 , Step 300/600, Loss : 0.0993\n",
      "Epoch 2/2 , Step 301/600, Loss : 0.0913\n",
      "Epoch 2/2 , Step 302/600, Loss : 0.1022\n",
      "Epoch 2/2 , Step 303/600, Loss : 0.0684\n",
      "Epoch 2/2 , Step 304/600, Loss : 0.1067\n",
      "Epoch 2/2 , Step 305/600, Loss : 0.1713\n",
      "Epoch 2/2 , Step 306/600, Loss : 0.2514\n",
      "Epoch 2/2 , Step 307/600, Loss : 0.1357\n",
      "Epoch 2/2 , Step 308/600, Loss : 0.1471\n",
      "Epoch 2/2 , Step 309/600, Loss : 0.2088\n",
      "Epoch 2/2 , Step 310/600, Loss : 0.0521\n",
      "Epoch 2/2 , Step 311/600, Loss : 0.1082\n",
      "Epoch 2/2 , Step 312/600, Loss : 0.0669\n",
      "Epoch 2/2 , Step 313/600, Loss : 0.0620\n",
      "Epoch 2/2 , Step 314/600, Loss : 0.1049\n",
      "Epoch 2/2 , Step 315/600, Loss : 0.0585\n",
      "Epoch 2/2 , Step 316/600, Loss : 0.1288\n",
      "Epoch 2/2 , Step 317/600, Loss : 0.0699\n",
      "Epoch 2/2 , Step 318/600, Loss : 0.0674\n",
      "Epoch 2/2 , Step 319/600, Loss : 0.1167\n",
      "Epoch 2/2 , Step 320/600, Loss : 0.0571\n",
      "Epoch 2/2 , Step 321/600, Loss : 0.1806\n",
      "Epoch 2/2 , Step 322/600, Loss : 0.1460\n",
      "Epoch 2/2 , Step 323/600, Loss : 0.0395\n",
      "Epoch 2/2 , Step 324/600, Loss : 0.1345\n",
      "Epoch 2/2 , Step 325/600, Loss : 0.0782\n",
      "Epoch 2/2 , Step 326/600, Loss : 0.0909\n",
      "Epoch 2/2 , Step 327/600, Loss : 0.0398\n",
      "Epoch 2/2 , Step 328/600, Loss : 0.1574\n",
      "Epoch 2/2 , Step 329/600, Loss : 0.1160\n",
      "Epoch 2/2 , Step 330/600, Loss : 0.0909\n",
      "Epoch 2/2 , Step 331/600, Loss : 0.1206\n",
      "Epoch 2/2 , Step 332/600, Loss : 0.0663\n",
      "Epoch 2/2 , Step 333/600, Loss : 0.1298\n",
      "Epoch 2/2 , Step 334/600, Loss : 0.0904\n",
      "Epoch 2/2 , Step 335/600, Loss : 0.0607\n",
      "Epoch 2/2 , Step 336/600, Loss : 0.1183\n",
      "Epoch 2/2 , Step 337/600, Loss : 0.0862\n",
      "Epoch 2/2 , Step 338/600, Loss : 0.1103\n",
      "Epoch 2/2 , Step 339/600, Loss : 0.1065\n",
      "Epoch 2/2 , Step 340/600, Loss : 0.0602\n",
      "Epoch 2/2 , Step 341/600, Loss : 0.1144\n",
      "Epoch 2/2 , Step 342/600, Loss : 0.1696\n",
      "Epoch 2/2 , Step 343/600, Loss : 0.1285\n",
      "Epoch 2/2 , Step 344/600, Loss : 0.1980\n",
      "Epoch 2/2 , Step 345/600, Loss : 0.1437\n",
      "Epoch 2/2 , Step 346/600, Loss : 0.0229\n",
      "Epoch 2/2 , Step 347/600, Loss : 0.1938\n",
      "Epoch 2/2 , Step 348/600, Loss : 0.0991\n",
      "Epoch 2/2 , Step 349/600, Loss : 0.0417\n",
      "Epoch 2/2 , Step 350/600, Loss : 0.2050\n",
      "Epoch 2/2 , Step 351/600, Loss : 0.1661\n",
      "Epoch 2/2 , Step 352/600, Loss : 0.0642\n",
      "Epoch 2/2 , Step 353/600, Loss : 0.2597\n",
      "Epoch 2/2 , Step 354/600, Loss : 0.0962\n",
      "Epoch 2/2 , Step 355/600, Loss : 0.1867\n",
      "Epoch 2/2 , Step 356/600, Loss : 0.1310\n",
      "Epoch 2/2 , Step 357/600, Loss : 0.1240\n",
      "Epoch 2/2 , Step 358/600, Loss : 0.0461\n",
      "Epoch 2/2 , Step 359/600, Loss : 0.1038\n",
      "Epoch 2/2 , Step 360/600, Loss : 0.1239\n",
      "Epoch 2/2 , Step 361/600, Loss : 0.1249\n",
      "Epoch 2/2 , Step 362/600, Loss : 0.0929\n",
      "Epoch 2/2 , Step 363/600, Loss : 0.0808\n",
      "Epoch 2/2 , Step 364/600, Loss : 0.1134\n",
      "Epoch 2/2 , Step 365/600, Loss : 0.0921\n",
      "Epoch 2/2 , Step 366/600, Loss : 0.0565\n",
      "Epoch 2/2 , Step 367/600, Loss : 0.1749\n",
      "Epoch 2/2 , Step 368/600, Loss : 0.1353\n",
      "Epoch 2/2 , Step 369/600, Loss : 0.0987\n",
      "Epoch 2/2 , Step 370/600, Loss : 0.0662\n",
      "Epoch 2/2 , Step 371/600, Loss : 0.1069\n",
      "Epoch 2/2 , Step 372/600, Loss : 0.0821\n",
      "Epoch 2/2 , Step 373/600, Loss : 0.0433\n",
      "Epoch 2/2 , Step 374/600, Loss : 0.0999\n",
      "Epoch 2/2 , Step 375/600, Loss : 0.0595\n",
      "Epoch 2/2 , Step 376/600, Loss : 0.0843\n",
      "Epoch 2/2 , Step 377/600, Loss : 0.1316\n",
      "Epoch 2/2 , Step 378/600, Loss : 0.0722\n",
      "Epoch 2/2 , Step 379/600, Loss : 0.1182\n",
      "Epoch 2/2 , Step 380/600, Loss : 0.1149\n",
      "Epoch 2/2 , Step 381/600, Loss : 0.1019\n",
      "Epoch 2/2 , Step 382/600, Loss : 0.1424\n",
      "Epoch 2/2 , Step 383/600, Loss : 0.1347\n",
      "Epoch 2/2 , Step 384/600, Loss : 0.1143\n",
      "Epoch 2/2 , Step 385/600, Loss : 0.1111\n",
      "Epoch 2/2 , Step 386/600, Loss : 0.0600\n",
      "Epoch 2/2 , Step 387/600, Loss : 0.0945\n",
      "Epoch 2/2 , Step 388/600, Loss : 0.1369\n",
      "Epoch 2/2 , Step 389/600, Loss : 0.0786\n",
      "Epoch 2/2 , Step 390/600, Loss : 0.0517\n",
      "Epoch 2/2 , Step 391/600, Loss : 0.0859\n",
      "Epoch 2/2 , Step 392/600, Loss : 0.1883\n",
      "Epoch 2/2 , Step 393/600, Loss : 0.0516\n",
      "Epoch 2/2 , Step 394/600, Loss : 0.1192\n",
      "Epoch 2/2 , Step 395/600, Loss : 0.1003\n",
      "Epoch 2/2 , Step 396/600, Loss : 0.1864\n",
      "Epoch 2/2 , Step 397/600, Loss : 0.1260\n",
      "Epoch 2/2 , Step 398/600, Loss : 0.1371\n",
      "Epoch 2/2 , Step 399/600, Loss : 0.1192\n",
      "Epoch 2/2 , Step 400/600, Loss : 0.1389\n",
      "Epoch 2/2 , Step 401/600, Loss : 0.0882\n",
      "Epoch 2/2 , Step 402/600, Loss : 0.2267\n",
      "Epoch 2/2 , Step 403/600, Loss : 0.0864\n",
      "Epoch 2/2 , Step 404/600, Loss : 0.1360\n",
      "Epoch 2/2 , Step 405/600, Loss : 0.0856\n",
      "Epoch 2/2 , Step 406/600, Loss : 0.0632\n",
      "Epoch 2/2 , Step 407/600, Loss : 0.0467\n",
      "Epoch 2/2 , Step 408/600, Loss : 0.0553\n",
      "Epoch 2/2 , Step 409/600, Loss : 0.0263\n",
      "Epoch 2/2 , Step 410/600, Loss : 0.1042\n",
      "Epoch 2/2 , Step 411/600, Loss : 0.0953\n",
      "Epoch 2/2 , Step 412/600, Loss : 0.2600\n",
      "Epoch 2/2 , Step 413/600, Loss : 0.1784\n",
      "Epoch 2/2 , Step 414/600, Loss : 0.0697\n",
      "Epoch 2/2 , Step 415/600, Loss : 0.2591\n",
      "Epoch 2/2 , Step 416/600, Loss : 0.1548\n",
      "Epoch 2/2 , Step 417/600, Loss : 0.0947\n",
      "Epoch 2/2 , Step 418/600, Loss : 0.0749\n",
      "Epoch 2/2 , Step 419/600, Loss : 0.2060\n",
      "Epoch 2/2 , Step 420/600, Loss : 0.1041\n",
      "Epoch 2/2 , Step 421/600, Loss : 0.0532\n",
      "Epoch 2/2 , Step 422/600, Loss : 0.1534\n",
      "Epoch 2/2 , Step 423/600, Loss : 0.0471\n",
      "Epoch 2/2 , Step 424/600, Loss : 0.1293\n",
      "Epoch 2/2 , Step 425/600, Loss : 0.1148\n",
      "Epoch 2/2 , Step 426/600, Loss : 0.1149\n",
      "Epoch 2/2 , Step 427/600, Loss : 0.1573\n",
      "Epoch 2/2 , Step 428/600, Loss : 0.1097\n",
      "Epoch 2/2 , Step 429/600, Loss : 0.1119\n",
      "Epoch 2/2 , Step 430/600, Loss : 0.0816\n",
      "Epoch 2/2 , Step 431/600, Loss : 0.0738\n",
      "Epoch 2/2 , Step 432/600, Loss : 0.0614\n",
      "Epoch 2/2 , Step 433/600, Loss : 0.0691\n",
      "Epoch 2/2 , Step 434/600, Loss : 0.0664\n",
      "Epoch 2/2 , Step 435/600, Loss : 0.1288\n",
      "Epoch 2/2 , Step 436/600, Loss : 0.0817\n",
      "Epoch 2/2 , Step 437/600, Loss : 0.0394\n",
      "Epoch 2/2 , Step 438/600, Loss : 0.1300\n",
      "Epoch 2/2 , Step 439/600, Loss : 0.0432\n",
      "Epoch 2/2 , Step 440/600, Loss : 0.1380\n",
      "Epoch 2/2 , Step 441/600, Loss : 0.0922\n",
      "Epoch 2/2 , Step 442/600, Loss : 0.0925\n",
      "Epoch 2/2 , Step 443/600, Loss : 0.1577\n",
      "Epoch 2/2 , Step 444/600, Loss : 0.1113\n",
      "Epoch 2/2 , Step 445/600, Loss : 0.1838\n",
      "Epoch 2/2 , Step 446/600, Loss : 0.1174\n",
      "Epoch 2/2 , Step 447/600, Loss : 0.1118\n",
      "Epoch 2/2 , Step 448/600, Loss : 0.1292\n",
      "Epoch 2/2 , Step 449/600, Loss : 0.1247\n",
      "Epoch 2/2 , Step 450/600, Loss : 0.0830\n",
      "Epoch 2/2 , Step 451/600, Loss : 0.1237\n",
      "Epoch 2/2 , Step 452/600, Loss : 0.1156\n",
      "Epoch 2/2 , Step 453/600, Loss : 0.0929\n",
      "Epoch 2/2 , Step 454/600, Loss : 0.1074\n",
      "Epoch 2/2 , Step 455/600, Loss : 0.1374\n",
      "Epoch 2/2 , Step 456/600, Loss : 0.0856\n",
      "Epoch 2/2 , Step 457/600, Loss : 0.1095\n",
      "Epoch 2/2 , Step 458/600, Loss : 0.1043\n",
      "Epoch 2/2 , Step 459/600, Loss : 0.1015\n",
      "Epoch 2/2 , Step 460/600, Loss : 0.1002\n",
      "Epoch 2/2 , Step 461/600, Loss : 0.0860\n",
      "Epoch 2/2 , Step 462/600, Loss : 0.1485\n",
      "Epoch 2/2 , Step 463/600, Loss : 0.1185\n",
      "Epoch 2/2 , Step 464/600, Loss : 0.1256\n",
      "Epoch 2/2 , Step 465/600, Loss : 0.1900\n",
      "Epoch 2/2 , Step 466/600, Loss : 0.1233\n",
      "Epoch 2/2 , Step 467/600, Loss : 0.0913\n",
      "Epoch 2/2 , Step 468/600, Loss : 0.0575\n",
      "Epoch 2/2 , Step 469/600, Loss : 0.0973\n",
      "Epoch 2/2 , Step 470/600, Loss : 0.1244\n",
      "Epoch 2/2 , Step 471/600, Loss : 0.0958\n",
      "Epoch 2/2 , Step 472/600, Loss : 0.1196\n",
      "Epoch 2/2 , Step 473/600, Loss : 0.1377\n",
      "Epoch 2/2 , Step 474/600, Loss : 0.1907\n",
      "Epoch 2/2 , Step 475/600, Loss : 0.1599\n",
      "Epoch 2/2 , Step 476/600, Loss : 0.1026\n",
      "Epoch 2/2 , Step 477/600, Loss : 0.1154\n",
      "Epoch 2/2 , Step 478/600, Loss : 0.0506\n",
      "Epoch 2/2 , Step 479/600, Loss : 0.1038\n",
      "Epoch 2/2 , Step 480/600, Loss : 0.0451\n",
      "Epoch 2/2 , Step 481/600, Loss : 0.0994\n",
      "Epoch 2/2 , Step 482/600, Loss : 0.0568\n",
      "Epoch 2/2 , Step 483/600, Loss : 0.2043\n",
      "Epoch 2/2 , Step 484/600, Loss : 0.2098\n",
      "Epoch 2/2 , Step 485/600, Loss : 0.1396\n",
      "Epoch 2/2 , Step 486/600, Loss : 0.1569\n",
      "Epoch 2/2 , Step 487/600, Loss : 0.0761\n",
      "Epoch 2/2 , Step 488/600, Loss : 0.0538\n",
      "Epoch 2/2 , Step 489/600, Loss : 0.0669\n",
      "Epoch 2/2 , Step 490/600, Loss : 0.2048\n",
      "Epoch 2/2 , Step 491/600, Loss : 0.0709\n",
      "Epoch 2/2 , Step 492/600, Loss : 0.0568\n",
      "Epoch 2/2 , Step 493/600, Loss : 0.0458\n",
      "Epoch 2/2 , Step 494/600, Loss : 0.0942\n",
      "Epoch 2/2 , Step 495/600, Loss : 0.1580\n",
      "Epoch 2/2 , Step 496/600, Loss : 0.0699\n",
      "Epoch 2/2 , Step 497/600, Loss : 0.0441\n",
      "Epoch 2/2 , Step 498/600, Loss : 0.1479\n",
      "Epoch 2/2 , Step 499/600, Loss : 0.2127\n",
      "Epoch 2/2 , Step 500/600, Loss : 0.1836\n",
      "Epoch 2/2 , Step 501/600, Loss : 0.0488\n",
      "Epoch 2/2 , Step 502/600, Loss : 0.1070\n",
      "Epoch 2/2 , Step 503/600, Loss : 0.1001\n",
      "Epoch 2/2 , Step 504/600, Loss : 0.1418\n",
      "Epoch 2/2 , Step 505/600, Loss : 0.1107\n",
      "Epoch 2/2 , Step 506/600, Loss : 0.1244\n",
      "Epoch 2/2 , Step 507/600, Loss : 0.1047\n",
      "Epoch 2/2 , Step 508/600, Loss : 0.1652\n",
      "Epoch 2/2 , Step 509/600, Loss : 0.0485\n",
      "Epoch 2/2 , Step 510/600, Loss : 0.0939\n",
      "Epoch 2/2 , Step 511/600, Loss : 0.1288\n",
      "Epoch 2/2 , Step 512/600, Loss : 0.0928\n",
      "Epoch 2/2 , Step 513/600, Loss : 0.0765\n",
      "Epoch 2/2 , Step 514/600, Loss : 0.0864\n",
      "Epoch 2/2 , Step 515/600, Loss : 0.1521\n",
      "Epoch 2/2 , Step 516/600, Loss : 0.1434\n",
      "Epoch 2/2 , Step 517/600, Loss : 0.1936\n",
      "Epoch 2/2 , Step 518/600, Loss : 0.0533\n",
      "Epoch 2/2 , Step 519/600, Loss : 0.1485\n",
      "Epoch 2/2 , Step 520/600, Loss : 0.0919\n",
      "Epoch 2/2 , Step 521/600, Loss : 0.0658\n",
      "Epoch 2/2 , Step 522/600, Loss : 0.0866\n",
      "Epoch 2/2 , Step 523/600, Loss : 0.0999\n",
      "Epoch 2/2 , Step 524/600, Loss : 0.0804\n",
      "Epoch 2/2 , Step 525/600, Loss : 0.1503\n",
      "Epoch 2/2 , Step 526/600, Loss : 0.0535\n",
      "Epoch 2/2 , Step 527/600, Loss : 0.0860\n",
      "Epoch 2/2 , Step 528/600, Loss : 0.0709\n",
      "Epoch 2/2 , Step 529/600, Loss : 0.2100\n",
      "Epoch 2/2 , Step 530/600, Loss : 0.1138\n",
      "Epoch 2/2 , Step 531/600, Loss : 0.1310\n",
      "Epoch 2/2 , Step 532/600, Loss : 0.0633\n",
      "Epoch 2/2 , Step 533/600, Loss : 0.1403\n",
      "Epoch 2/2 , Step 534/600, Loss : 0.1030\n",
      "Epoch 2/2 , Step 535/600, Loss : 0.1189\n",
      "Epoch 2/2 , Step 536/600, Loss : 0.1367\n",
      "Epoch 2/2 , Step 537/600, Loss : 0.1048\n",
      "Epoch 2/2 , Step 538/600, Loss : 0.0975\n",
      "Epoch 2/2 , Step 539/600, Loss : 0.0817\n",
      "Epoch 2/2 , Step 540/600, Loss : 0.0752\n",
      "Epoch 2/2 , Step 541/600, Loss : 0.1848\n",
      "Epoch 2/2 , Step 542/600, Loss : 0.0528\n",
      "Epoch 2/2 , Step 543/600, Loss : 0.1056\n",
      "Epoch 2/2 , Step 544/600, Loss : 0.0909\n",
      "Epoch 2/2 , Step 545/600, Loss : 0.1502\n",
      "Epoch 2/2 , Step 546/600, Loss : 0.0638\n",
      "Epoch 2/2 , Step 547/600, Loss : 0.2059\n",
      "Epoch 2/2 , Step 548/600, Loss : 0.0351\n",
      "Epoch 2/2 , Step 549/600, Loss : 0.0537\n",
      "Epoch 2/2 , Step 550/600, Loss : 0.0659\n",
      "Epoch 2/2 , Step 551/600, Loss : 0.0666\n",
      "Epoch 2/2 , Step 552/600, Loss : 0.1126\n",
      "Epoch 2/2 , Step 553/600, Loss : 0.1413\n",
      "Epoch 2/2 , Step 554/600, Loss : 0.0830\n",
      "Epoch 2/2 , Step 555/600, Loss : 0.1189\n",
      "Epoch 2/2 , Step 556/600, Loss : 0.1001\n",
      "Epoch 2/2 , Step 557/600, Loss : 0.1313\n",
      "Epoch 2/2 , Step 558/600, Loss : 0.0653\n",
      "Epoch 2/2 , Step 559/600, Loss : 0.2155\n",
      "Epoch 2/2 , Step 560/600, Loss : 0.0828\n",
      "Epoch 2/2 , Step 561/600, Loss : 0.0460\n",
      "Epoch 2/2 , Step 562/600, Loss : 0.2439\n",
      "Epoch 2/2 , Step 563/600, Loss : 0.0683\n",
      "Epoch 2/2 , Step 564/600, Loss : 0.1146\n",
      "Epoch 2/2 , Step 565/600, Loss : 0.2106\n",
      "Epoch 2/2 , Step 566/600, Loss : 0.1927\n",
      "Epoch 2/2 , Step 567/600, Loss : 0.0530\n",
      "Epoch 2/2 , Step 568/600, Loss : 0.0939\n",
      "Epoch 2/2 , Step 569/600, Loss : 0.1003\n",
      "Epoch 2/2 , Step 570/600, Loss : 0.0669\n",
      "Epoch 2/2 , Step 571/600, Loss : 0.0885\n",
      "Epoch 2/2 , Step 572/600, Loss : 0.0999\n",
      "Epoch 2/2 , Step 573/600, Loss : 0.0549\n",
      "Epoch 2/2 , Step 574/600, Loss : 0.0719\n",
      "Epoch 2/2 , Step 575/600, Loss : 0.1406\n",
      "Epoch 2/2 , Step 576/600, Loss : 0.0813\n",
      "Epoch 2/2 , Step 577/600, Loss : 0.0714\n",
      "Epoch 2/2 , Step 578/600, Loss : 0.0416\n",
      "Epoch 2/2 , Step 579/600, Loss : 0.1049\n",
      "Epoch 2/2 , Step 580/600, Loss : 0.0599\n",
      "Epoch 2/2 , Step 581/600, Loss : 0.0851\n",
      "Epoch 2/2 , Step 582/600, Loss : 0.1258\n",
      "Epoch 2/2 , Step 583/600, Loss : 0.1154\n",
      "Epoch 2/2 , Step 584/600, Loss : 0.0470\n",
      "Epoch 2/2 , Step 585/600, Loss : 0.1021\n",
      "Epoch 2/2 , Step 586/600, Loss : 0.1089\n",
      "Epoch 2/2 , Step 587/600, Loss : 0.0746\n",
      "Epoch 2/2 , Step 588/600, Loss : 0.0279\n",
      "Epoch 2/2 , Step 589/600, Loss : 0.1151\n",
      "Epoch 2/2 , Step 590/600, Loss : 0.1167\n",
      "Epoch 2/2 , Step 591/600, Loss : 0.1484\n",
      "Epoch 2/2 , Step 592/600, Loss : 0.0781\n",
      "Epoch 2/2 , Step 593/600, Loss : 0.1053\n",
      "Epoch 2/2 , Step 594/600, Loss : 0.0711\n",
      "Epoch 2/2 , Step 595/600, Loss : 0.1550\n",
      "Epoch 2/2 , Step 596/600, Loss : 0.0151\n",
      "Epoch 2/2 , Step 597/600, Loss : 0.0914\n",
      "Epoch 2/2 , Step 598/600, Loss : 0.0719\n",
      "Epoch 2/2 , Step 599/600, Loss : 0.1610\n",
      "Epoch 2/2 , Step 600/600, Loss : 0.0809\n"
     ]
    }
   ],
   "source": [
    "## Model training\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (image, label) in enumerate(train_loader):\n",
    "        images = image.reshape(-1, 28*28).to(device)\n",
    "        labels = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_labels = model(images)\n",
    "        l = criterion(pred_labels, labels)\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch%1==0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} , Step {i+1}/{n_total_steps}, Loss : {l.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network with 10000 is 97.25\n"
     ]
    }
   ],
   "source": [
    "## Predictions \n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct =0\n",
    "    n_test_samples = len(test_loader.dataset)\n",
    "\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        images = image.reshape(-1,28*28).to(device)\n",
    "        labels = label.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "\n",
    "        n_correct+= (predicted == labels).sum().item()\n",
    "\n",
    "    acc = n_correct/n_test_samples\n",
    "\n",
    "    print(f\"Accuracy of the network with {n_test_samples} is {acc*100:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
