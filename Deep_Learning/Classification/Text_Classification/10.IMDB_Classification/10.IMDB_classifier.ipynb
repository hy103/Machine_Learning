{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "# allows to have a progress bar in pandas, useful for long processing operations\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 1391524.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "\n",
    "data['label'] = data['sentiment'].progress_apply(transform_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harshayarravarapu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/harshayarravarapu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshayarravarapu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/harshayarravarapu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def rm_link(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "\n",
    "# handle case like \"shut up okay?Im only 10 years old\"\n",
    "# become \"shut up okay Im only 10 years old\"\n",
    "def rm_punct2(text):\n",
    "    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "\n",
    "def rm_html(text):\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove <br /> tags\n",
    "    return re.sub(r'<br />', '', text)\n",
    "\n",
    "\n",
    "def space_bt_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)  # add whitespaces between punctuation\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)  # remove double whitespaces\n",
    "    return s\n",
    "\n",
    "\n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "\n",
    "def spell_correction(text):\n",
    "    # if too slow: return text\n",
    "    return text\n",
    "    # https://pypi.org/project/pyspellchecker/\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            candidate = spell.correction(word)\n",
    "            if candidate is not None:\n",
    "                corrected_text.append(candidate)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "def clean_pipeline(text):\n",
    "    text = text.lower()\n",
    "    no_link = rm_link(text)\n",
    "    no_html = rm_html(no_link)\n",
    "    space_punct = space_bt_punct(no_html)\n",
    "    no_punct = rm_punct2(space_punct)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonasci = rm_nonascii(no_whitespaces)\n",
    "    no_emoji = rm_emoji(no_nonasci)\n",
    "    #spell_corrected = spell_correction(no_emoji)\n",
    "    return no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 7917.41it/s]\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].progress_apply(clean_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def rm_stopwords(text):\n",
    "    return [i for i in text if i not in stopwords]\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    # make sure lemmas does not contains stopwords\n",
    "    return rm_stopwords(lemmas)\n",
    "\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    tokens = tokenize(text)\n",
    "    no_stopwords = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:51<00:00, 970.19it/s] \n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].progress_apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = data.review.values\n",
    "# merge into single variable, separated by whitespaces\n",
    "words = ' '.join(reviews)\n",
    "# obtain list of words\n",
    "words = words.split()\n",
    "# build vocabulary\n",
    "counter = Counter(words)\n",
    "# only keep top 2000 words\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)[:2000]\n",
    "int2word = dict(enumerate(vocab, 2))\n",
    "int2word[0] = '<PAD>'\n",
    "int2word[1] = '<UNK>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 100%|██████████| 50000/50000 [00:00<00:00, 63428.11it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_enc = [[word2int[word] if word in word2int else word2int['<UNK>'] for word in review.split()] for review in tqdm(reviews, desc='encoding')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    # features = np.zeros((len(reviews), seq_length), dtype=int)\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        start_index = max(0, seq_length - len(row))\n",
    "        # if seq_length < len(row) then review will be trimmed\n",
    "        features[i, start_index:] = np.array(row)[:min(seq_length, len(row))]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "seq_length = 128\n",
    "features = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.label.to_numpy()\n",
    "\n",
    "# train test split\n",
    "train_size = .75  # we will use 75% of whole data as train set\n",
    "val_size = .5  # and we will use 50% of test set as validation set\n",
    "\n",
    "# stratify will make sure that train and test set have same distribution of labels\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=1 - train_size, stratify=labels)\n",
    "\n",
    "# split test set into validation and test set\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=val_size, stratify=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# create tensor datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        x = pack_padded_sequence(embedded, lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out).squeeze()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 2\n",
    "output_size = 1\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "model = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  tensor(0.6213, grad_fn=<DivBackward0>)\n",
      "Epoch: 1/40... Loss: 0.676340... Accuracy: 0.565742... Val Loss: 0.621272 val accuracy: 0.662977\n",
      "Model saved at  tensor(0.6146, grad_fn=<DivBackward0>)\n",
      "Epoch: 2/40... Loss: 0.615677... Accuracy: 0.669721... Val Loss: 0.614592 val accuracy: 0.680834\n",
      "Model saved at  tensor(0.5838, grad_fn=<DivBackward0>)\n",
      "Epoch: 3/40... Loss: 0.587790... Accuracy: 0.699844... Val Loss: 0.583782 val accuracy: 0.703816\n",
      "Model saved at  tensor(0.5488, grad_fn=<DivBackward0>)\n",
      "Epoch: 4/40... Loss: 0.556519... Accuracy: 0.725818... Val Loss: 0.548774 val accuracy: 0.736493\n",
      "Model saved at  tensor(0.5380, grad_fn=<DivBackward0>)\n",
      "Epoch: 5/40... Loss: 0.532498... Accuracy: 0.742689... Val Loss: 0.537975 val accuracy: 0.738498\n",
      "Model saved at  tensor(0.5056, grad_fn=<DivBackward0>)\n",
      "Epoch: 6/40... Loss: 0.516622... Accuracy: 0.756787... Val Loss: 0.505560 val accuracy: 0.757296\n",
      "Epoch: 7/40... Loss: 0.500474... Accuracy: 0.768391... Val Loss: 0.517035 val accuracy: 0.747252\n",
      "Model saved at  tensor(0.4806, grad_fn=<DivBackward0>)\n",
      "Epoch: 8/40... Loss: 0.479185... Accuracy: 0.781542... Val Loss: 0.480577 val accuracy: 0.771494\n",
      "Epoch: 9/40... Loss: 0.466220... Accuracy: 0.789430... Val Loss: 0.508751 val accuracy: 0.771889\n",
      "Epoch: 10/40... Loss: 0.450377... Accuracy: 0.801840... Val Loss: 0.483349 val accuracy: 0.778600\n",
      "Model saved at  tensor(0.4797, grad_fn=<DivBackward0>)\n",
      "Epoch: 11/40... Loss: 0.438284... Accuracy: 0.805169... Val Loss: 0.479740 val accuracy: 0.783938\n",
      "Model saved at  tensor(0.4687, grad_fn=<DivBackward0>)\n",
      "Epoch: 12/40... Loss: 0.433425... Accuracy: 0.810758... Val Loss: 0.468676 val accuracy: 0.792950\n",
      "Epoch: 13/40... Loss: 0.426385... Accuracy: 0.816450... Val Loss: 0.470680 val accuracy: 0.786724\n",
      "Model saved at  tensor(0.4681, grad_fn=<DivBackward0>)\n",
      "Epoch: 14/40... Loss: 0.409431... Accuracy: 0.824748... Val Loss: 0.468094 val accuracy: 0.786975\n",
      "Model saved at  tensor(0.4381, grad_fn=<DivBackward0>)\n",
      "Epoch: 15/40... Loss: 0.406863... Accuracy: 0.828056... Val Loss: 0.438112 val accuracy: 0.812075\n",
      "Model saved at  tensor(0.4254, grad_fn=<DivBackward0>)\n",
      "Epoch: 16/40... Loss: 0.395616... Accuracy: 0.833209... Val Loss: 0.425413 val accuracy: 0.810321\n",
      "Epoch: 17/40... Loss: 0.386429... Accuracy: 0.837352... Val Loss: 0.448880 val accuracy: 0.795486\n",
      "Epoch: 18/40... Loss: 0.378105... Accuracy: 0.842367... Val Loss: 0.434067 val accuracy: 0.814778\n",
      "Epoch: 19/40... Loss: 0.374124... Accuracy: 0.845462... Val Loss: 0.437324 val accuracy: 0.802509\n",
      "Model saved at  tensor(0.4134, grad_fn=<DivBackward0>)\n",
      "Epoch: 20/40... Loss: 0.367787... Accuracy: 0.847846... Val Loss: 0.413365 val accuracy: 0.820373\n",
      "Epoch: 21/40... Loss: 0.361798... Accuracy: 0.849627... Val Loss: 0.443726 val accuracy: 0.814945\n",
      "Epoch: 22/40... Loss: 0.354356... Accuracy: 0.855340... Val Loss: 0.423515 val accuracy: 0.810799\n",
      "Epoch: 23/40... Loss: 0.352805... Accuracy: 0.856670... Val Loss: 0.446938 val accuracy: 0.824921\n",
      "Model saved at  tensor(0.4118, grad_fn=<DivBackward0>)\n",
      "Epoch: 24/40... Loss: 0.347832... Accuracy: 0.860642... Val Loss: 0.411786 val accuracy: 0.831921\n",
      "Model saved at  tensor(0.4112, grad_fn=<DivBackward0>)\n",
      "Epoch: 25/40... Loss: 0.334244... Accuracy: 0.864608... Val Loss: 0.411235 val accuracy: 0.824830\n",
      "Model saved at  tensor(0.4003, grad_fn=<DivBackward0>)\n",
      "Epoch: 26/40... Loss: 0.325063... Accuracy: 0.870005... Val Loss: 0.400302 val accuracy: 0.828421\n",
      "Epoch: 27/40... Loss: 0.327085... Accuracy: 0.867660... Val Loss: 0.423130 val accuracy: 0.828983\n",
      "Epoch: 28/40... Loss: 0.319505... Accuracy: 0.872263... Val Loss: 0.431892 val accuracy: 0.831215\n",
      "Epoch: 29/40... Loss: 0.310136... Accuracy: 0.875978... Val Loss: 0.454538 val accuracy: 0.799472\n",
      "Epoch: 30/40... Loss: 0.308833... Accuracy: 0.875633... Val Loss: 0.424480 val accuracy: 0.827859\n",
      "Epoch: 31/40... Loss: 0.301058... Accuracy: 0.881230... Val Loss: 0.413459 val accuracy: 0.834867\n",
      "Epoch: 32/40... Loss: 0.292891... Accuracy: 0.884423... Val Loss: 0.442769 val accuracy: 0.837190\n",
      "Epoch: 33/40... Loss: 0.293391... Accuracy: 0.883948... Val Loss: 0.456993 val accuracy: 0.798674\n",
      "Epoch: 34/40... Loss: 0.285357... Accuracy: 0.888538... Val Loss: 0.420326 val accuracy: 0.836628\n",
      "Epoch: 35/40... Loss: 0.272449... Accuracy: 0.895821... Val Loss: 0.442180 val accuracy: 0.833599\n",
      "Epoch: 36/40... Loss: 0.268922... Accuracy: 0.894849... Val Loss: 0.476141 val accuracy: 0.812470\n",
      "Epoch: 37/40... Loss: 0.263250... Accuracy: 0.898402... Val Loss: 0.450157 val accuracy: 0.830084\n",
      "Epoch: 38/40... Loss: 0.259148... Accuracy: 0.900112... Val Loss: 0.434988 val accuracy: 0.830646\n",
      "Epoch: 39/40... Loss: 0.247438... Accuracy: 0.905466... Val Loss: 0.437164 val accuracy: 0.813646\n",
      "Epoch: 40/40... Loss: 0.242366... Accuracy: 0.907701... Val Loss: 0.457694 val accuracy: 0.823714\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(output, labels):\n",
    "    pred = (output > 0.5).float()\n",
    "    \n",
    "    correct = pred.eq(labels.view_as(pred)).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float(\"Inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        lengths = torch.clamp(inputs.sum(dim=1), max=seq_length)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(inputs, lengths)\n",
    "\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        accuracy = calculate_accuracy(output, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy\n",
    "\n",
    "    #wandb.log({'Training Loss': total_loss / len(train_loader), 'Training Accuracy': total_acc / len(train_loader)})\n",
    "\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        lengths = torch.clamp(inputs.sum(dim=1), max=seq_length)\n",
    "        output = model(inputs, lengths)\n",
    "        val_loss = criterion(output, labels.float())\n",
    "\n",
    "        accuracy = calculate_accuracy(output, labels)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += accuracy\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"model_2.pth\")\n",
    "        print(\"Model saved at \", best_val_loss)\n",
    "        \n",
    "    #wandb.log({'Validation Loss': total_val_loss / len(valid_loader), 'Validation Accuracy': total_val_acc / len(valid_loader)})\n",
    "\n",
    "    model.train()\n",
    "    print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "          \"Loss: {:.6f}...\".format(total_loss / len(train_loader)),\n",
    "          \"Accuracy: {:.6f}...\".format(total_acc / len(train_loader)),\n",
    "          \"Val Loss: {:.6f}\".format(total_val_loss / len(valid_loader)),\n",
    "         \"val accuracy: {:.6f}\".format(total_val_acc / len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.460258\n",
      "Test Accuracy: 0.822119\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        lengths = torch.clamp(inputs.sum(dim=1), max=seq_length)\n",
    "\n",
    "        output = model(inputs, lengths)\n",
    "        loss = criterion(output, labels.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        accuracy = calculate_accuracy(output, labels)\n",
    "        test_acc += accuracy\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "avg_test_acc = test_acc / len(test_loader)\n",
    "\n",
    "print(f'Test Loss: {avg_test_loss:.6f}')\n",
    "print(f'Test Accuracy: {avg_test_acc:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
