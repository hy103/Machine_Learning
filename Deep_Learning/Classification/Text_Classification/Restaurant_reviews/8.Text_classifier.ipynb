{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter='\\t')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  1000 non-null   object\n",
      " 1   Liked   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "import re\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    new_row = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    new_row = new_row.lower()\n",
    "    new_row = new_row.split()\n",
    "    clean_row = [ps.stem(word) for word in new_row if word not in (stopwords.words('english'))]\n",
    "    clean_row = ' '.join(clean_row)\n",
    "    corpus.append(clean_row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into numeric vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df = 3, max_df = 0.6)\n",
    "text_vectors = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 467), (200, 467), (800,), (200,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(text_vectors, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 467\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size =32\n",
    "hidden_size = 500\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textclassifier(\n",
      "  (layer1): Linear(in_features=467, out_features=500, bias=True)\n",
      "  (layer2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (layer3): Linear(in_features=500, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Textclassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Textclassifier, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        output = F.relu(self.layer1(x))\n",
    "        #print(output.shape)\n",
    "        output = F.relu(self.layer2(output))\n",
    "        #print(output.shape)\n",
    "        output = self.layer3(output)\n",
    "        #print(output.shape)\n",
    "        #print(F.log_softmax(output, dim=1).shape)\n",
    "        return F.log_softmax(output, dim=1)\n",
    "    \n",
    "model = Textclassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.tensor(np.array(X_train), dtype = torch.float32, requires_grad=True)\n",
    "#y_train = torch.tensor(np.array(y_train).reshape(-1,1), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800, 467]), torch.Size([800]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 0.6934576630592346\n",
      "Epoch : 1, Loss : 0.6844948530197144\n",
      "Epoch : 2, Loss : 0.5731980204582214\n",
      "Epoch : 3, Loss : 0.3894008994102478\n",
      "Epoch : 4, Loss : 0.24066786468029022\n",
      "Epoch : 5, Loss : 0.17016109824180603\n",
      "Epoch : 6, Loss : 0.11300583928823471\n",
      "Epoch : 7, Loss : 0.08748533576726913\n",
      "Epoch : 8, Loss : 0.06843941658735275\n",
      "Epoch : 9, Loss : 0.05187882483005524\n",
      "Epoch : 10, Loss : 0.048570141196250916\n",
      "Epoch : 11, Loss : 0.04335484653711319\n",
      "Epoch : 12, Loss : 0.034375496208667755\n",
      "Epoch : 13, Loss : 0.029017005115747452\n",
      "Epoch : 14, Loss : 0.026481665670871735\n",
      "Epoch : 15, Loss : 0.026256414130330086\n",
      "Epoch : 16, Loss : 0.0251470897346735\n",
      "Epoch : 17, Loss : 0.021195316687226295\n",
      "Epoch : 18, Loss : 0.021703390404582024\n",
      "Epoch : 19, Loss : 0.023493222892284393\n",
      "Epoch : 20, Loss : 0.023327112197875977\n",
      "Epoch : 21, Loss : 0.021438205614686012\n",
      "Epoch : 22, Loss : 0.0194769948720932\n",
      "Epoch : 23, Loss : 0.021559065207839012\n",
      "Epoch : 24, Loss : 0.021221280097961426\n",
      "Epoch : 25, Loss : 0.01966155506670475\n",
      "Epoch : 26, Loss : 0.020373554900288582\n",
      "Epoch : 27, Loss : 0.02040925994515419\n",
      "Epoch : 28, Loss : 0.02030944637954235\n",
      "Epoch : 29, Loss : 0.019444961100816727\n",
      "Epoch : 30, Loss : 0.020292330533266068\n",
      "Epoch : 31, Loss : 0.019502051174640656\n",
      "Epoch : 32, Loss : 0.01987857185304165\n",
      "Epoch : 33, Loss : 0.019310465082526207\n",
      "Epoch : 34, Loss : 0.019757144153118134\n",
      "Epoch : 35, Loss : 0.019412539899349213\n",
      "Epoch : 36, Loss : 0.01945466175675392\n",
      "Epoch : 37, Loss : 0.019358579069375992\n",
      "Epoch : 38, Loss : 0.019210657104849815\n",
      "Epoch : 39, Loss : 0.019528040662407875\n",
      "Epoch : 40, Loss : 0.01906662993133068\n",
      "Epoch : 41, Loss : 0.019321158528327942\n",
      "Epoch : 42, Loss : 0.019189929589629173\n",
      "Epoch : 43, Loss : 0.01919153891503811\n",
      "Epoch : 44, Loss : 0.019089482724666595\n",
      "Epoch : 45, Loss : 0.019156642258167267\n",
      "Epoch : 46, Loss : 0.019158072769641876\n",
      "Epoch : 47, Loss : 0.019012831151485443\n",
      "Epoch : 48, Loss : 0.01916731894016266\n",
      "Epoch : 49, Loss : 0.01903531700372696\n",
      "Epoch : 50, Loss : 0.019056977704167366\n",
      "Epoch : 51, Loss : 0.019078398123383522\n",
      "Epoch : 52, Loss : 0.019029738381505013\n",
      "Epoch : 53, Loss : 0.01903460919857025\n",
      "Epoch : 54, Loss : 0.01902979239821434\n",
      "Epoch : 55, Loss : 0.01904413476586342\n",
      "Epoch : 56, Loss : 0.018998730927705765\n",
      "Epoch : 57, Loss : 0.019017847254872322\n",
      "Epoch : 58, Loss : 0.019023552536964417\n",
      "Epoch : 59, Loss : 0.01899408921599388\n",
      "Epoch : 60, Loss : 0.019007649272680283\n",
      "Epoch : 61, Loss : 0.018994010984897614\n",
      "Epoch : 62, Loss : 0.019001062959432602\n",
      "Epoch : 63, Loss : 0.018998632207512856\n",
      "Epoch : 64, Loss : 0.01897503063082695\n",
      "Epoch : 65, Loss : 0.01900017261505127\n",
      "Epoch : 66, Loss : 0.018991373479366302\n",
      "Epoch : 67, Loss : 0.018973207101225853\n",
      "Epoch : 68, Loss : 0.018992016091942787\n",
      "Epoch : 69, Loss : 0.018981462344527245\n",
      "Epoch : 70, Loss : 0.01897653378546238\n",
      "Epoch : 71, Loss : 0.018984558060765266\n",
      "Epoch : 72, Loss : 0.01897531934082508\n",
      "Epoch : 73, Loss : 0.018977779895067215\n",
      "Epoch : 74, Loss : 0.018979283049702644\n",
      "Epoch : 75, Loss : 0.018972143530845642\n",
      "Epoch : 76, Loss : 0.01897577941417694\n",
      "Epoch : 77, Loss : 0.018975436687469482\n",
      "Epoch : 78, Loss : 0.018972648307681084\n",
      "Epoch : 79, Loss : 0.018972069025039673\n",
      "Epoch : 80, Loss : 0.018972935155034065\n",
      "Epoch : 81, Loss : 0.018973302096128464\n",
      "Epoch : 82, Loss : 0.018969282507896423\n",
      "Epoch : 83, Loss : 0.018971288576722145\n",
      "Epoch : 84, Loss : 0.018972402438521385\n",
      "Epoch : 85, Loss : 0.018968477845191956\n",
      "Epoch : 86, Loss : 0.018970295786857605\n",
      "Epoch : 87, Loss : 0.018970370292663574\n",
      "Epoch : 88, Loss : 0.018969377502799034\n",
      "Epoch : 89, Loss : 0.01896929182112217\n",
      "Epoch : 90, Loss : 0.018968481570482254\n",
      "Epoch : 91, Loss : 0.018969926983118057\n",
      "Epoch : 92, Loss : 0.01896861009299755\n",
      "Epoch : 93, Loss : 0.018967846408486366\n",
      "Epoch : 94, Loss : 0.01896916702389717\n",
      "Epoch : 95, Loss : 0.01896815374493599\n",
      "Epoch : 96, Loss : 0.01896826922893524\n",
      "Epoch : 97, Loss : 0.018968068063259125\n",
      "Epoch : 98, Loss : 0.018967818468809128\n",
      "Epoch : 99, Loss : 0.018968142569065094\n",
      "Epoch : 100, Loss : 0.018967894837260246\n",
      "Epoch : 101, Loss : 0.01896773651242256\n",
      "Epoch : 102, Loss : 0.01896738074719906\n",
      "Epoch : 103, Loss : 0.018967807292938232\n",
      "Epoch : 104, Loss : 0.018967794254422188\n",
      "Epoch : 105, Loss : 0.018967391923069954\n",
      "Epoch : 106, Loss : 0.01896754465997219\n",
      "Epoch : 107, Loss : 0.01896718330681324\n",
      "Epoch : 108, Loss : 0.01896733231842518\n",
      "Epoch : 109, Loss : 0.018967442214488983\n",
      "Epoch : 110, Loss : 0.018967358395457268\n",
      "Epoch : 111, Loss : 0.01896732673048973\n",
      "Epoch : 112, Loss : 0.01896720752120018\n",
      "Epoch : 113, Loss : 0.018967146053910255\n",
      "Epoch : 114, Loss : 0.01896701753139496\n",
      "Epoch : 115, Loss : 0.018967155367136\n",
      "Epoch : 116, Loss : 0.018967056646943092\n",
      "Epoch : 117, Loss : 0.01896713860332966\n",
      "Epoch : 118, Loss : 0.01896717958152294\n",
      "Epoch : 119, Loss : 0.01896708272397518\n",
      "Epoch : 120, Loss : 0.018967166543006897\n",
      "Epoch : 121, Loss : 0.018967078998684883\n",
      "Epoch : 122, Loss : 0.01896708272397518\n",
      "Epoch : 123, Loss : 0.018967069685459137\n",
      "Epoch : 124, Loss : 0.01896706223487854\n",
      "Epoch : 125, Loss : 0.018967071548104286\n",
      "Epoch : 126, Loss : 0.01896713115274906\n",
      "Epoch : 127, Loss : 0.018967216834425926\n",
      "Epoch : 128, Loss : 0.018967395648360252\n",
      "Epoch : 129, Loss : 0.01896778866648674\n",
      "Epoch : 130, Loss : 0.0189684871584177\n",
      "Epoch : 131, Loss : 0.01897004060447216\n",
      "Epoch : 132, Loss : 0.018973106518387794\n",
      "Epoch : 133, Loss : 0.01897999458014965\n",
      "Epoch : 134, Loss : 0.018994461745023727\n",
      "Epoch : 135, Loss : 0.019028546288609505\n",
      "Epoch : 136, Loss : 0.019092796370387077\n",
      "Epoch : 137, Loss : 0.01923798769712448\n",
      "Epoch : 138, Loss : 0.01941174454987049\n",
      "Epoch : 139, Loss : 0.019707409664988518\n",
      "Epoch : 140, Loss : 0.019601043313741684\n",
      "Epoch : 141, Loss : 0.01942250318825245\n",
      "Epoch : 142, Loss : 0.019087444990873337\n",
      "Epoch : 143, Loss : 0.018969623371958733\n",
      "Epoch : 144, Loss : 0.0190424844622612\n",
      "Epoch : 145, Loss : 0.019214443862438202\n",
      "Epoch : 146, Loss : 0.019380206242203712\n",
      "Epoch : 147, Loss : 0.01926344819366932\n",
      "Epoch : 148, Loss : 0.019100235775113106\n",
      "Epoch : 149, Loss : 0.018976448103785515\n",
      "Epoch : 150, Loss : 0.01899608224630356\n",
      "Epoch : 151, Loss : 0.019106557592749596\n",
      "Epoch : 152, Loss : 0.019176004454493523\n",
      "Epoch : 153, Loss : 0.019170185551047325\n",
      "Epoch : 154, Loss : 0.01904595084488392\n",
      "Epoch : 155, Loss : 0.018971296027302742\n",
      "Epoch : 156, Loss : 0.018988529220223427\n",
      "Epoch : 157, Loss : 0.019054977223277092\n",
      "Epoch : 158, Loss : 0.01911081001162529\n",
      "Epoch : 159, Loss : 0.019078942015767097\n",
      "Epoch : 160, Loss : 0.01901613175868988\n",
      "Epoch : 161, Loss : 0.01896960288286209\n",
      "Epoch : 162, Loss : 0.01898111030459404\n",
      "Epoch : 163, Loss : 0.019024774432182312\n",
      "Epoch : 164, Loss : 0.019048508256673813\n",
      "Epoch : 165, Loss : 0.019043222069740295\n",
      "Epoch : 166, Loss : 0.019001983106136322\n",
      "Epoch : 167, Loss : 0.018970994278788567\n",
      "Epoch : 168, Loss : 0.018971242010593414\n",
      "Epoch : 169, Loss : 0.018992671743035316\n",
      "Epoch : 170, Loss : 0.019014663994312286\n",
      "Epoch : 171, Loss : 0.01901961863040924\n",
      "Epoch : 172, Loss : 0.0190130602568388\n",
      "Epoch : 173, Loss : 0.01898953504860401\n",
      "Epoch : 174, Loss : 0.01897231489419937\n",
      "Epoch : 175, Loss : 0.018968457356095314\n",
      "Epoch : 176, Loss : 0.01897561363875866\n",
      "Epoch : 177, Loss : 0.018988385796546936\n",
      "Epoch : 178, Loss : 0.018994683399796486\n",
      "Epoch : 179, Loss : 0.018991978839039803\n",
      "Epoch : 180, Loss : 0.018982209265232086\n",
      "Epoch : 181, Loss : 0.0189737007021904\n",
      "Epoch : 182, Loss : 0.018967729061841965\n",
      "Epoch : 183, Loss : 0.0189676471054554\n",
      "Epoch : 184, Loss : 0.018971730023622513\n",
      "Epoch : 185, Loss : 0.018976405262947083\n",
      "Epoch : 186, Loss : 0.018980251625180244\n",
      "Epoch : 187, Loss : 0.01898166351020336\n",
      "Epoch : 188, Loss : 0.018981361761689186\n",
      "Epoch : 189, Loss : 0.018978659063577652\n",
      "Epoch : 190, Loss : 0.018975859507918358\n",
      "Epoch : 191, Loss : 0.018972259014844894\n",
      "Epoch : 192, Loss : 0.018969690427184105\n",
      "Epoch : 193, Loss : 0.018968038260936737\n",
      "Epoch : 194, Loss : 0.018967123702168465\n",
      "Epoch : 195, Loss : 0.018966877833008766\n",
      "Epoch : 196, Loss : 0.018967155367136\n",
      "Epoch : 197, Loss : 0.01896767131984234\n",
      "Epoch : 198, Loss : 0.018968528136610985\n",
      "Epoch : 199, Loss : 0.018970143049955368\n",
      "Epoch : 200, Loss : 0.018972402438521385\n",
      "Epoch : 201, Loss : 0.018976561725139618\n",
      "Epoch : 202, Loss : 0.01898341067135334\n",
      "Epoch : 203, Loss : 0.018996134400367737\n",
      "Epoch : 204, Loss : 0.01901676505804062\n",
      "Epoch : 205, Loss : 0.019066767767071724\n",
      "Epoch : 206, Loss : 0.019148170948028564\n",
      "Epoch : 207, Loss : 0.01934044063091278\n",
      "Epoch : 208, Loss : 0.01951228268444538\n",
      "Epoch : 209, Loss : 0.01980065181851387\n",
      "Epoch : 210, Loss : 0.019546030089259148\n",
      "Epoch : 211, Loss : 0.019315673038363457\n",
      "Epoch : 212, Loss : 0.019059021025896072\n",
      "Epoch : 213, Loss : 0.018969818949699402\n",
      "Epoch : 214, Loss : 0.019006438553333282\n",
      "Epoch : 215, Loss : 0.01911073550581932\n",
      "Epoch : 216, Loss : 0.019236255437135696\n",
      "Epoch : 217, Loss : 0.019226256757974625\n",
      "Epoch : 218, Loss : 0.01916886493563652\n",
      "Epoch : 219, Loss : 0.019034795463085175\n",
      "Epoch : 220, Loss : 0.018970785662531853\n",
      "Epoch : 221, Loss : 0.01899011991918087\n",
      "Epoch : 222, Loss : 0.019054818898439407\n",
      "Epoch : 223, Loss : 0.019120192155241966\n",
      "Epoch : 224, Loss : 0.01908859983086586\n",
      "Epoch : 225, Loss : 0.019031455740332603\n",
      "Epoch : 226, Loss : 0.018979158252477646\n",
      "Epoch : 227, Loss : 0.01896972581744194\n",
      "Epoch : 228, Loss : 0.018995214253664017\n",
      "Epoch : 229, Loss : 0.01902472972869873\n",
      "Epoch : 230, Loss : 0.019038965925574303\n",
      "Epoch : 231, Loss : 0.019016996026039124\n",
      "Epoch : 232, Loss : 0.018987935036420822\n",
      "Epoch : 233, Loss : 0.01896880567073822\n",
      "Epoch : 234, Loss : 0.018971778452396393\n",
      "Epoch : 235, Loss : 0.018988249823451042\n",
      "Epoch : 236, Loss : 0.01900281012058258\n",
      "Epoch : 237, Loss : 0.019005687907338142\n",
      "Epoch : 238, Loss : 0.018997807055711746\n",
      "Epoch : 239, Loss : 0.01898822747170925\n",
      "Epoch : 240, Loss : 0.018977003172039986\n",
      "Epoch : 241, Loss : 0.01896962709724903\n",
      "Epoch : 242, Loss : 0.01896725594997406\n",
      "Epoch : 243, Loss : 0.01896989904344082\n",
      "Epoch : 244, Loss : 0.01897488720715046\n",
      "Epoch : 245, Loss : 0.018979957327246666\n",
      "Epoch : 246, Loss : 0.01898309402167797\n",
      "Epoch : 247, Loss : 0.01898319087922573\n",
      "Epoch : 248, Loss : 0.018981875851750374\n",
      "Epoch : 249, Loss : 0.01897798478603363\n",
      "Epoch : 250, Loss : 0.018973927944898605\n",
      "Epoch : 251, Loss : 0.018970398232340813\n",
      "Epoch : 252, Loss : 0.01896820217370987\n",
      "Epoch : 253, Loss : 0.018967119976878166\n",
      "Epoch : 254, Loss : 0.01896722987294197\n",
      "Epoch : 255, Loss : 0.018968090415000916\n",
      "Epoch : 256, Loss : 0.018969371914863586\n",
      "Epoch : 257, Loss : 0.018971368670463562\n",
      "Epoch : 258, Loss : 0.01897384226322174\n",
      "Epoch : 259, Loss : 0.018977420404553413\n",
      "Epoch : 260, Loss : 0.01898094266653061\n",
      "Epoch : 261, Loss : 0.01898583583533764\n",
      "Epoch : 262, Loss : 0.018990300595760345\n",
      "Epoch : 263, Loss : 0.01899719424545765\n",
      "Epoch : 264, Loss : 0.019005393609404564\n",
      "Epoch : 265, Loss : 0.01902190037071705\n",
      "Epoch : 266, Loss : 0.019039323553442955\n",
      "Epoch : 267, Loss : 0.019071435555815697\n",
      "Epoch : 268, Loss : 0.019098972901701927\n",
      "Epoch : 269, Loss : 0.0191558338701725\n",
      "Epoch : 270, Loss : 0.01918245479464531\n",
      "Epoch : 271, Loss : 0.019241325557231903\n",
      "Epoch : 272, Loss : 0.019223010167479515\n",
      "Epoch : 273, Loss : 0.019230082631111145\n",
      "Epoch : 274, Loss : 0.01915423758327961\n",
      "Epoch : 275, Loss : 0.019100848585367203\n",
      "Epoch : 276, Loss : 0.01903422363102436\n",
      "Epoch : 277, Loss : 0.018993645906448364\n",
      "Epoch : 278, Loss : 0.018970977514982224\n",
      "Epoch : 279, Loss : 0.01896773651242256\n",
      "Epoch : 280, Loss : 0.01897869072854519\n",
      "Epoch : 281, Loss : 0.018998365849256516\n",
      "Epoch : 282, Loss : 0.019024558365345\n",
      "Epoch : 283, Loss : 0.019043905660510063\n",
      "Epoch : 284, Loss : 0.019065208733081818\n",
      "Epoch : 285, Loss : 0.019063571467995644\n",
      "Epoch : 286, Loss : 0.019062193110585213\n",
      "Epoch : 287, Loss : 0.019039606675505638\n",
      "Epoch : 288, Loss : 0.01902039349079132\n",
      "Epoch : 289, Loss : 0.01899687573313713\n",
      "Epoch : 290, Loss : 0.01898098550736904\n",
      "Epoch : 291, Loss : 0.018971050158143044\n",
      "Epoch : 292, Loss : 0.018967105075716972\n",
      "Epoch : 293, Loss : 0.018968097865581512\n",
      "Epoch : 294, Loss : 0.018972357735037804\n",
      "Epoch : 295, Loss : 0.018978659063577652\n",
      "Epoch : 296, Loss : 0.018984438851475716\n",
      "Epoch : 297, Loss : 0.018990246579051018\n",
      "Epoch : 298, Loss : 0.01899315044283867\n",
      "Epoch : 299, Loss : 0.01899593695998192\n",
      "Epoch : 300, Loss : 0.018994668498635292\n",
      "Epoch : 301, Loss : 0.01899384893476963\n",
      "Epoch : 302, Loss : 0.018990499898791313\n",
      "Epoch : 303, Loss : 0.018988192081451416\n",
      "Epoch : 304, Loss : 0.018984412774443626\n",
      "Epoch : 305, Loss : 0.01898179203271866\n",
      "Epoch : 306, Loss : 0.018978914245963097\n",
      "Epoch : 307, Loss : 0.018977057188749313\n",
      "Epoch : 308, Loss : 0.018975242972373962\n",
      "Epoch : 309, Loss : 0.01897415705025196\n",
      "Epoch : 310, Loss : 0.01897326484322548\n",
      "Epoch : 311, Loss : 0.018972957506775856\n",
      "Epoch : 312, Loss : 0.018972845748066902\n",
      "Epoch : 313, Loss : 0.01897331327199936\n",
      "Epoch : 314, Loss : 0.018974076956510544\n",
      "Epoch : 315, Loss : 0.018975744023919106\n",
      "Epoch : 316, Loss : 0.018978025764226913\n",
      "Epoch : 317, Loss : 0.01898217760026455\n",
      "Epoch : 318, Loss : 0.018987737596035004\n",
      "Epoch : 319, Loss : 0.01899799145758152\n",
      "Epoch : 320, Loss : 0.019011391326785088\n",
      "Epoch : 321, Loss : 0.019037069752812386\n",
      "Epoch : 322, Loss : 0.019066987559199333\n",
      "Epoch : 323, Loss : 0.019126025959849358\n",
      "Epoch : 324, Loss : 0.019177095964550972\n",
      "Epoch : 325, Loss : 0.01927446573972702\n",
      "Epoch : 326, Loss : 0.019287174567580223\n",
      "Epoch : 327, Loss : 0.019321031868457794\n",
      "Epoch : 328, Loss : 0.019224204123020172\n",
      "Epoch : 329, Loss : 0.019152827560901642\n",
      "Epoch : 330, Loss : 0.019053256139159203\n",
      "Epoch : 331, Loss : 0.018994012847542763\n",
      "Epoch : 332, Loss : 0.0189680103212595\n",
      "Epoch : 333, Loss : 0.018974555656313896\n",
      "Epoch : 334, Loss : 0.0190032497048378\n",
      "Epoch : 335, Loss : 0.019035089761018753\n",
      "Epoch : 336, Loss : 0.019064122810959816\n",
      "Epoch : 337, Loss : 0.01906006596982479\n",
      "Epoch : 338, Loss : 0.0190462376922369\n",
      "Epoch : 339, Loss : 0.019012412056326866\n",
      "Epoch : 340, Loss : 0.018985465168952942\n",
      "Epoch : 341, Loss : 0.018969332799315453\n",
      "Epoch : 342, Loss : 0.018968235701322556\n",
      "Epoch : 343, Loss : 0.018978416919708252\n",
      "Epoch : 344, Loss : 0.01899191178381443\n",
      "Epoch : 345, Loss : 0.019003309309482574\n",
      "Epoch : 346, Loss : 0.019003668799996376\n",
      "Epoch : 347, Loss : 0.01899855025112629\n",
      "Epoch : 348, Loss : 0.018986385315656662\n",
      "Epoch : 349, Loss : 0.01897565834224224\n",
      "Epoch : 350, Loss : 0.018968624994158745\n",
      "Epoch : 351, Loss : 0.018967097625136375\n",
      "Epoch : 352, Loss : 0.01897008717060089\n",
      "Epoch : 353, Loss : 0.018975170329213142\n",
      "Epoch : 354, Loss : 0.018980227410793304\n",
      "Epoch : 355, Loss : 0.018982505425810814\n",
      "Epoch : 356, Loss : 0.01898268051445484\n",
      "Epoch : 357, Loss : 0.018979649990797043\n",
      "Epoch : 358, Loss : 0.018975945189595222\n",
      "Epoch : 359, Loss : 0.018971862271428108\n",
      "Epoch : 360, Loss : 0.01896887831389904\n",
      "Epoch : 361, Loss : 0.01896722987294197\n",
      "Epoch : 362, Loss : 0.01896696910262108\n",
      "Epoch : 363, Loss : 0.018967773765325546\n",
      "Epoch : 364, Loss : 0.018969185650348663\n",
      "Epoch : 365, Loss : 0.018970834091305733\n",
      "Epoch : 366, Loss : 0.01897224597632885\n",
      "Epoch : 367, Loss : 0.018973493948578835\n",
      "Epoch : 368, Loss : 0.018974144011735916\n",
      "Epoch : 369, Loss : 0.018974680453538895\n",
      "Epoch : 370, Loss : 0.018974656239151955\n",
      "Epoch : 371, Loss : 0.018974732607603073\n",
      "Epoch : 372, Loss : 0.01897447369992733\n",
      "Epoch : 373, Loss : 0.01897450163960457\n",
      "Epoch : 374, Loss : 0.018974367529153824\n",
      "Epoch : 375, Loss : 0.018974676728248596\n",
      "Epoch : 376, Loss : 0.018974974751472473\n",
      "Epoch : 377, Loss : 0.018975920975208282\n",
      "Epoch : 378, Loss : 0.018977008759975433\n",
      "Epoch : 379, Loss : 0.018979201093316078\n",
      "Epoch : 380, Loss : 0.018981827422976494\n",
      "Epoch : 381, Loss : 0.018986623734235764\n",
      "Epoch : 382, Loss : 0.018992656841874123\n",
      "Epoch : 383, Loss : 0.019003478810191154\n",
      "Epoch : 384, Loss : 0.019016114994883537\n",
      "Epoch : 385, Loss : 0.019039666280150414\n",
      "Epoch : 386, Loss : 0.01906409114599228\n",
      "Epoch : 387, Loss : 0.01911131851375103\n",
      "Epoch : 388, Loss : 0.019148100167512894\n",
      "Epoch : 389, Loss : 0.019222820177674294\n",
      "Epoch : 390, Loss : 0.01924220845103264\n",
      "Epoch : 391, Loss : 0.019298819825053215\n",
      "Epoch : 392, Loss : 0.019264597445726395\n",
      "Epoch : 393, Loss : 0.019259072840213776\n",
      "Epoch : 394, Loss : 0.019171984866261482\n",
      "Epoch : 395, Loss : 0.019112568348646164\n",
      "Epoch : 396, Loss : 0.019037794321775436\n",
      "Epoch : 397, Loss : 0.01898951828479767\n",
      "Epoch : 398, Loss : 0.01896936073899269\n",
      "Epoch : 399, Loss : 0.018972275778651237\n",
      "Epoch : 400, Loss : 0.018993550911545753\n",
      "Epoch : 401, Loss : 0.019016394391655922\n",
      "Epoch : 402, Loss : 0.01903633400797844\n",
      "Epoch : 403, Loss : 0.019035156816244125\n",
      "Epoch : 404, Loss : 0.019023682922124863\n",
      "Epoch : 405, Loss : 0.018998315557837486\n",
      "Epoch : 406, Loss : 0.018977971747517586\n",
      "Epoch : 407, Loss : 0.018968094140291214\n",
      "Epoch : 408, Loss : 0.018969392403960228\n",
      "Epoch : 409, Loss : 0.018978511914610863\n",
      "Epoch : 410, Loss : 0.01898820698261261\n",
      "Epoch : 411, Loss : 0.018994400277733803\n",
      "Epoch : 412, Loss : 0.018992889672517776\n",
      "Epoch : 413, Loss : 0.0189862921833992\n",
      "Epoch : 414, Loss : 0.018976973369717598\n",
      "Epoch : 415, Loss : 0.018969858065247536\n",
      "Epoch : 416, Loss : 0.018967151641845703\n",
      "Epoch : 417, Loss : 0.01896861009299755\n",
      "Epoch : 418, Loss : 0.018972551450133324\n",
      "Epoch : 419, Loss : 0.018976494669914246\n",
      "Epoch : 420, Loss : 0.018978789448738098\n",
      "Epoch : 421, Loss : 0.018978124484419823\n",
      "Epoch : 422, Loss : 0.018975509330630302\n",
      "Epoch : 423, Loss : 0.01897197961807251\n",
      "Epoch : 424, Loss : 0.018968965858221054\n",
      "Epoch : 425, Loss : 0.018967280164361\n",
      "Epoch : 426, Loss : 0.01896706596016884\n",
      "Epoch : 427, Loss : 0.018967999145388603\n",
      "Epoch : 428, Loss : 0.018969498574733734\n",
      "Epoch : 429, Loss : 0.018970923498272896\n",
      "Epoch : 430, Loss : 0.018971789628267288\n",
      "Epoch : 431, Loss : 0.018972041085362434\n",
      "Epoch : 432, Loss : 0.01897158846259117\n",
      "Epoch : 433, Loss : 0.01897079311311245\n",
      "Epoch : 434, Loss : 0.018969686701893806\n",
      "Epoch : 435, Loss : 0.018968693912029266\n",
      "Epoch : 436, Loss : 0.018967827782034874\n",
      "Epoch : 437, Loss : 0.018967265263199806\n",
      "Epoch : 438, Loss : 0.0189669206738472\n",
      "Epoch : 439, Loss : 0.01896684430539608\n",
      "Epoch : 440, Loss : 0.018966905772686005\n",
      "Epoch : 441, Loss : 0.018967119976878166\n",
      "Epoch : 442, Loss : 0.018967395648360252\n",
      "Epoch : 443, Loss : 0.018967725336551666\n",
      "Epoch : 444, Loss : 0.018968110904097557\n",
      "Epoch : 445, Loss : 0.01896851696074009\n",
      "Epoch : 446, Loss : 0.018969032913446426\n",
      "Epoch : 447, Loss : 0.018969589844346046\n",
      "Epoch : 448, Loss : 0.018970394507050514\n",
      "Epoch : 449, Loss : 0.018971361219882965\n",
      "Epoch : 450, Loss : 0.0189728494733572\n",
      "Epoch : 451, Loss : 0.018974723294377327\n",
      "Epoch : 452, Loss : 0.018977798521518707\n",
      "Epoch : 453, Loss : 0.0189818162471056\n",
      "Epoch : 454, Loss : 0.018988728523254395\n",
      "Epoch : 455, Loss : 0.01899762824177742\n",
      "Epoch : 456, Loss : 0.019013749435544014\n",
      "Epoch : 457, Loss : 0.01903335563838482\n",
      "Epoch : 458, Loss : 0.019070619717240334\n",
      "Epoch : 459, Loss : 0.019108517095446587\n",
      "Epoch : 460, Loss : 0.019183877855539322\n",
      "Epoch : 461, Loss : 0.019230183213949203\n",
      "Epoch : 462, Loss : 0.019324764609336853\n",
      "Epoch : 463, Loss : 0.019302450120449066\n",
      "Epoch : 464, Loss : 0.019301488995552063\n",
      "Epoch : 465, Loss : 0.019182536751031876\n",
      "Epoch : 466, Loss : 0.019094878807663918\n",
      "Epoch : 467, Loss : 0.019010374322533607\n",
      "Epoch : 468, Loss : 0.018971119076013565\n",
      "Epoch : 469, Loss : 0.018971528857946396\n",
      "Epoch : 470, Loss : 0.019000060856342316\n",
      "Epoch : 471, Loss : 0.019039377570152283\n",
      "Epoch : 472, Loss : 0.019055908545851707\n",
      "Epoch : 473, Loss : 0.01905578188598156\n",
      "Epoch : 474, Loss : 0.01902257651090622\n",
      "Epoch : 475, Loss : 0.018990570679306984\n",
      "Epoch : 476, Loss : 0.018969664350152016\n",
      "Epoch : 477, Loss : 0.018969066441059113\n",
      "Epoch : 478, Loss : 0.01898336410522461\n",
      "Epoch : 479, Loss : 0.01899857632815838\n",
      "Epoch : 480, Loss : 0.019006190821528435\n",
      "Epoch : 481, Loss : 0.018997326493263245\n",
      "Epoch : 482, Loss : 0.018983257934451103\n",
      "Epoch : 483, Loss : 0.018970586359500885\n",
      "Epoch : 484, Loss : 0.018967028707265854\n",
      "Epoch : 485, Loss : 0.018972046673297882\n",
      "Epoch : 486, Loss : 0.018979882821440697\n",
      "Epoch : 487, Loss : 0.018984999507665634\n",
      "Epoch : 488, Loss : 0.01898275315761566\n",
      "Epoch : 489, Loss : 0.018982699140906334\n",
      "Epoch : 490, Loss : 0.018984954804182053\n",
      "Epoch : 491, Loss : 0.018994102254509926\n",
      "Epoch : 492, Loss : 0.019004805013537407\n",
      "Epoch : 493, Loss : 0.019014563411474228\n",
      "Epoch : 494, Loss : 0.01901388354599476\n",
      "Epoch : 495, Loss : 0.019013438373804092\n",
      "Epoch : 496, Loss : 0.019001774489879608\n",
      "Epoch : 497, Loss : 0.018989700824022293\n",
      "Epoch : 498, Loss : 0.0189773328602314\n",
      "Epoch : 499, Loss : 0.018970035016536713\n",
      "Epoch : 500, Loss : 0.018966950476169586\n",
      "Epoch : 501, Loss : 0.018968798220157623\n",
      "Epoch : 502, Loss : 0.01897300034761429\n",
      "Epoch : 503, Loss : 0.018978148698806763\n",
      "Epoch : 504, Loss : 0.018982382491230965\n",
      "Epoch : 505, Loss : 0.018983539193868637\n",
      "Epoch : 506, Loss : 0.01898352988064289\n",
      "Epoch : 507, Loss : 0.018980098888278008\n",
      "Epoch : 508, Loss : 0.018976593390107155\n",
      "Epoch : 509, Loss : 0.01897239498794079\n",
      "Epoch : 510, Loss : 0.01896951161324978\n",
      "Epoch : 511, Loss : 0.018967418000102043\n",
      "Epoch : 512, Loss : 0.018966902047395706\n",
      "Epoch : 513, Loss : 0.018967261537909508\n",
      "Epoch : 514, Loss : 0.018968408927321434\n",
      "Epoch : 515, Loss : 0.01896984502673149\n",
      "Epoch : 516, Loss : 0.018971167504787445\n",
      "Epoch : 517, Loss : 0.018972476944327354\n",
      "Epoch : 518, Loss : 0.018973004072904587\n",
      "Epoch : 519, Loss : 0.018973523750901222\n",
      "Epoch : 520, Loss : 0.01897338405251503\n",
      "Epoch : 521, Loss : 0.01897333189845085\n",
      "Epoch : 522, Loss : 0.018972687423229218\n",
      "Epoch : 523, Loss : 0.01897227205336094\n",
      "Epoch : 524, Loss : 0.018971668556332588\n",
      "Epoch : 525, Loss : 0.01897122897207737\n",
      "Epoch : 526, Loss : 0.018970729783177376\n",
      "Epoch : 527, Loss : 0.018970446661114693\n",
      "Epoch : 528, Loss : 0.018970200791954994\n",
      "Epoch : 529, Loss : 0.01897011324763298\n",
      "Epoch : 530, Loss : 0.018970094621181488\n",
      "Epoch : 531, Loss : 0.01897033303976059\n",
      "Epoch : 532, Loss : 0.018970636650919914\n",
      "Epoch : 533, Loss : 0.018971309065818787\n",
      "Epoch : 534, Loss : 0.018972139805555344\n",
      "Epoch : 535, Loss : 0.01897367089986801\n",
      "Epoch : 536, Loss : 0.01897551864385605\n",
      "Epoch : 537, Loss : 0.0189787819981575\n",
      "Epoch : 538, Loss : 0.018982836976647377\n",
      "Epoch : 539, Loss : 0.01899004727602005\n",
      "Epoch : 540, Loss : 0.01900850050151348\n",
      "Epoch : 541, Loss : 0.019050266593694687\n",
      "Epoch : 542, Loss : 0.019103338941931725\n",
      "Epoch : 543, Loss : 0.019219087436795235\n",
      "Epoch : 544, Loss : 0.019276123493909836\n",
      "Epoch : 545, Loss : 0.019376033917069435\n",
      "Epoch : 546, Loss : 0.019296173006296158\n",
      "Epoch : 547, Loss : 0.01923341676592827\n",
      "Epoch : 548, Loss : 0.019072147086262703\n",
      "Epoch : 549, Loss : 0.018983008340001106\n",
      "Epoch : 550, Loss : 0.01897430419921875\n",
      "Epoch : 551, Loss : 0.019023211672902107\n",
      "Epoch : 552, Loss : 0.019092243164777756\n",
      "Epoch : 553, Loss : 0.019092634320259094\n",
      "Epoch : 554, Loss : 0.01906670816242695\n",
      "Epoch : 555, Loss : 0.01900501176714897\n",
      "Epoch : 556, Loss : 0.018969904631376266\n",
      "Epoch : 557, Loss : 0.01897532306611538\n",
      "Epoch : 558, Loss : 0.019004734233021736\n",
      "Epoch : 559, Loss : 0.01903110183775425\n",
      "Epoch : 560, Loss : 0.019019095227122307\n",
      "Epoch : 561, Loss : 0.018991708755493164\n",
      "Epoch : 562, Loss : 0.018969649448990822\n",
      "Epoch : 563, Loss : 0.018970824778079987\n",
      "Epoch : 564, Loss : 0.018988333642482758\n",
      "Epoch : 565, Loss : 0.018998324871063232\n",
      "Epoch : 566, Loss : 0.018994230777025223\n",
      "Epoch : 567, Loss : 0.018977656960487366\n",
      "Epoch : 568, Loss : 0.018967555835843086\n",
      "Epoch : 569, Loss : 0.018970146775245667\n",
      "Epoch : 570, Loss : 0.01897919736802578\n",
      "Epoch : 571, Loss : 0.01898469217121601\n",
      "Epoch : 572, Loss : 0.018979836255311966\n",
      "Epoch : 573, Loss : 0.018971726298332214\n",
      "Epoch : 574, Loss : 0.018966954201459885\n",
      "Epoch : 575, Loss : 0.01896940916776657\n",
      "Epoch : 576, Loss : 0.01897464692592621\n",
      "Epoch : 577, Loss : 0.018976563587784767\n",
      "Epoch : 578, Loss : 0.01897384040057659\n",
      "Epoch : 579, Loss : 0.01896907016634941\n",
      "Epoch : 580, Loss : 0.01896688900887966\n",
      "Epoch : 581, Loss : 0.018968366086483\n",
      "Epoch : 582, Loss : 0.018971066921949387\n",
      "Epoch : 583, Loss : 0.018972204998135567\n",
      "Epoch : 584, Loss : 0.018970636650919914\n",
      "Epoch : 585, Loss : 0.01896824687719345\n",
      "Epoch : 586, Loss : 0.018966829404234886\n",
      "Epoch : 587, Loss : 0.018967382609844208\n",
      "Epoch : 588, Loss : 0.018968842923641205\n",
      "Epoch : 589, Loss : 0.01896967552602291\n",
      "Epoch : 590, Loss : 0.018969284370541573\n",
      "Epoch : 591, Loss : 0.018967965617775917\n",
      "Epoch : 592, Loss : 0.01896694302558899\n",
      "Epoch : 593, Loss : 0.018966786563396454\n",
      "Epoch : 594, Loss : 0.018967416137456894\n",
      "Epoch : 595, Loss : 0.01896807737648487\n",
      "Epoch : 596, Loss : 0.018968256190419197\n",
      "Epoch : 597, Loss : 0.018967842683196068\n",
      "Epoch : 598, Loss : 0.018967173993587494\n",
      "Epoch : 599, Loss : 0.01896672695875168\n",
      "Epoch : 600, Loss : 0.018966704607009888\n",
      "Epoch : 601, Loss : 0.018966997042298317\n",
      "Epoch : 602, Loss : 0.01896733231842518\n",
      "Epoch : 603, Loss : 0.018967438489198685\n",
      "Epoch : 604, Loss : 0.0189672838896513\n",
      "Epoch : 605, Loss : 0.018966978415846825\n",
      "Epoch : 606, Loss : 0.018966708332300186\n",
      "Epoch : 607, Loss : 0.018966587260365486\n",
      "Epoch : 608, Loss : 0.01896664686501026\n",
      "Epoch : 609, Loss : 0.018966788426041603\n",
      "Epoch : 610, Loss : 0.01896691508591175\n",
      "Epoch : 611, Loss : 0.018966954201459885\n",
      "Epoch : 612, Loss : 0.018966881558299065\n",
      "Epoch : 613, Loss : 0.018966753035783768\n",
      "Epoch : 614, Loss : 0.01896662265062332\n",
      "Epoch : 615, Loss : 0.018966536968946457\n",
      "Epoch : 616, Loss : 0.018966510891914368\n",
      "Epoch : 617, Loss : 0.018966544419527054\n",
      "Epoch : 618, Loss : 0.018966592848300934\n",
      "Epoch : 619, Loss : 0.018966639414429665\n",
      "Epoch : 620, Loss : 0.018966661766171455\n",
      "Epoch : 621, Loss : 0.01896664686501026\n",
      "Epoch : 622, Loss : 0.018966607749462128\n",
      "Epoch : 623, Loss : 0.01896655559539795\n",
      "Epoch : 624, Loss : 0.01896650344133377\n",
      "Epoch : 625, Loss : 0.01896646060049534\n",
      "Epoch : 626, Loss : 0.0189664363861084\n",
      "Epoch : 627, Loss : 0.018966423347592354\n",
      "Epoch : 628, Loss : 0.018966425210237503\n",
      "Epoch : 629, Loss : 0.018966438248753548\n",
      "Epoch : 630, Loss : 0.018966447561979294\n",
      "Epoch : 631, Loss : 0.01896645873785019\n",
      "Epoch : 632, Loss : 0.018966464325785637\n",
      "Epoch : 633, Loss : 0.018966464325785637\n",
      "Epoch : 634, Loss : 0.01896646060049534\n",
      "Epoch : 635, Loss : 0.01896645314991474\n",
      "Epoch : 636, Loss : 0.018966445699334145\n",
      "Epoch : 637, Loss : 0.018966438248753548\n",
      "Epoch : 638, Loss : 0.018966425210237503\n",
      "Epoch : 639, Loss : 0.018966414034366608\n",
      "Epoch : 640, Loss : 0.01896640844643116\n",
      "Epoch : 641, Loss : 0.018966400995850563\n",
      "Epoch : 642, Loss : 0.018966397270560265\n",
      "Epoch : 643, Loss : 0.018966395407915115\n",
      "Epoch : 644, Loss : 0.018966399133205414\n",
      "Epoch : 645, Loss : 0.01896640658378601\n",
      "Epoch : 646, Loss : 0.018966417759656906\n",
      "Epoch : 647, Loss : 0.018966443836688995\n",
      "Epoch : 648, Loss : 0.01896648108959198\n",
      "Epoch : 649, Loss : 0.018966546282172203\n",
      "Epoch : 650, Loss : 0.01896665059030056\n",
      "Epoch : 651, Loss : 0.01896682195365429\n",
      "Epoch : 652, Loss : 0.018967077136039734\n",
      "Epoch : 653, Loss : 0.018967490643262863\n",
      "Epoch : 654, Loss : 0.01896812580525875\n",
      "Epoch : 655, Loss : 0.01896919682621956\n",
      "Epoch : 656, Loss : 0.018970858305692673\n",
      "Epoch : 657, Loss : 0.01897379197180271\n",
      "Epoch : 658, Loss : 0.018978334963321686\n",
      "Epoch : 659, Loss : 0.018986772745847702\n",
      "Epoch : 660, Loss : 0.018999405205249786\n",
      "Epoch : 661, Loss : 0.019024260342121124\n",
      "Epoch : 662, Loss : 0.01905781403183937\n",
      "Epoch : 663, Loss : 0.019127333536744118\n",
      "Epoch : 664, Loss : 0.019196540117263794\n",
      "Epoch : 665, Loss : 0.019343407824635506\n",
      "Epoch : 666, Loss : 0.019387809559702873\n",
      "Epoch : 667, Loss : 0.019493024796247482\n",
      "Epoch : 668, Loss : 0.01933361403644085\n",
      "Epoch : 669, Loss : 0.019213933497667313\n",
      "Epoch : 670, Loss : 0.01905137300491333\n",
      "Epoch : 671, Loss : 0.018975378945469856\n",
      "Epoch : 672, Loss : 0.01897510327398777\n",
      "Epoch : 673, Loss : 0.01902749203145504\n",
      "Epoch : 674, Loss : 0.019093893468379974\n",
      "Epoch : 675, Loss : 0.019093800336122513\n",
      "Epoch : 676, Loss : 0.01905638724565506\n",
      "Epoch : 677, Loss : 0.01899190992116928\n",
      "Epoch : 678, Loss : 0.01896669529378414\n",
      "Epoch : 679, Loss : 0.018986312672495842\n",
      "Epoch : 680, Loss : 0.0190182663500309\n",
      "Epoch : 681, Loss : 0.019030505791306496\n",
      "Epoch : 682, Loss : 0.019002489745616913\n",
      "Epoch : 683, Loss : 0.01897347718477249\n",
      "Epoch : 684, Loss : 0.018968231976032257\n",
      "Epoch : 685, Loss : 0.01898525096476078\n",
      "Epoch : 686, Loss : 0.019000587984919548\n",
      "Epoch : 687, Loss : 0.01899225078523159\n",
      "Epoch : 688, Loss : 0.01897469535470009\n",
      "Epoch : 689, Loss : 0.018966790288686752\n",
      "Epoch : 690, Loss : 0.01897488906979561\n",
      "Epoch : 691, Loss : 0.01898529753088951\n",
      "Epoch : 692, Loss : 0.01898273080587387\n",
      "Epoch : 693, Loss : 0.018972713500261307\n",
      "Epoch : 694, Loss : 0.018966855481266975\n",
      "Epoch : 695, Loss : 0.018970683217048645\n",
      "Epoch : 696, Loss : 0.018977154046297073\n",
      "Epoch : 697, Loss : 0.018976636230945587\n",
      "Epoch : 698, Loss : 0.018970854580402374\n",
      "Epoch : 699, Loss : 0.018966875970363617\n",
      "Epoch : 700, Loss : 0.018968652933835983\n",
      "Epoch : 701, Loss : 0.018972503021359444\n",
      "Epoch : 702, Loss : 0.018972765654325485\n",
      "Epoch : 703, Loss : 0.018969614058732986\n",
      "Epoch : 704, Loss : 0.01896689459681511\n",
      "Epoch : 705, Loss : 0.01896750181913376\n",
      "Epoch : 706, Loss : 0.018969770520925522\n",
      "Epoch : 707, Loss : 0.018970418721437454\n",
      "Epoch : 708, Loss : 0.018968941643834114\n",
      "Epoch : 709, Loss : 0.018967073410749435\n",
      "Epoch : 710, Loss : 0.01896684244275093\n",
      "Epoch : 711, Loss : 0.018968043848872185\n",
      "Epoch : 712, Loss : 0.01896885223686695\n",
      "Epoch : 713, Loss : 0.018968354910612106\n",
      "Epoch : 714, Loss : 0.01896718330681324\n",
      "Epoch : 715, Loss : 0.018966663628816605\n",
      "Epoch : 716, Loss : 0.018967119976878166\n",
      "Epoch : 717, Loss : 0.01896779052913189\n",
      "Epoch : 718, Loss : 0.018967851996421814\n",
      "Epoch : 719, Loss : 0.01896725222468376\n",
      "Epoch : 720, Loss : 0.018966706469655037\n",
      "Epoch : 721, Loss : 0.018966680392622948\n",
      "Epoch : 722, Loss : 0.018967047333717346\n",
      "Epoch : 723, Loss : 0.01896732486784458\n",
      "Epoch : 724, Loss : 0.018967200070619583\n",
      "Epoch : 725, Loss : 0.018966831266880035\n",
      "Epoch : 726, Loss : 0.018966585397720337\n",
      "Epoch : 727, Loss : 0.018966633826494217\n",
      "Epoch : 728, Loss : 0.01896684244275093\n",
      "Epoch : 729, Loss : 0.018966957926750183\n",
      "Epoch : 730, Loss : 0.01896687224507332\n",
      "Epoch : 731, Loss : 0.0189666710793972\n",
      "Epoch : 732, Loss : 0.01896653138101101\n",
      "Epoch : 733, Loss : 0.018966544419527054\n",
      "Epoch : 734, Loss : 0.01896664686501026\n",
      "Epoch : 735, Loss : 0.01896671950817108\n",
      "Epoch : 736, Loss : 0.018966693431138992\n",
      "Epoch : 737, Loss : 0.018966587260365486\n",
      "Epoch : 738, Loss : 0.018966494128108025\n",
      "Epoch : 739, Loss : 0.018966466188430786\n",
      "Epoch : 740, Loss : 0.018966499716043472\n",
      "Epoch : 741, Loss : 0.018966548144817352\n",
      "Epoch : 742, Loss : 0.018966559320688248\n",
      "Epoch : 743, Loss : 0.01896652765572071\n",
      "Epoch : 744, Loss : 0.018966468051075935\n",
      "Epoch : 745, Loss : 0.018966419622302055\n",
      "Epoch : 746, Loss : 0.01896640844643116\n",
      "Epoch : 747, Loss : 0.018966423347592354\n",
      "Epoch : 748, Loss : 0.018966443836688995\n",
      "Epoch : 749, Loss : 0.018966447561979294\n",
      "Epoch : 750, Loss : 0.0189664326608181\n",
      "Epoch : 751, Loss : 0.018966402858495712\n",
      "Epoch : 752, Loss : 0.018966371193528175\n",
      "Epoch : 753, Loss : 0.018966425210237503\n",
      "Epoch : 754, Loss : 0.018966984003782272\n",
      "Epoch : 755, Loss : 0.018968243151903152\n",
      "Epoch : 756, Loss : 0.01896923966705799\n",
      "Epoch : 757, Loss : 0.018969517201185226\n",
      "Epoch : 758, Loss : 0.01896890252828598\n",
      "Epoch : 759, Loss : 0.018967868760228157\n",
      "Epoch : 760, Loss : 0.01896689087152481\n",
      "Epoch : 761, Loss : 0.01896635815501213\n",
      "Epoch : 762, Loss : 0.018966374918818474\n",
      "Epoch : 763, Loss : 0.018966779112815857\n",
      "Epoch : 764, Loss : 0.018967293202877045\n",
      "Epoch : 765, Loss : 0.018967635929584503\n",
      "Epoch : 766, Loss : 0.018967727199196815\n",
      "Epoch : 767, Loss : 0.018967507407069206\n",
      "Epoch : 768, Loss : 0.01896713674068451\n",
      "Epoch : 769, Loss : 0.01896672695875168\n",
      "Epoch : 770, Loss : 0.018966417759656906\n",
      "Epoch : 771, Loss : 0.01896625943481922\n",
      "Epoch : 772, Loss : 0.01896626316010952\n",
      "Epoch : 773, Loss : 0.018966374918818474\n",
      "Epoch : 774, Loss : 0.018966538831591606\n",
      "Epoch : 775, Loss : 0.01896669529378414\n",
      "Epoch : 776, Loss : 0.018966788426041603\n",
      "Epoch : 777, Loss : 0.018966825678944588\n",
      "Epoch : 778, Loss : 0.018966788426041603\n",
      "Epoch : 779, Loss : 0.018966708332300186\n",
      "Epoch : 780, Loss : 0.018966594710946083\n",
      "Epoch : 781, Loss : 0.01896648108959198\n",
      "Epoch : 782, Loss : 0.018966371193528175\n",
      "Epoch : 783, Loss : 0.01896628737449646\n",
      "Epoch : 784, Loss : 0.018966222181916237\n",
      "Epoch : 785, Loss : 0.018966183066368103\n",
      "Epoch : 786, Loss : 0.01896616630256176\n",
      "Epoch : 787, Loss : 0.01896616257727146\n",
      "Epoch : 788, Loss : 0.018966173753142357\n",
      "Epoch : 789, Loss : 0.01896618865430355\n",
      "Epoch : 790, Loss : 0.01896621286869049\n",
      "Epoch : 791, Loss : 0.01896623894572258\n",
      "Epoch : 792, Loss : 0.018966268748044968\n",
      "Epoch : 793, Loss : 0.018966306000947952\n",
      "Epoch : 794, Loss : 0.018966348841786385\n",
      "Epoch : 795, Loss : 0.018966400995850563\n",
      "Epoch : 796, Loss : 0.018966464325785637\n",
      "Epoch : 797, Loss : 0.0189665500074625\n",
      "Epoch : 798, Loss : 0.0189666710793972\n",
      "Epoch : 799, Loss : 0.018966834992170334\n",
      "Epoch : 800, Loss : 0.018967073410749435\n",
      "Epoch : 801, Loss : 0.0189674012362957\n",
      "Epoch : 802, Loss : 0.018967904150485992\n",
      "Epoch : 803, Loss : 0.01896861381828785\n",
      "Epoch : 804, Loss : 0.018969736993312836\n",
      "Epoch : 805, Loss : 0.018971335142850876\n",
      "Epoch : 806, Loss : 0.01897399313747883\n",
      "Epoch : 807, Loss : 0.018977729603648186\n",
      "Epoch : 808, Loss : 0.018984001129865646\n",
      "Epoch : 809, Loss : 0.018991917371749878\n",
      "Epoch : 810, Loss : 0.019005706533789635\n",
      "Epoch : 811, Loss : 0.019021490588784218\n",
      "Epoch : 812, Loss : 0.01905043050646782\n",
      "Epoch : 813, Loss : 0.019077695906162262\n",
      "Epoch : 814, Loss : 0.01913086324930191\n",
      "Epoch : 815, Loss : 0.019160723313689232\n",
      "Epoch : 816, Loss : 0.019217807799577713\n",
      "Epoch : 817, Loss : 0.019193509593605995\n",
      "Epoch : 818, Loss : 0.019177088513970375\n",
      "Epoch : 819, Loss : 0.019092468544840813\n",
      "Epoch : 820, Loss : 0.01902935281395912\n",
      "Epoch : 821, Loss : 0.0189804844558239\n",
      "Epoch : 822, Loss : 0.018966272473335266\n",
      "Epoch : 823, Loss : 0.01898134872317314\n",
      "Epoch : 824, Loss : 0.019008686766028404\n",
      "Epoch : 825, Loss : 0.019032573327422142\n",
      "Epoch : 826, Loss : 0.019026849418878555\n",
      "Epoch : 827, Loss : 0.019007792696356773\n",
      "Epoch : 828, Loss : 0.01898062601685524\n",
      "Epoch : 829, Loss : 0.018966855481266975\n",
      "Epoch : 830, Loss : 0.018970483914017677\n",
      "Epoch : 831, Loss : 0.018983755260705948\n",
      "Epoch : 832, Loss : 0.018994631245732307\n",
      "Epoch : 833, Loss : 0.01899138279259205\n",
      "Epoch : 834, Loss : 0.018980467692017555\n",
      "Epoch : 835, Loss : 0.018969200551509857\n",
      "Epoch : 836, Loss : 0.018966544419527054\n",
      "Epoch : 837, Loss : 0.018972009420394897\n",
      "Epoch : 838, Loss : 0.018978474661707878\n",
      "Epoch : 839, Loss : 0.018980184569954872\n",
      "Epoch : 840, Loss : 0.01897505298256874\n",
      "Epoch : 841, Loss : 0.018968895077705383\n",
      "Epoch : 842, Loss : 0.018966279923915863\n",
      "Epoch : 843, Loss : 0.01896836794912815\n",
      "Epoch : 844, Loss : 0.018972136080265045\n",
      "Epoch : 845, Loss : 0.01897347718477249\n",
      "Epoch : 846, Loss : 0.018971670418977737\n",
      "Epoch : 847, Loss : 0.018968256190419197\n",
      "Epoch : 848, Loss : 0.01896633580327034\n",
      "Epoch : 849, Loss : 0.01896694116294384\n",
      "Epoch : 850, Loss : 0.01896885596215725\n",
      "Epoch : 851, Loss : 0.018970109522342682\n",
      "Epoch : 852, Loss : 0.018969504162669182\n",
      "Epoch : 853, Loss : 0.018967848271131516\n",
      "Epoch : 854, Loss : 0.01896647736430168\n",
      "Epoch : 855, Loss : 0.01896633766591549\n",
      "Epoch : 856, Loss : 0.01896718703210354\n",
      "Epoch : 857, Loss : 0.01896805316209793\n",
      "Epoch : 858, Loss : 0.018968207761645317\n",
      "Epoch : 859, Loss : 0.018967527896165848\n",
      "Epoch : 860, Loss : 0.01896667294204235\n",
      "Epoch : 861, Loss : 0.018966220319271088\n",
      "Epoch : 862, Loss : 0.018966374918818474\n",
      "Epoch : 863, Loss : 0.018966861069202423\n",
      "Epoch : 864, Loss : 0.01896720379590988\n",
      "Epoch : 865, Loss : 0.018967166543006897\n",
      "Epoch : 866, Loss : 0.018966786563396454\n",
      "Epoch : 867, Loss : 0.018966376781463623\n",
      "Epoch : 868, Loss : 0.018966173753142357\n",
      "Epoch : 869, Loss : 0.018966250121593475\n",
      "Epoch : 870, Loss : 0.018966473639011383\n",
      "Epoch : 871, Loss : 0.01896665245294571\n",
      "Epoch : 872, Loss : 0.018966663628816605\n",
      "Epoch : 873, Loss : 0.018966512754559517\n",
      "Epoch : 874, Loss : 0.01896630972623825\n",
      "Epoch : 875, Loss : 0.01896616257727146\n",
      "Epoch : 876, Loss : 0.018966132774949074\n",
      "Epoch : 877, Loss : 0.018966207280755043\n",
      "Epoch : 878, Loss : 0.0189663078635931\n",
      "Epoch : 879, Loss : 0.018966365605592728\n",
      "Epoch : 880, Loss : 0.018966352567076683\n",
      "Epoch : 881, Loss : 0.018966279923915863\n",
      "Epoch : 882, Loss : 0.01896618865430355\n",
      "Epoch : 883, Loss : 0.01896611601114273\n",
      "Epoch : 884, Loss : 0.01896609179675579\n",
      "Epoch : 885, Loss : 0.018966108560562134\n",
      "Epoch : 886, Loss : 0.01896614395081997\n",
      "Epoch : 887, Loss : 0.018966179341077805\n",
      "Epoch : 888, Loss : 0.0189661905169487\n",
      "Epoch : 889, Loss : 0.018966175615787506\n",
      "Epoch : 890, Loss : 0.018966149538755417\n",
      "Epoch : 891, Loss : 0.018966127187013626\n",
      "Epoch : 892, Loss : 0.018966102972626686\n",
      "Epoch : 893, Loss : 0.018966076895594597\n",
      "Epoch : 894, Loss : 0.018966056406497955\n",
      "Epoch : 895, Loss : 0.01896604523062706\n",
      "Epoch : 896, Loss : 0.01896604150533676\n",
      "Epoch : 897, Loss : 0.01896604336798191\n",
      "Epoch : 898, Loss : 0.01896604895591736\n",
      "Epoch : 899, Loss : 0.018966052681207657\n",
      "Epoch : 900, Loss : 0.018966054543852806\n",
      "Epoch : 901, Loss : 0.018966050818562508\n",
      "Epoch : 902, Loss : 0.01896604336798191\n",
      "Epoch : 903, Loss : 0.018966034054756165\n",
      "Epoch : 904, Loss : 0.01896602474153042\n",
      "Epoch : 905, Loss : 0.018966013565659523\n",
      "Epoch : 906, Loss : 0.018966004252433777\n",
      "Epoch : 907, Loss : 0.01896599680185318\n",
      "Epoch : 908, Loss : 0.018965991213917732\n",
      "Epoch : 909, Loss : 0.018965985625982285\n",
      "Epoch : 910, Loss : 0.018965983763337135\n",
      "Epoch : 911, Loss : 0.018965981900691986\n",
      "Epoch : 912, Loss : 0.018965981900691986\n",
      "Epoch : 913, Loss : 0.018965978175401688\n",
      "Epoch : 914, Loss : 0.018965978175401688\n",
      "Epoch : 915, Loss : 0.018965978175401688\n",
      "Epoch : 916, Loss : 0.01896597445011139\n",
      "Epoch : 917, Loss : 0.01896597258746624\n",
      "Epoch : 918, Loss : 0.01896597445011139\n",
      "Epoch : 919, Loss : 0.01896597072482109\n",
      "Epoch : 920, Loss : 0.01896597072482109\n",
      "Epoch : 921, Loss : 0.01896597258746624\n",
      "Epoch : 922, Loss : 0.01896597445011139\n",
      "Epoch : 923, Loss : 0.018965978175401688\n",
      "Epoch : 924, Loss : 0.018965985625982285\n",
      "Epoch : 925, Loss : 0.01896599866449833\n",
      "Epoch : 926, Loss : 0.01896601729094982\n",
      "Epoch : 927, Loss : 0.01896604336798191\n",
      "Epoch : 928, Loss : 0.018966084346175194\n",
      "Epoch : 929, Loss : 0.01896614395081997\n",
      "Epoch : 930, Loss : 0.01896623522043228\n",
      "Epoch : 931, Loss : 0.018966373056173325\n",
      "Epoch : 932, Loss : 0.018966589123010635\n",
      "Epoch : 933, Loss : 0.01896691508591175\n",
      "Epoch : 934, Loss : 0.01896744593977928\n",
      "Epoch : 935, Loss : 0.018968261778354645\n",
      "Epoch : 936, Loss : 0.01896963082253933\n",
      "Epoch : 937, Loss : 0.01897173933684826\n",
      "Epoch : 938, Loss : 0.0189753957092762\n",
      "Epoch : 939, Loss : 0.018980927765369415\n",
      "Epoch : 940, Loss : 0.018991203978657722\n",
      "Epoch : 941, Loss : 0.019005760550498962\n",
      "Epoch : 942, Loss : 0.019033364951610565\n",
      "Epoch : 943, Loss : 0.019069300964474678\n",
      "Epoch : 944, Loss : 0.019139671698212624\n",
      "Epoch : 945, Loss : 0.019196532666683197\n",
      "Epoch : 946, Loss : 0.019301440566778183\n",
      "Epoch : 947, Loss : 0.019316671416163445\n",
      "Epoch : 948, Loss : 0.019333472475409508\n",
      "Epoch : 949, Loss : 0.019187936559319496\n",
      "Epoch : 950, Loss : 0.0190677922219038\n",
      "Epoch : 951, Loss : 0.01897934265434742\n",
      "Epoch : 952, Loss : 0.01897413469851017\n",
      "Epoch : 953, Loss : 0.019029397517442703\n",
      "Epoch : 954, Loss : 0.019077030941843987\n",
      "Epoch : 955, Loss : 0.01908070780336857\n",
      "Epoch : 956, Loss : 0.01901767961680889\n",
      "Epoch : 957, Loss : 0.018971478566527367\n",
      "Epoch : 958, Loss : 0.01897430047392845\n",
      "Epoch : 959, Loss : 0.019007662311196327\n",
      "Epoch : 960, Loss : 0.01902749389410019\n",
      "Epoch : 961, Loss : 0.01900194026529789\n",
      "Epoch : 962, Loss : 0.01897156611084938\n",
      "Epoch : 963, Loss : 0.018970312550663948\n",
      "Epoch : 964, Loss : 0.018991272896528244\n",
      "Epoch : 965, Loss : 0.019001400098204613\n",
      "Epoch : 966, Loss : 0.01898379437625408\n",
      "Epoch : 967, Loss : 0.01896761730313301\n",
      "Epoch : 968, Loss : 0.018972080200910568\n",
      "Epoch : 969, Loss : 0.018984762951731682\n",
      "Epoch : 970, Loss : 0.018985295668244362\n",
      "Epoch : 971, Loss : 0.01897217519581318\n",
      "Epoch : 972, Loss : 0.018966788426041603\n",
      "Epoch : 973, Loss : 0.018973801285028458\n",
      "Epoch : 974, Loss : 0.018979057669639587\n",
      "Epoch : 975, Loss : 0.018974432721734047\n",
      "Epoch : 976, Loss : 0.01896732859313488\n",
      "Epoch : 977, Loss : 0.018968258053064346\n",
      "Epoch : 978, Loss : 0.01897347718477249\n",
      "Epoch : 979, Loss : 0.01897362992167473\n",
      "Epoch : 980, Loss : 0.01896877959370613\n",
      "Epoch : 981, Loss : 0.018966637551784515\n",
      "Epoch : 982, Loss : 0.018969327211380005\n",
      "Epoch : 983, Loss : 0.018971631303429604\n",
      "Epoch : 984, Loss : 0.018969547003507614\n",
      "Epoch : 985, Loss : 0.01896681636571884\n",
      "Epoch : 986, Loss : 0.018967173993587494\n",
      "Epoch : 987, Loss : 0.018969202414155006\n",
      "Epoch : 988, Loss : 0.01896938681602478\n",
      "Epoch : 989, Loss : 0.018967434763908386\n",
      "Epoch : 990, Loss : 0.018966497853398323\n",
      "Epoch : 991, Loss : 0.01896747760474682\n",
      "Epoch : 992, Loss : 0.01896842196583748\n",
      "Epoch : 993, Loss : 0.018967779353260994\n",
      "Epoch : 994, Loss : 0.01896665431559086\n",
      "Epoch : 995, Loss : 0.01896657422184944\n",
      "Epoch : 996, Loss : 0.018967365846037865\n",
      "Epoch : 997, Loss : 0.01896759681403637\n",
      "Epoch : 998, Loss : 0.018966972827911377\n",
      "Epoch : 999, Loss : 0.018966414034366608\n"
     ]
    }
   ],
   "source": [
    "bce_loss_list = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    sgd_optim.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    #print(outputs.shape, y_train.shape)\n",
    "    bce_loss = criterion(outputs, y_train)\n",
    "\n",
    "    bce_loss.backward()\n",
    "    bce_loss_list.append(bce_loss)\n",
    "\n",
    "    sgd_optim.step()\n",
    "\n",
    "    print(f\"Epoch : {epoch}, Loss : {bce_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\"The movie is awfull\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = vectorizer.transform(sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2022, -1.6977]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentiment = model(torch.from_numpy(sample).float())\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = [\"Good tasty and the texture was just great\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = vectorizer.transform(sample2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18.2555,   0.0000]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentiment2 = model(torch.from_numpy(sample2).float())\n",
    "sentiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.1307,  0.0726,  0.0958,  ...,  0.1334,  0.0718,  0.0705],\n",
       "                      [ 0.0877,  0.0685, -0.0980,  ..., -0.0843,  0.0227,  0.1508],\n",
       "                      [-0.0370, -0.0433,  0.0319,  ..., -0.0438, -0.0215, -0.0413],\n",
       "                      ...,\n",
       "                      [ 0.0398, -0.0607, -0.0142,  ...,  0.0854,  0.0401, -0.1142],\n",
       "                      [-0.0443,  0.0784, -0.0827,  ..., -0.1570, -0.0266, -0.0497],\n",
       "                      [ 0.0946, -0.0495,  0.0760,  ...,  0.1187,  0.0729, -0.0185]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.0321,  0.0084, -0.0586,  0.0158, -0.0265, -0.0128, -0.1272,  0.0265,\n",
       "                      -0.0196, -0.0355, -0.0897, -0.0184, -0.0385,  0.0082, -0.0206,  0.0247,\n",
       "                      -0.0219,  0.0161,  0.0150,  0.0304, -0.0136, -0.0058, -0.0171,  0.0002,\n",
       "                       0.0345,  0.0480, -0.0575, -0.0248, -0.0059, -0.0212,  0.0206,  0.0161,\n",
       "                       0.0228,  0.0084, -0.0024, -0.0709,  0.0204,  0.0040, -0.0105, -0.0050,\n",
       "                       0.0048, -0.0221, -0.0153, -0.0375,  0.0138,  0.0079, -0.0689,  0.0133,\n",
       "                       0.0158, -0.0071,  0.0304,  0.0119, -0.0639, -0.0163,  0.0382,  0.0161,\n",
       "                      -0.0103,  0.0136, -0.0233,  0.0147, -0.0294,  0.0203, -0.0181, -0.1175,\n",
       "                      -0.0219,  0.0033, -0.0015,  0.0200,  0.0181, -0.0079,  0.0099, -0.0678,\n",
       "                      -0.0010,  0.0252,  0.0168,  0.0496,  0.0385, -0.0915,  0.0139, -0.0647,\n",
       "                      -0.0403,  0.0209,  0.0101, -0.0732, -0.0309, -0.0064, -0.0358,  0.0191,\n",
       "                       0.0548, -0.0282, -0.0222, -0.1064, -0.0281,  0.0070, -0.0687,  0.0263,\n",
       "                      -0.0048, -0.1008, -0.0174,  0.0119, -0.0640,  0.0430, -0.0394, -0.0870,\n",
       "                      -0.0034, -0.0186,  0.0046, -0.0262,  0.0083,  0.0019, -0.0483,  0.0065,\n",
       "                      -0.0663, -0.0108, -0.0160, -0.0431, -0.0112, -0.0016, -0.0176,  0.0132,\n",
       "                      -0.0455,  0.0193, -0.0254,  0.0347, -0.0746, -0.0379,  0.0048, -0.1110,\n",
       "                      -0.0119,  0.0273,  0.0180, -0.0034, -0.0021, -0.0011, -0.0570, -0.0435,\n",
       "                      -0.0035, -0.0144,  0.0023, -0.0611, -0.0093, -0.0231, -0.1286, -0.0296,\n",
       "                      -0.1358,  0.0134, -0.0482, -0.0067,  0.0104, -0.0531, -0.0292,  0.0266,\n",
       "                      -0.0490, -0.0137,  0.0234,  0.0011, -0.0422, -0.0129,  0.0102, -0.0078,\n",
       "                       0.0305, -0.0106, -0.0036, -0.0487,  0.0027, -0.0460,  0.0088,  0.0177,\n",
       "                      -0.0058, -0.0146, -0.0153, -0.0254,  0.0091, -0.0452, -0.0121,  0.0191,\n",
       "                       0.0200,  0.0378,  0.0355,  0.0192, -0.0275,  0.0162, -0.0205, -0.1241,\n",
       "                       0.0290,  0.0269,  0.0133, -0.0589, -0.0097,  0.0089,  0.0112,  0.0202,\n",
       "                      -0.0076, -0.0393,  0.0273,  0.0124, -0.0604, -0.0553, -0.0223,  0.0150,\n",
       "                      -0.0593,  0.0486, -0.1316, -0.0211, -0.0146, -0.0389,  0.0134, -0.0138,\n",
       "                      -0.0024,  0.0096, -0.0186, -0.0047, -0.1034,  0.0142, -0.0467, -0.0379,\n",
       "                      -0.0998, -0.0618,  0.0203, -0.0680, -0.0085,  0.0227, -0.0059, -0.0150,\n",
       "                       0.0151,  0.0092, -0.0536,  0.0501, -0.0362,  0.0006, -0.0032, -0.0540,\n",
       "                      -0.0638, -0.0239, -0.0922, -0.1105,  0.0417,  0.0407, -0.0346,  0.0251,\n",
       "                      -0.0040, -0.0761,  0.0315, -0.0099,  0.0349, -0.0088,  0.0064, -0.0273,\n",
       "                      -0.0925, -0.0169, -0.0150, -0.0595, -0.0743, -0.0091, -0.1155,  0.0133,\n",
       "                      -0.0577,  0.0069,  0.0374, -0.0022, -0.0250, -0.0025, -0.0113, -0.0501,\n",
       "                      -0.0491, -0.0124,  0.0215, -0.0345,  0.0182,  0.0412,  0.0019, -0.0009,\n",
       "                      -0.0091, -0.0181, -0.0075,  0.0094, -0.0920, -0.0345, -0.0028,  0.0303,\n",
       "                      -0.0287,  0.0180, -0.0506, -0.0257, -0.0083, -0.0891, -0.0327, -0.0603,\n",
       "                      -0.0016, -0.0095, -0.0366, -0.0060,  0.0186, -0.0200, -0.0692,  0.0069,\n",
       "                       0.0056, -0.0500, -0.0048,  0.0045,  0.0047,  0.0163, -0.0007,  0.0045,\n",
       "                      -0.0150,  0.0158,  0.0490, -0.0031, -0.0070, -0.0608,  0.0222, -0.0130,\n",
       "                       0.0486,  0.0105, -0.1009, -0.1174,  0.0489, -0.0501, -0.0364, -0.0147,\n",
       "                      -0.0038, -0.0509,  0.0160,  0.0284,  0.0164, -0.0104,  0.0086,  0.0191,\n",
       "                      -0.0147,  0.0560, -0.0393, -0.0027, -0.0713, -0.0320,  0.0061, -0.0614,\n",
       "                       0.0131, -0.0258, -0.0136, -0.0115, -0.0064,  0.0187,  0.0406,  0.0098,\n",
       "                       0.0021, -0.0546, -0.0121, -0.0100, -0.0444, -0.0261, -0.0361, -0.0540,\n",
       "                      -0.1010, -0.1178,  0.0099, -0.0242,  0.0134, -0.1151,  0.0372, -0.0121,\n",
       "                      -0.0389, -0.0445, -0.0026,  0.0083,  0.0250, -0.0200, -0.0325, -0.1117,\n",
       "                       0.0168, -0.0259, -0.0142,  0.0202, -0.0187,  0.0142, -0.1091,  0.0619,\n",
       "                      -0.0152,  0.0191, -0.0070, -0.0003, -0.0272, -0.0139, -0.0038, -0.0329,\n",
       "                      -0.0235, -0.1006,  0.0556,  0.0195,  0.0298, -0.0217, -0.0081, -0.0292,\n",
       "                      -0.0046, -0.0174,  0.0236, -0.1254,  0.0101,  0.0447, -0.0188,  0.0329,\n",
       "                      -0.0522, -0.0093, -0.0636, -0.0214,  0.0002,  0.0236,  0.0181,  0.0242,\n",
       "                      -0.0134, -0.0102,  0.0055,  0.0231, -0.1309, -0.0332, -0.0151, -0.0070,\n",
       "                      -0.0360, -0.0997, -0.0942,  0.0218,  0.0343, -0.0085, -0.0555, -0.0636,\n",
       "                       0.0037, -0.0817, -0.0237,  0.0032, -0.0079, -0.0088,  0.0323, -0.0046,\n",
       "                      -0.0036,  0.0184,  0.0308,  0.0139, -0.1043,  0.0029,  0.0522,  0.0377,\n",
       "                      -0.0885, -0.0061, -0.0077, -0.0512,  0.0248, -0.1282, -0.0095,  0.0297,\n",
       "                      -0.0149, -0.0770, -0.0458, -0.0966, -0.0432,  0.0256, -0.1163,  0.0065,\n",
       "                       0.0199, -0.0975, -0.0441,  0.0004,  0.0002,  0.0431, -0.0179, -0.0349,\n",
       "                      -0.0307, -0.0942,  0.0432,  0.0349,  0.0193, -0.0367,  0.0486, -0.0261,\n",
       "                       0.0192, -0.0343, -0.0471, -0.0194, -0.0202, -0.0203,  0.0453, -0.0436,\n",
       "                      -0.0565,  0.0495,  0.0365,  0.0427, -0.0710, -0.0562, -0.0064, -0.0609,\n",
       "                      -0.0583,  0.0163,  0.0185, -0.0002, -0.0046, -0.0085, -0.1032, -0.0223,\n",
       "                      -0.0010,  0.0092,  0.0153, -0.0150])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.0434, -0.0159, -0.0322,  ...,  0.0587, -0.0203, -0.0883],\n",
       "                      [-0.0273, -0.0255, -0.0041,  ..., -0.0886, -0.0827, -0.0607],\n",
       "                      [-0.0245, -0.0607,  0.0883,  ..., -0.0461, -0.0489,  0.0351],\n",
       "                      ...,\n",
       "                      [ 0.0497, -0.0447,  0.2055,  ...,  0.0984,  0.1276,  0.0399],\n",
       "                      [ 0.0146,  0.1184, -0.1369,  ...,  0.0049,  0.0066, -0.0007],\n",
       "                      [ 0.0400, -0.0226,  0.2040,  ...,  0.0165,  0.0980,  0.0461]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([-7.1159e-02, -6.7083e-02, -3.5332e-02, -7.3114e-02, -4.2201e-02,\n",
       "                       4.3373e-02,  5.1154e-02,  3.0934e-02,  7.0871e-02,  2.7188e-02,\n",
       "                       5.1138e-02,  4.9734e-02,  7.1386e-02,  6.8904e-02, -7.8559e-02,\n",
       "                      -8.4721e-02,  9.5895e-02,  5.4987e-02, -1.0132e-01,  3.9227e-02,\n",
       "                      -6.3897e-02, -1.1678e-02,  1.8366e-04,  2.1840e-02, -7.4991e-02,\n",
       "                       4.3045e-02,  9.0399e-02,  8.5059e-02,  7.2681e-02, -7.2375e-02,\n",
       "                       8.3825e-02,  4.3559e-02,  5.6245e-02,  4.8618e-02,  8.7171e-03,\n",
       "                      -5.4270e-02,  4.2164e-02, -8.2283e-02,  5.7828e-02,  3.3517e-02,\n",
       "                      -1.2152e-02, -8.9957e-02,  1.5379e-02, -7.1929e-02,  5.9656e-02,\n",
       "                      -2.1346e-02,  7.4940e-02, -3.9048e-02,  3.5062e-02, -5.6820e-03,\n",
       "                       7.3557e-02, -5.9064e-02, -3.7204e-02,  3.0517e-02,  3.0552e-02,\n",
       "                       6.0471e-02,  4.8298e-02, -5.0206e-02, -6.0374e-02,  5.6428e-02,\n",
       "                       6.3640e-02, -6.7317e-03, -4.6874e-02,  5.5459e-02, -3.9670e-02,\n",
       "                      -4.4734e-02, -4.3561e-02, -9.3128e-02,  5.9461e-02,  5.3440e-02,\n",
       "                       6.8059e-02,  3.3830e-02,  3.4759e-02,  4.7396e-02, -8.1696e-02,\n",
       "                       5.6915e-02,  6.5296e-02,  3.5793e-02,  2.0233e-02, -8.6933e-02,\n",
       "                       7.0080e-02, -4.1942e-02, -9.1769e-02,  5.2617e-02,  6.1994e-02,\n",
       "                       6.6888e-03,  5.0236e-02,  1.0764e-02,  5.9378e-02, -2.1672e-01,\n",
       "                       6.4664e-02, -5.3219e-02,  7.3972e-02, -2.0307e-02,  3.4393e-02,\n",
       "                       1.7198e-02,  5.3541e-02, -9.6060e-02,  4.3616e-02, -6.9801e-02,\n",
       "                      -7.4564e-02, -4.3600e-02, -2.8320e-02, -7.4411e-02,  2.8533e-02,\n",
       "                       3.6133e-02,  4.0431e-02,  8.6490e-02, -7.0284e-02, -6.1488e-02,\n",
       "                       7.1293e-02,  4.1329e-02,  8.4291e-02, -9.6411e-02, -7.8142e-02,\n",
       "                       5.0000e-02,  5.7432e-02,  6.6692e-02, -7.7782e-03,  4.9115e-02,\n",
       "                       5.0695e-02, -8.9438e-02, -9.6187e-02,  6.7339e-02, -7.5752e-02,\n",
       "                      -7.0214e-02,  4.3736e-02,  5.6829e-03,  4.4088e-02,  2.2307e-02,\n",
       "                       7.4161e-02,  6.0336e-02,  6.0187e-02, -2.5466e-02, -8.7856e-02,\n",
       "                       3.6283e-02, -1.0684e-02,  5.5105e-02,  5.9531e-02, -8.1610e-02,\n",
       "                       5.4536e-02, -9.0802e-02,  7.2046e-02, -7.8827e-02,  1.8488e-02,\n",
       "                      -4.3572e-02, -7.8278e-02,  4.8530e-02, -9.5281e-02, -4.2045e-02,\n",
       "                       6.4229e-02, -4.5793e-02,  3.0409e-02, -4.3574e-02,  5.9383e-02,\n",
       "                       6.3555e-02, -7.5910e-02,  7.7065e-02, -9.5777e-02, -8.3487e-02,\n",
       "                      -7.1684e-02,  5.6683e-02, -8.2795e-02,  2.2693e-02,  4.2960e-02,\n",
       "                       5.7286e-02,  6.8754e-02,  5.3122e-02, -6.4660e-03,  7.8479e-02,\n",
       "                       3.2765e-02, -1.3625e-02,  4.8538e-02,  6.8298e-02, -1.2944e-02,\n",
       "                       5.8000e-02, -9.1452e-03, -5.1829e-02, -2.0787e-02,  8.1376e-02,\n",
       "                       3.3791e-02,  1.0095e-02, -5.2425e-02, -8.4993e-02,  6.6639e-02,\n",
       "                       4.9868e-02,  7.4926e-02,  5.9130e-02,  3.3515e-02, -3.5587e-02,\n",
       "                      -5.4856e-02, -8.0949e-02,  4.1895e-02, -6.0707e-02, -3.7729e-02,\n",
       "                       2.8297e-02,  1.3075e-02, -2.8587e-02,  8.2418e-02, -3.4323e-02,\n",
       "                      -6.5862e-02, -7.4564e-02,  5.7090e-02,  3.8905e-02,  1.8305e-02,\n",
       "                      -8.1739e-02,  4.7538e-02, -5.7236e-02,  7.3352e-02,  6.5353e-02,\n",
       "                       5.6583e-02, -8.8792e-02,  7.4760e-02,  6.2209e-02,  2.4592e-02,\n",
       "                       6.1159e-02, -4.3913e-02, -2.7632e-02,  7.7467e-02,  7.5509e-02,\n",
       "                       1.6362e-02, -7.9730e-02,  5.8271e-02, -6.4644e-02,  5.9976e-02,\n",
       "                       1.2955e-02,  6.0949e-02, -9.5198e-02, -7.8816e-02,  4.4731e-02,\n",
       "                      -1.4776e-02, -2.4306e-02, -5.2604e-02, -3.1078e-02,  4.6357e-02,\n",
       "                       6.3037e-02,  5.6768e-02,  7.6620e-02, -1.0665e-03, -1.8868e-02,\n",
       "                       7.7108e-02, -7.3387e-02,  6.5835e-02,  6.3838e-02,  4.8859e-02,\n",
       "                       8.0437e-02,  6.1162e-02,  3.4130e-02, -5.7181e-02, -6.9928e-02,\n",
       "                       1.6856e-02, -7.1804e-02,  3.1909e-02, -4.8038e-02, -3.5448e-02,\n",
       "                      -5.7998e-02,  6.9299e-02,  2.8643e-02, -8.6518e-02,  5.9440e-02,\n",
       "                      -3.6423e-03,  3.4521e-02, -5.3647e-02, -3.8807e-02, -5.9618e-02,\n",
       "                      -9.1532e-03,  7.1104e-02, -7.9733e-02,  4.0161e-02, -8.0508e-02,\n",
       "                       2.4650e-02,  2.2925e-02, -2.5898e-02, -6.3223e-02, -6.0435e-03,\n",
       "                      -8.4218e-02,  4.7666e-03, -1.4547e-02, -9.5548e-02,  3.4545e-02,\n",
       "                      -8.3306e-02, -3.2718e-02, -9.2658e-02,  4.6933e-02, -6.7945e-02,\n",
       "                       4.1363e-02,  5.2213e-02,  7.7567e-02, -7.3246e-02, -6.1061e-02,\n",
       "                      -5.7662e-02,  8.0900e-02,  5.5009e-02,  4.7798e-02,  9.0128e-02,\n",
       "                       1.9463e-02, -9.1868e-02, -2.1705e-02, -8.6700e-02, -7.8392e-02,\n",
       "                      -8.6902e-02, -8.5897e-02,  2.7171e-02,  7.8646e-02, -2.5588e-01,\n",
       "                      -6.0121e-02,  5.2525e-02,  7.4374e-02, -8.6074e-02, -4.4233e-02,\n",
       "                      -8.6732e-02, -1.9563e-01, -8.5511e-02, -2.5117e-02,  8.0578e-02,\n",
       "                       5.6662e-02,  7.6807e-02, -4.8380e-02,  2.7498e-02, -5.9391e-02,\n",
       "                      -8.6947e-02,  3.7050e-02,  3.3965e-02, -1.8784e-02,  3.5454e-02,\n",
       "                      -8.5910e-02, -6.3398e-02, -7.3261e-02, -2.2482e-01,  5.9640e-03,\n",
       "                      -6.6248e-02, -8.2427e-02, -8.4558e-02,  3.3921e-02,  4.5261e-02,\n",
       "                       4.4873e-02,  7.6536e-02, -1.3940e-01, -3.7023e-02, -8.1098e-02,\n",
       "                       8.0470e-02, -1.1043e-01, -3.7207e-02, -8.7081e-02, -9.2795e-02,\n",
       "                       6.6231e-02, -6.7446e-02,  6.6250e-02, -7.4249e-02,  5.7878e-02,\n",
       "                       2.7555e-02,  5.6925e-02,  6.1478e-02, -8.4379e-02, -3.9943e-02,\n",
       "                      -8.6971e-02,  2.9370e-02,  5.2202e-02, -5.3746e-02,  4.8025e-02,\n",
       "                       5.0979e-02,  6.2652e-02, -8.1631e-02, -5.1032e-02, -1.5826e-02,\n",
       "                       4.6883e-02, -9.0549e-02,  2.4968e-03,  5.6627e-02, -7.3969e-02,\n",
       "                      -6.5363e-02,  2.0877e-02,  3.2333e-02,  3.0753e-02, -7.7430e-02,\n",
       "                      -5.5490e-02, -3.0022e-02, -6.8864e-02, -4.1132e-02,  6.4843e-02,\n",
       "                      -7.9442e-02, -8.4997e-02, -9.4097e-02, -6.2460e-02, -8.0245e-02,\n",
       "                      -3.6551e-02,  7.1584e-03,  3.9964e-02, -8.9841e-02, -7.6178e-02,\n",
       "                      -5.5926e-02, -1.0317e-01, -8.4512e-02, -2.4909e-02,  5.2717e-02,\n",
       "                      -4.9037e-02, -6.6861e-02,  7.0317e-02, -7.5001e-02,  6.7355e-02,\n",
       "                      -2.1905e-02, -8.7495e-02,  4.6155e-02, -6.1553e-02,  4.8392e-02,\n",
       "                      -3.4700e-02,  6.7543e-02,  2.7676e-02, -5.5388e-03,  5.8089e-02,\n",
       "                      -1.1978e-02,  7.1135e-02, -9.0743e-02,  8.2399e-02, -3.2397e-02,\n",
       "                       5.5815e-02, -9.4828e-02, -2.9226e-02,  7.2552e-02, -4.8179e-02,\n",
       "                       1.4308e-02, -6.6713e-02,  3.7106e-02, -7.4665e-02, -2.3396e-02,\n",
       "                       6.0956e-02, -1.1134e-02, -3.3184e-02,  3.4383e-02,  4.6987e-02,\n",
       "                      -6.1628e-02,  7.5972e-02,  7.6624e-02,  4.2610e-02, -5.2270e-02,\n",
       "                      -3.1431e-02,  8.3725e-02,  6.2008e-02, -5.2698e-02, -7.8072e-02,\n",
       "                       4.2810e-02,  1.6778e-02, -8.7598e-02,  6.4964e-02, -7.8580e-02,\n",
       "                      -6.9626e-02,  5.0810e-02,  8.9339e-02,  4.7848e-02, -9.1252e-02,\n",
       "                       7.9420e-02, -4.6876e-02, -8.4583e-02,  8.1002e-02,  3.3679e-02,\n",
       "                      -2.4816e-02, -1.1853e-02, -5.8895e-02, -1.5317e-01, -8.9484e-02,\n",
       "                      -4.3625e-02,  6.3159e-02,  9.3419e-02,  5.7194e-02, -4.9532e-02,\n",
       "                      -1.8075e-01, -2.6101e-02, -4.8860e-02,  8.3149e-02,  2.7972e-02,\n",
       "                      -4.0898e-02,  3.5376e-02, -6.2072e-02,  6.0327e-02, -2.0363e-02,\n",
       "                      -3.6640e-02, -8.6530e-02, -9.4839e-03, -5.0773e-02,  5.2177e-02,\n",
       "                       4.6946e-02,  5.5690e-02,  6.0449e-02, -2.0386e-03, -8.3630e-02,\n",
       "                       6.5968e-02,  8.5590e-03,  3.8017e-02,  9.1597e-02,  4.9060e-02,\n",
       "                      -5.5048e-02,  6.3485e-02,  4.2609e-02,  5.0836e-02,  7.0101e-02,\n",
       "                       2.9463e-02,  4.8627e-02,  2.8683e-02,  4.4889e-02,  1.4583e-03])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[ 2.3911e-02,  8.1227e-02, -7.9317e-03,  4.0350e-02,  3.3758e-02,\n",
       "                       -6.1104e-02,  4.7183e-02, -6.9952e-02, -7.5633e-02,  9.2336e-02,\n",
       "                       -6.8034e-02,  3.7680e-02,  5.1547e-02, -7.6076e-02,  4.9039e-02,\n",
       "                        1.0382e-01, -1.1840e-01,  4.2016e-02,  4.8482e-02, -2.0743e-02,\n",
       "                        3.0615e-02,  7.5488e-02,  5.0372e-02, -3.5599e-02,  5.9042e-02,\n",
       "                        4.0799e-02, -1.2854e-01, -9.4771e-02, -9.8277e-02,  3.3187e-02,\n",
       "                        1.2608e-01,  8.4792e-02,  8.4155e-02,  7.1969e-02,  5.3796e-02,\n",
       "                       -5.3576e-02, -6.9668e-02,  7.2410e-02,  6.7988e-02, -6.6482e-02,\n",
       "                       -5.8896e-02,  1.6303e-02,  7.7297e-03,  2.0285e-02,  6.9213e-02,\n",
       "                       -1.1610e-02,  9.2483e-02, -4.6568e-02, -4.6224e-02,  8.5577e-02,\n",
       "                       -6.6663e-02,  5.2331e-02,  3.2157e-02,  3.7833e-02,  1.7422e-02,\n",
       "                        4.8951e-02, -1.0443e-01, -6.4988e-02, -3.9329e-02,  9.7004e-02,\n",
       "                        6.6015e-02,  6.4310e-02,  3.5960e-03, -7.3711e-02,  3.7616e-02,\n",
       "                        6.1896e-02,  6.4541e-02, -6.0598e-02, -3.4240e-02, -8.1930e-02,\n",
       "                       -7.2448e-02,  2.1523e-02, -5.2747e-02,  6.1701e-02, -3.7155e-02,\n",
       "                        7.2233e-02,  1.1123e-01, -5.1903e-03,  1.1189e-01, -4.4336e-02,\n",
       "                       -8.0807e-02, -1.8930e-02,  4.8338e-02,  1.1389e-01,  8.5464e-02,\n",
       "                        9.4433e-02, -1.2293e-01,  7.2249e-02, -1.1067e-01,  4.2682e-01,\n",
       "                        4.4390e-02,  5.4644e-02,  6.6209e-02,  7.7460e-02, -4.6790e-02,\n",
       "                       -6.6399e-02, -4.1404e-02,  6.6584e-02,  8.9616e-02, -2.4085e-02,\n",
       "                        2.8753e-02, -4.4232e-02,  2.6383e-02,  6.5951e-02,  6.3958e-02,\n",
       "                       -6.0698e-02, -1.0712e-01, -8.0214e-02,  5.2829e-02, -2.0952e-02,\n",
       "                        7.0955e-02, -8.3220e-02,  7.7505e-02,  2.5945e-02,  2.2622e-02,\n",
       "                       -6.6991e-02, -9.1723e-02,  9.7060e-02,  7.6141e-03, -6.5276e-02,\n",
       "                        4.2944e-02,  4.2902e-02,  2.7758e-02, -7.6573e-02,  2.7406e-02,\n",
       "                        5.5999e-02,  6.5501e-02,  8.5541e-02,  8.5296e-02,  9.4380e-02,\n",
       "                       -9.6237e-02, -9.2753e-02,  9.5276e-02,  1.2653e-02,  8.4316e-02,\n",
       "                       -3.9884e-02,  7.5652e-02,  7.6750e-02,  7.6055e-02,  8.6230e-02,\n",
       "                       -7.5896e-02,  3.8614e-02,  5.5798e-02,  1.6033e-02, -5.2278e-02,\n",
       "                        4.7678e-02, -5.9526e-02, -8.0598e-03,  5.6900e-02, -5.6175e-02,\n",
       "                        7.8481e-02, -3.2571e-02, -9.9982e-02,  6.3759e-02,  9.0702e-02,\n",
       "                       -1.2432e-01,  3.2540e-02,  1.0138e-01,  4.6259e-02,  2.3882e-02,\n",
       "                        2.9731e-02, -6.0800e-02,  2.2534e-02,  5.4663e-02, -3.6506e-02,\n",
       "                       -4.8108e-02,  7.2895e-02, -4.3257e-02, -2.9588e-02,  1.0309e-01,\n",
       "                        5.4281e-02,  7.8651e-02, -9.8258e-02,  1.1573e-01, -4.6150e-02,\n",
       "                        7.9918e-02,  6.4953e-02, -2.1203e-02,  1.0464e-02, -7.8117e-02,\n",
       "                       -1.0975e-01,  8.1398e-02,  1.0820e-02, -1.3855e-02, -6.3385e-02,\n",
       "                       -1.0114e-01, -4.6449e-02,  4.7657e-02, -1.0264e-01,  7.8948e-02,\n",
       "                       -3.2106e-02,  1.7095e-02, -8.7100e-02, -1.0361e-02, -4.5241e-02,\n",
       "                       -4.5826e-02,  5.8220e-02,  7.4359e-02, -1.0586e-01, -1.6419e-02,\n",
       "                        4.5079e-02, -9.9084e-02,  5.8943e-02, -1.0870e-01, -5.3892e-02,\n",
       "                       -2.4860e-03,  2.6971e-02, -6.0605e-02, -7.5539e-02, -4.9756e-02,\n",
       "                       -7.7954e-02, -8.4190e-02, -9.7956e-02,  7.4803e-02, -2.8348e-03,\n",
       "                        3.3319e-02,  2.6750e-02, -1.6053e-02, -9.1906e-02, -4.7139e-02,\n",
       "                       -4.4852e-02, -3.1281e-03, -5.2927e-02, -2.5956e-02, -1.0144e-01,\n",
       "                       -8.2818e-02, -8.8281e-02, -1.8444e-02,  7.0021e-03,  5.7209e-02,\n",
       "                        3.8367e-02,  1.2615e-02,  3.5453e-02,  7.7247e-02,  5.2471e-02,\n",
       "                       -6.2583e-02,  9.4165e-02, -8.1155e-02, -2.9239e-03,  7.0806e-02,\n",
       "                       -8.8227e-02,  2.9730e-02, -4.9002e-02, -4.2183e-02,  5.6108e-02,\n",
       "                        8.2109e-02, -5.3570e-02,  8.5685e-03,  6.9911e-02,  2.5064e-02,\n",
       "                       -1.1629e-02,  5.6943e-02,  3.5812e-02,  1.8957e-02,  2.1526e-02,\n",
       "                        6.8513e-02,  1.0626e-01,  2.5546e-02,  3.5414e-02, -6.2162e-02,\n",
       "                        9.4343e-02, -6.2402e-02,  3.9381e-02,  1.0544e-01, -5.2001e-02,\n",
       "                       -5.7101e-02,  7.5279e-02,  5.0872e-02,  9.3719e-02,  7.4446e-02,\n",
       "                       -5.4233e-02,  1.5987e-02, -1.0155e-02, -8.8949e-02,  6.5792e-02,\n",
       "                       -2.7771e-02,  3.3873e-02,  4.0671e-02,  4.5722e-02,  4.7204e-02,\n",
       "                       -8.3910e-03, -7.4729e-02, -3.8025e-02, -6.3996e-02, -6.1659e-02,\n",
       "                        3.8499e-02, -5.6788e-02,  8.6328e-02,  6.1762e-02, -9.7204e-02,\n",
       "                       -2.0464e-02, -8.0618e-02,  6.0524e-02,  5.3715e-02,  1.2151e-01,\n",
       "                        2.7846e-02, -7.9200e-02, -1.9444e-02, -4.7596e-02, -5.0864e-02,\n",
       "                       -4.8767e-02,  4.2538e-02, -6.3060e-02,  1.0872e-01, -7.9507e-02,\n",
       "                       -3.8214e-02, -8.0417e-02, -9.5485e-02,  8.5847e-02, -9.0335e-02,\n",
       "                       -6.8927e-02,  3.2915e-02,  7.4659e-02,  1.7071e-02, -9.7856e-02,\n",
       "                       -1.0098e-01,  1.1264e-01, -9.8035e-03,  4.3950e-02, -7.1604e-02,\n",
       "                        4.1154e-03,  7.7696e-02,  4.8415e-02,  7.1630e-02,  5.1947e-02,\n",
       "                       -1.8122e-02,  8.4467e-02,  5.5709e-02,  6.0082e-02,  5.3795e-02,\n",
       "                        3.1288e-02, -2.9563e-02,  1.9108e-02, -5.5372e-02,  5.5104e-02,\n",
       "                       -9.9658e-02, -7.8003e-02,  1.4515e-01,  3.2258e-03,  5.3315e-02,\n",
       "                       -7.7771e-02, -1.6436e-01, -5.8986e-02, -4.1317e-02, -5.8611e-02,\n",
       "                        6.6432e-02,  5.8096e-02, -7.8772e-02, -4.2226e-02, -5.5155e-02,\n",
       "                        8.5267e-02,  7.7177e-02,  6.0558e-02,  8.0572e-02,  8.8583e-02,\n",
       "                        3.7903e-02,  5.9293e-02,  9.6277e-02,  4.1642e-02, -9.5089e-02,\n",
       "                       -7.3732e-02, -5.5580e-02, -3.0225e-02,  2.2411e-02,  9.8599e-02,\n",
       "                       -4.5290e-02, -3.2179e-02,  5.7074e-02,  3.5497e-02,  3.0493e-02,\n",
       "                       -2.6052e-02,  3.7862e-02, -2.8118e-02, -6.9403e-02,  8.1733e-02,\n",
       "                        9.1357e-02, -2.2931e-02,  1.7613e-02,  1.8889e-01,  8.8439e-02,\n",
       "                       -1.7781e-02,  7.3561e-02,  5.8175e-02,  8.8142e-03, -4.1221e-02,\n",
       "                        5.0207e-02,  4.1338e-02, -6.7674e-02,  4.9575e-02,  2.4245e-02,\n",
       "                        6.7239e-02,  1.5940e-02,  5.9763e-02, -4.9121e-02,  8.7702e-02,\n",
       "                        6.6012e-02,  3.5138e-02, -6.8973e-02,  3.2763e-02,  8.3215e-02,\n",
       "                        8.0423e-02,  1.8707e-02,  2.1671e-02,  6.4443e-02,  6.0333e-02,\n",
       "                        4.9253e-02, -6.9407e-02,  4.8080e-02, -5.4688e-02,  8.9542e-02,\n",
       "                        2.2289e-03, -5.4479e-02, -1.6318e-02, -7.7036e-02,  8.6561e-02,\n",
       "                       -1.1202e-01, -1.0396e-02, -1.4452e-02, -7.0150e-02,  1.4844e-02,\n",
       "                        4.5640e-02,  5.2793e-02,  9.4535e-02, -2.9485e-02,  5.0606e-02,\n",
       "                       -1.1505e-01,  6.7732e-02,  8.9483e-03,  1.0106e-01, -5.3538e-02,\n",
       "                        5.1231e-02, -8.2219e-02,  5.8243e-02,  7.6901e-02, -4.6902e-02,\n",
       "                        3.3395e-02,  9.4432e-02,  3.0633e-02,  3.6230e-02, -3.3285e-02,\n",
       "                        7.7172e-02,  1.0866e-01, -8.5610e-02,  9.2313e-02, -2.2830e-02,\n",
       "                        3.6809e-02,  7.3373e-02, -1.0195e-01,  8.9681e-02,  8.1781e-02,\n",
       "                        8.8621e-02, -4.7444e-02,  1.6941e-02,  1.0538e-01,  5.6794e-02,\n",
       "                       -1.3510e-02,  9.2057e-02, -4.5416e-02, -5.3404e-02,  3.1530e-02,\n",
       "                        4.8545e-02,  8.6943e-02, -1.1038e-01, -1.4379e-02,  8.4229e-03,\n",
       "                       -4.3955e-02,  6.2253e-02,  3.8305e-02, -7.2530e-02,  3.2712e-02,\n",
       "                        6.8848e-02,  6.1400e-02,  4.2461e-02, -9.2516e-02,  8.2046e-02,\n",
       "                       -3.7583e-02,  2.8068e-02, -6.4833e-02,  4.4664e-03,  8.7906e-02,\n",
       "                       -2.4053e-02,  6.5795e-02, -8.4105e-02,  6.5884e-02,  2.3901e-02,\n",
       "                       -8.5794e-02, -1.0634e-02,  3.6438e-02, -1.0483e-01,  4.3447e-02,\n",
       "                        7.0432e-02, -9.7482e-02,  1.0121e-01, -5.6362e-02, -1.0055e-01,\n",
       "                       -1.0948e-02, -3.4704e-02, -5.2948e-02,  6.2168e-02,  2.1303e-02],\n",
       "                      [-7.7170e-02, -3.7578e-02,  6.3233e-02, -7.1494e-02, -6.5203e-02,\n",
       "                        4.0188e-02, -6.7318e-03,  6.7116e-02,  7.9219e-02, -1.1943e-01,\n",
       "                        7.3614e-02, -6.1745e-02, -9.2983e-02,  8.2484e-02, -1.3313e-02,\n",
       "                       -1.0025e-01,  1.2557e-01, -7.4308e-02, -3.6916e-02, -4.0555e-02,\n",
       "                       -6.5678e-02, -7.0610e-02, -1.3757e-02,  8.6117e-02, -3.9592e-02,\n",
       "                       -4.4606e-02,  9.6059e-02,  8.3587e-02,  1.1263e-01, -7.3937e-02,\n",
       "                       -1.0654e-01, -2.5705e-02, -8.3051e-02, -7.2829e-02, -4.7456e-03,\n",
       "                        5.5835e-02,  7.3764e-02, -2.1039e-02, -8.3838e-02,  3.9597e-02,\n",
       "                        2.7163e-02, -9.6237e-02,  2.1229e-02, -3.9420e-02, -7.2133e-02,\n",
       "                       -4.0866e-02, -5.8986e-02, -2.7151e-03,  8.8110e-02, -7.0198e-02,\n",
       "                        8.2654e-02, -4.3064e-02,  2.6849e-02, -8.4275e-02, -8.0418e-02,\n",
       "                       -8.7593e-02,  8.7926e-02,  2.9057e-02,  3.3061e-02, -4.3434e-02,\n",
       "                       -7.2652e-02, -7.8689e-02,  7.8288e-02,  8.0804e-02,  1.9615e-02,\n",
       "                       -3.2910e-02, -1.7240e-02,  1.3437e-02, -7.7279e-03,  8.6143e-02,\n",
       "                        6.7855e-02, -8.1500e-02,  9.0080e-02, -7.1187e-02,  3.0156e-02,\n",
       "                       -1.0377e-01, -1.2299e-01,  1.5323e-02, -5.2226e-02,  4.4546e-02,\n",
       "                        1.1118e-01, -9.5326e-05, -9.2666e-02, -1.1665e-01, -1.2198e-01,\n",
       "                       -8.6055e-02,  7.3478e-02, -7.4331e-02,  1.0854e-01, -4.1126e-01,\n",
       "                       -1.0139e-01, -4.2274e-02, -1.2423e-01, -8.6267e-02,  7.2219e-02,\n",
       "                        3.6111e-02,  6.6243e-02, -2.1510e-02, -3.4437e-02,  1.5211e-03,\n",
       "                        4.2911e-02,  4.3055e-02, -4.4043e-02, -2.7240e-02, -4.7278e-02,\n",
       "                        9.2245e-02,  5.2056e-02,  6.3902e-02, -1.9195e-02,  6.7816e-02,\n",
       "                       -9.0792e-02,  4.1311e-02, -9.4449e-02, -4.8609e-02,  6.8167e-04,\n",
       "                        1.0337e-01,  3.4884e-02, -5.4649e-02, -5.5068e-02,  7.0014e-02,\n",
       "                       -9.8476e-02, -1.1015e-02, -6.2032e-02,  5.0942e-02, -6.6460e-02,\n",
       "                       -5.3224e-02, -6.7674e-02, -5.8736e-02, -1.1304e-01, -7.3447e-02,\n",
       "                        5.4838e-02,  4.8197e-02, -2.8501e-02, -1.0640e-02, -1.7352e-02,\n",
       "                       -1.7900e-03, -1.0835e-02, -5.9114e-02, -9.7867e-02, -1.0894e-02,\n",
       "                        1.0403e-01, -5.6431e-02, -1.0075e-01, -3.5794e-02,  2.8531e-02,\n",
       "                       -3.6615e-02,  3.8831e-02,  7.9561e-02, -5.8604e-02,  4.2041e-02,\n",
       "                       -5.7679e-02, -3.6982e-02,  9.2151e-02, -3.4867e-02, -9.6122e-02,\n",
       "                        1.3187e-01, -2.6198e-02, -6.1951e-02, -6.3050e-02, -1.7372e-02,\n",
       "                       -3.1022e-02,  5.1723e-02, -4.1014e-02, -4.4114e-02,  7.8227e-02,\n",
       "                        8.7333e-02, -7.0092e-02,  9.0590e-02,  2.7886e-02, -4.3160e-02,\n",
       "                       -4.6502e-02, -6.1724e-02,  9.2785e-02, -1.0997e-01,  6.0616e-02,\n",
       "                       -1.0292e-01, -6.0843e-02,  8.3189e-02, -3.7204e-02,  8.4530e-02,\n",
       "                        6.1001e-02, -4.0034e-02, -1.7517e-03, -3.3935e-03,  7.2702e-02,\n",
       "                        5.9638e-02,  9.0463e-02, -1.0320e-01,  6.6178e-02, -9.5436e-02,\n",
       "                        4.6275e-02, -7.0896e-02,  8.2039e-02,  2.1649e-03,  1.0352e-03,\n",
       "                        3.4096e-02, -5.3046e-02, -7.3419e-02,  7.5990e-02,  3.6416e-02,\n",
       "                       -2.0662e-02,  2.6881e-02, -7.5140e-02,  1.2126e-01,  4.9898e-02,\n",
       "                       -1.9691e-02, -9.3033e-02,  7.6689e-03,  7.9883e-02,  8.4580e-02,\n",
       "                        7.8496e-02,  3.1230e-02,  8.0265e-02, -6.8262e-02,  6.6671e-02,\n",
       "                       -7.2146e-02, -6.6539e-02,  2.5228e-02,  6.9749e-02,  9.0358e-02,\n",
       "                        5.2118e-02,  2.0151e-02,  8.3205e-02,  5.2646e-02,  8.9390e-02,\n",
       "                        5.0578e-02,  9.1755e-02,  5.4331e-03, -8.0386e-02, -1.0456e-01,\n",
       "                       -8.0245e-02, -4.8274e-03,  4.7168e-02, -7.8335e-02, -6.2380e-02,\n",
       "                        6.5497e-02, -5.7148e-02,  9.0952e-02,  4.7762e-02, -5.9773e-02,\n",
       "                        9.5130e-02, -3.2546e-02,  1.0543e-01,  9.7213e-02, -6.5253e-02,\n",
       "                       -9.1969e-02,  7.1261e-02, -7.4152e-02, -3.3046e-02, -3.7598e-02,\n",
       "                       -7.8740e-03, -2.9837e-02, -1.0054e-01, -2.2854e-02,  6.2838e-03,\n",
       "                       -4.9376e-02, -7.0563e-02, -8.5418e-02, -5.2993e-02,  6.7744e-02,\n",
       "                       -6.0553e-02,  5.9665e-02, -1.1435e-02, -1.0069e-01,  3.0501e-02,\n",
       "                        1.3569e-02, -6.7218e-02, -6.1073e-02, -9.2498e-02, -1.6647e-02,\n",
       "                        6.3447e-02, -7.1403e-02,  6.6401e-02,  7.0886e-02, -4.6911e-02,\n",
       "                        8.6747e-02, -9.5646e-02, -8.1293e-02, -6.9313e-02, -5.7655e-02,\n",
       "                       -5.4731e-02,  5.3022e-03,  2.6063e-02,  1.0216e-01,  5.6260e-02,\n",
       "                       -8.0848e-02,  9.1053e-02, -1.0603e-01, -4.7196e-02,  3.7671e-02,\n",
       "                       -1.5849e-02,  7.0774e-02, -6.4204e-02, -8.9512e-02, -5.0296e-02,\n",
       "                       -1.1165e-01,  1.6899e-02,  2.1736e-02,  1.5556e-02, -6.2254e-03,\n",
       "                        1.7684e-02, -5.3605e-02,  6.7125e-02, -7.1309e-02,  1.0618e-02,\n",
       "                        7.9041e-02,  7.0581e-02,  8.5059e-02, -2.0380e-02,  2.6070e-02,\n",
       "                       -1.0754e-03, -7.4118e-02, -3.0053e-02, -6.9003e-02,  5.1994e-02,\n",
       "                        6.6209e-02, -1.0866e-01, -2.1316e-02, -8.5063e-02,  2.1691e-02,\n",
       "                       -2.8044e-02, -4.6175e-02, -5.5388e-02, -4.2810e-02, -6.0101e-02,\n",
       "                        4.6547e-02, -1.6619e-02, -5.1753e-02, -1.2135e-01, -4.9160e-02,\n",
       "                       -3.5687e-02,  5.2294e-02, -5.1318e-02,  6.0708e-02, -6.7211e-02,\n",
       "                        4.7607e-02,  8.2650e-02, -1.1952e-01,  1.6734e-02, -3.3042e-02,\n",
       "                        8.1995e-02,  8.8668e-02,  2.8232e-02,  6.7093e-02,  5.0019e-02,\n",
       "                       -6.8020e-02, -3.5195e-02,  9.1836e-02,  6.6712e-02,  7.6737e-02,\n",
       "                       -6.6430e-02, -3.9990e-02, -8.7432e-02, -3.6278e-02, -3.9917e-02,\n",
       "                       -1.7196e-02, -8.7672e-02, -3.3400e-02, -1.6722e-02,  1.1089e-01,\n",
       "                        6.2080e-02,  1.2184e-01,  2.0785e-02,  2.6773e-02, -6.4677e-02,\n",
       "                        4.0094e-02,  4.7102e-02, -4.7576e-02, -7.8679e-02, -5.5264e-02,\n",
       "                        9.1925e-02, -9.8906e-02,  1.6700e-02,  9.7978e-02, -3.6178e-02,\n",
       "                       -2.0058e-02, -2.0137e-02, -9.7694e-02, -1.7266e-01, -4.3293e-02,\n",
       "                        5.4814e-02, -1.6397e-02, -5.2601e-02,  1.9774e-02, -6.0536e-03,\n",
       "                       -7.7729e-02, -5.8849e-02,  9.1605e-02, -8.0422e-03, -5.1428e-02,\n",
       "                       -4.8721e-02, -3.0828e-02, -3.2924e-02,  1.1143e-02, -7.6220e-02,\n",
       "                       -8.4068e-03, -5.1598e-02,  7.7186e-02, -5.5434e-02, -3.9523e-02,\n",
       "                       -3.8236e-02, -5.8362e-02, -7.7772e-02, -3.5858e-02, -7.0145e-02,\n",
       "                        1.5984e-02,  7.1223e-02, -6.9048e-03,  6.9479e-02, -4.5821e-02,\n",
       "                        2.4098e-02,  1.0862e-01,  2.1649e-02,  1.1448e-01, -6.8666e-02,\n",
       "                        6.1119e-02,  2.8104e-02, -2.2539e-02,  7.9446e-02, -7.8175e-02,\n",
       "                       -9.2122e-02, -6.3149e-02, -7.0866e-02,  4.9939e-02, -4.7537e-02,\n",
       "                        6.8556e-02, -8.5077e-02, -4.1860e-02, -9.8511e-02,  1.1643e-01,\n",
       "                       -5.9760e-02,  5.9753e-02, -9.6105e-02, -7.7307e-02,  4.2426e-02,\n",
       "                       -7.0481e-02, -1.0530e-01, -7.1832e-02, -3.6528e-02,  8.5896e-02,\n",
       "                       -6.6766e-02, -4.1110e-02,  8.4858e-02, -6.1365e-02,  4.8216e-02,\n",
       "                       -5.1356e-02, -3.7606e-02,  8.2928e-02, -9.3269e-02, -3.4766e-02,\n",
       "                       -9.9844e-02,  2.1503e-02, -3.1612e-02, -1.1183e-01, -4.8016e-02,\n",
       "                        6.5231e-02, -1.7524e-02,  1.6391e-02,  3.6737e-02, -5.2295e-02,\n",
       "                        6.5534e-03, -6.3734e-02,  7.7036e-02,  6.7772e-02,  7.9034e-04,\n",
       "                        3.9836e-02, -7.0914e-02, -4.1250e-02,  7.6141e-02, -7.3173e-02,\n",
       "                       -3.2362e-02, -8.2069e-02, -9.4161e-02,  6.6781e-02, -5.9942e-02,\n",
       "                        5.4046e-02, -2.5394e-02,  6.8210e-02,  3.2223e-02, -8.3900e-02,\n",
       "                        2.5726e-02, -6.7746e-02,  6.0469e-02, -7.5293e-02, -5.5281e-02,\n",
       "                        6.0851e-02, -1.2768e-02, -7.7965e-02,  7.9914e-02, -6.6351e-02,\n",
       "                       -1.4580e-02,  9.2910e-02, -3.0325e-02,  1.1250e-01,  1.0262e-01,\n",
       "                       -4.9943e-02,  1.1786e-01,  1.0873e-01, -5.7069e-02,  2.1716e-02]])),\n",
       "             ('layer3.bias', tensor([-0.0305,  0.0049]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
