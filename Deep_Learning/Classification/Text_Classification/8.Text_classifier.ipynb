{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter='\\t')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  1000 non-null   object\n",
      " 1   Liked   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "import re\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    new_row = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    new_row = new_row.lower()\n",
    "    new_row = new_row.split()\n",
    "    clean_row = [ps.stem(word) for word in new_row if word not in (stopwords.words('english'))]\n",
    "    clean_row = ' '.join(clean_row)\n",
    "    corpus.append(clean_row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into numeric vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df = 3, max_df = 0.6)\n",
    "text_vectors = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 467), (200, 467), (800,), (200,))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(text_vectors, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 467\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size =32\n",
    "hidden_size = 500\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textclassifier(\n",
      "  (layer1): Linear(in_features=467, out_features=500, bias=True)\n",
      "  (layer2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (layer3): Linear(in_features=500, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Textclassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Textclassifier, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        output = F.relu(self.layer1(x))\n",
    "        #print(output.shape)\n",
    "        output = F.relu(self.layer2(output))\n",
    "        #print(output.shape)\n",
    "        output = self.layer3(output)\n",
    "        #print(output.shape)\n",
    "        #print(F.log_softmax(output, dim=1).shape)\n",
    "        return F.log_softmax(output, dim=1)\n",
    "    \n",
    "model = Textclassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.tensor(np.array(X_train), dtype = torch.float32, requires_grad=True)\n",
    "#y_train = torch.tensor(np.array(y_train).reshape(-1,1), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800, 467]), torch.Size([800]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 0.6933846473693848\n",
      "Epoch : 1, Loss : 0.6666982769966125\n",
      "Epoch : 2, Loss : 0.5024487972259521\n",
      "Epoch : 3, Loss : 0.3084805905818939\n",
      "Epoch : 4, Loss : 0.19375364482402802\n",
      "Epoch : 5, Loss : 0.15536852180957794\n",
      "Epoch : 6, Loss : 0.11050688475370407\n",
      "Epoch : 7, Loss : 0.07044658809900284\n",
      "Epoch : 8, Loss : 0.06507411599159241\n",
      "Epoch : 9, Loss : 0.054927147924900055\n",
      "Epoch : 10, Loss : 0.0403677262365818\n",
      "Epoch : 11, Loss : 0.03444969654083252\n",
      "Epoch : 12, Loss : 0.03334742784500122\n",
      "Epoch : 13, Loss : 0.03017677739262581\n",
      "Epoch : 14, Loss : 0.027559921145439148\n",
      "Epoch : 15, Loss : 0.024322323501110077\n",
      "Epoch : 16, Loss : 0.024343283846974373\n",
      "Epoch : 17, Loss : 0.02520822547376156\n",
      "Epoch : 18, Loss : 0.02617214247584343\n",
      "Epoch : 19, Loss : 0.024289477616548538\n",
      "Epoch : 20, Loss : 0.02056623063981533\n",
      "Epoch : 21, Loss : 0.0200934037566185\n",
      "Epoch : 22, Loss : 0.023264706134796143\n",
      "Epoch : 23, Loss : 0.022025292739272118\n",
      "Epoch : 24, Loss : 0.019808899611234665\n",
      "Epoch : 25, Loss : 0.020084721967577934\n",
      "Epoch : 26, Loss : 0.021127063781023026\n",
      "Epoch : 27, Loss : 0.02085895463824272\n",
      "Epoch : 28, Loss : 0.020324569195508957\n",
      "Epoch : 29, Loss : 0.019910959526896477\n",
      "Epoch : 30, Loss : 0.01951363869011402\n",
      "Epoch : 31, Loss : 0.02001323737204075\n",
      "Epoch : 32, Loss : 0.020140085369348526\n",
      "Epoch : 33, Loss : 0.019665421918034554\n",
      "Epoch : 34, Loss : 0.019314607605338097\n",
      "Epoch : 35, Loss : 0.019443446770310402\n",
      "Epoch : 36, Loss : 0.019894255325198174\n",
      "Epoch : 37, Loss : 0.019430436193943024\n",
      "Epoch : 38, Loss : 0.01903141662478447\n",
      "Epoch : 39, Loss : 0.019327955320477486\n",
      "Epoch : 40, Loss : 0.01960650086402893\n",
      "Epoch : 41, Loss : 0.01933402381837368\n",
      "Epoch : 42, Loss : 0.019055593758821487\n",
      "Epoch : 43, Loss : 0.019127734005451202\n",
      "Epoch : 44, Loss : 0.0193826574832201\n",
      "Epoch : 45, Loss : 0.019181037321686745\n",
      "Epoch : 46, Loss : 0.019037604331970215\n",
      "Epoch : 47, Loss : 0.019107598811388016\n",
      "Epoch : 48, Loss : 0.01922154240310192\n",
      "Epoch : 49, Loss : 0.01910868100821972\n",
      "Epoch : 50, Loss : 0.019023003056645393\n",
      "Epoch : 51, Loss : 0.019064337015151978\n",
      "Epoch : 52, Loss : 0.019128495827317238\n",
      "Epoch : 53, Loss : 0.019060594961047173\n",
      "Epoch : 54, Loss : 0.019004369154572487\n",
      "Epoch : 55, Loss : 0.019030705094337463\n",
      "Epoch : 56, Loss : 0.019082164391875267\n",
      "Epoch : 57, Loss : 0.019023016095161438\n",
      "Epoch : 58, Loss : 0.018981970846652985\n",
      "Epoch : 59, Loss : 0.019024435430765152\n",
      "Epoch : 60, Loss : 0.01904786378145218\n",
      "Epoch : 61, Loss : 0.019000092521309853\n",
      "Epoch : 62, Loss : 0.018977558240294456\n",
      "Epoch : 63, Loss : 0.019015753641724586\n",
      "Epoch : 64, Loss : 0.01901642046868801\n",
      "Epoch : 65, Loss : 0.01898445188999176\n",
      "Epoch : 66, Loss : 0.018982546404004097\n",
      "Epoch : 67, Loss : 0.01900476962327957\n",
      "Epoch : 68, Loss : 0.018995199352502823\n",
      "Epoch : 69, Loss : 0.018979940563440323\n",
      "Epoch : 70, Loss : 0.018983084708452225\n",
      "Epoch : 71, Loss : 0.018993373960256577\n",
      "Epoch : 72, Loss : 0.018984751775860786\n",
      "Epoch : 73, Loss : 0.018974803388118744\n",
      "Epoch : 74, Loss : 0.018982773646712303\n",
      "Epoch : 75, Loss : 0.01898631453514099\n",
      "Epoch : 76, Loss : 0.018976954743266106\n",
      "Epoch : 77, Loss : 0.01897364668548107\n",
      "Epoch : 78, Loss : 0.018982548266649246\n",
      "Epoch : 79, Loss : 0.018979614600539207\n",
      "Epoch : 80, Loss : 0.018972324207425117\n",
      "Epoch : 81, Loss : 0.01897597499191761\n",
      "Epoch : 82, Loss : 0.01897864043712616\n",
      "Epoch : 83, Loss : 0.018974680453538895\n",
      "Epoch : 84, Loss : 0.018972322344779968\n",
      "Epoch : 85, Loss : 0.0189756378531456\n",
      "Epoch : 86, Loss : 0.01897539757192135\n",
      "Epoch : 87, Loss : 0.018972091376781464\n",
      "Epoch : 88, Loss : 0.01897295005619526\n",
      "Epoch : 89, Loss : 0.01897432841360569\n",
      "Epoch : 90, Loss : 0.018972797319293022\n",
      "Epoch : 91, Loss : 0.018971415236592293\n",
      "Epoch : 92, Loss : 0.018972955644130707\n",
      "Epoch : 93, Loss : 0.018972957506775856\n",
      "Epoch : 94, Loss : 0.018970901146531105\n",
      "Epoch : 95, Loss : 0.018971793353557587\n",
      "Epoch : 96, Loss : 0.01897248439490795\n",
      "Epoch : 97, Loss : 0.018970975652337074\n",
      "Epoch : 98, Loss : 0.01897093839943409\n",
      "Epoch : 99, Loss : 0.0189716424793005\n",
      "Epoch : 100, Loss : 0.018971096724271774\n",
      "Epoch : 101, Loss : 0.018970543518662453\n",
      "Epoch : 102, Loss : 0.018970845267176628\n",
      "Epoch : 103, Loss : 0.018971001729369164\n",
      "Epoch : 104, Loss : 0.018970390781760216\n",
      "Epoch : 105, Loss : 0.01897032931447029\n",
      "Epoch : 106, Loss : 0.01897071674466133\n",
      "Epoch : 107, Loss : 0.018970275297760963\n",
      "Epoch : 108, Loss : 0.01897004432976246\n",
      "Epoch : 109, Loss : 0.01897040195763111\n",
      "Epoch : 110, Loss : 0.018970133736729622\n",
      "Epoch : 111, Loss : 0.01896986924111843\n",
      "Epoch : 112, Loss : 0.018970103934407234\n",
      "Epoch : 113, Loss : 0.018969988450407982\n",
      "Epoch : 114, Loss : 0.018969738855957985\n",
      "Epoch : 115, Loss : 0.018969839438796043\n",
      "Epoch : 116, Loss : 0.018969831988215446\n",
      "Epoch : 117, Loss : 0.01896963082253933\n",
      "Epoch : 118, Loss : 0.018969621509313583\n",
      "Epoch : 119, Loss : 0.018969669938087463\n",
      "Epoch : 120, Loss : 0.018969517201185226\n",
      "Epoch : 121, Loss : 0.01896945759654045\n",
      "Epoch : 122, Loss : 0.018969513475894928\n",
      "Epoch : 123, Loss : 0.018969401717185974\n",
      "Epoch : 124, Loss : 0.018969332799315453\n",
      "Epoch : 125, Loss : 0.01896936260163784\n",
      "Epoch : 126, Loss : 0.018969282507896423\n",
      "Epoch : 127, Loss : 0.01896921917796135\n",
      "Epoch : 128, Loss : 0.018969230353832245\n",
      "Epoch : 129, Loss : 0.018969174474477768\n",
      "Epoch : 130, Loss : 0.018969107419252396\n",
      "Epoch : 131, Loss : 0.018969107419252396\n",
      "Epoch : 132, Loss : 0.01896907202899456\n",
      "Epoch : 133, Loss : 0.018969004973769188\n",
      "Epoch : 134, Loss : 0.018968995660543442\n",
      "Epoch : 135, Loss : 0.01896897330880165\n",
      "Epoch : 136, Loss : 0.018968911841511726\n",
      "Epoch : 137, Loss : 0.018968895077705383\n",
      "Epoch : 138, Loss : 0.018968874588608742\n",
      "Epoch : 139, Loss : 0.018968822434544563\n",
      "Epoch : 140, Loss : 0.01896880380809307\n",
      "Epoch : 141, Loss : 0.01896877959370613\n",
      "Epoch : 142, Loss : 0.018968738615512848\n",
      "Epoch : 143, Loss : 0.018968716263771057\n",
      "Epoch : 144, Loss : 0.018968690186738968\n",
      "Epoch : 145, Loss : 0.018968656659126282\n",
      "Epoch : 146, Loss : 0.018968632444739342\n",
      "Epoch : 147, Loss : 0.018968606367707253\n",
      "Epoch : 148, Loss : 0.018968574702739716\n",
      "Epoch : 149, Loss : 0.018968550488352776\n",
      "Epoch : 150, Loss : 0.018968529999256134\n",
      "Epoch : 151, Loss : 0.018968496471643448\n",
      "Epoch : 152, Loss : 0.018968475982546806\n",
      "Epoch : 153, Loss : 0.018968453630805016\n",
      "Epoch : 154, Loss : 0.018968425691127777\n",
      "Epoch : 155, Loss : 0.018968405202031136\n",
      "Epoch : 156, Loss : 0.018968380987644196\n",
      "Epoch : 157, Loss : 0.018968351185321808\n",
      "Epoch : 158, Loss : 0.018968328833580017\n",
      "Epoch : 159, Loss : 0.018968306481838226\n",
      "Epoch : 160, Loss : 0.018968282267451286\n",
      "Epoch : 161, Loss : 0.018968261778354645\n",
      "Epoch : 162, Loss : 0.018968243151903152\n",
      "Epoch : 163, Loss : 0.018968217074871063\n",
      "Epoch : 164, Loss : 0.01896819658577442\n",
      "Epoch : 165, Loss : 0.01896817609667778\n",
      "Epoch : 166, Loss : 0.01896815188229084\n",
      "Epoch : 167, Loss : 0.018968133255839348\n",
      "Epoch : 168, Loss : 0.018968112766742706\n",
      "Epoch : 169, Loss : 0.018968092277646065\n",
      "Epoch : 170, Loss : 0.018968071788549423\n",
      "Epoch : 171, Loss : 0.018968049436807632\n",
      "Epoch : 172, Loss : 0.01896803081035614\n",
      "Epoch : 173, Loss : 0.018968012183904648\n",
      "Epoch : 174, Loss : 0.018967991694808006\n",
      "Epoch : 175, Loss : 0.018967971205711365\n",
      "Epoch : 176, Loss : 0.018967948853969574\n",
      "Epoch : 177, Loss : 0.01896793395280838\n",
      "Epoch : 178, Loss : 0.018967915326356888\n",
      "Epoch : 179, Loss : 0.018967896699905396\n",
      "Epoch : 180, Loss : 0.018967878073453903\n",
      "Epoch : 181, Loss : 0.01896786130964756\n",
      "Epoch : 182, Loss : 0.018967844545841217\n",
      "Epoch : 183, Loss : 0.01896783336997032\n",
      "Epoch : 184, Loss : 0.018967824056744576\n",
      "Epoch : 185, Loss : 0.018967825919389725\n",
      "Epoch : 186, Loss : 0.018967844545841217\n",
      "Epoch : 187, Loss : 0.018967898562550545\n",
      "Epoch : 188, Loss : 0.01896803453564644\n",
      "Epoch : 189, Loss : 0.018968338146805763\n",
      "Epoch : 190, Loss : 0.018969034776091576\n",
      "Epoch : 191, Loss : 0.018970491364598274\n",
      "Epoch : 192, Loss : 0.01897393725812435\n",
      "Epoch : 193, Loss : 0.018981095403432846\n",
      "Epoch : 194, Loss : 0.01899684965610504\n",
      "Epoch : 195, Loss : 0.019017117097973824\n",
      "Epoch : 196, Loss : 0.019062386825680733\n",
      "Epoch : 197, Loss : 0.019094126299023628\n",
      "Epoch : 198, Loss : 0.01917298696935177\n",
      "Epoch : 199, Loss : 0.019150573760271072\n",
      "Epoch : 200, Loss : 0.019162558019161224\n",
      "Epoch : 201, Loss : 0.01907201111316681\n",
      "Epoch : 202, Loss : 0.019018856808543205\n",
      "Epoch : 203, Loss : 0.018977278843522072\n",
      "Epoch : 204, Loss : 0.0189681276679039\n",
      "Epoch : 205, Loss : 0.018983615562319756\n",
      "Epoch : 206, Loss : 0.01900983415544033\n",
      "Epoch : 207, Loss : 0.019045639783143997\n",
      "Epoch : 208, Loss : 0.01904796063899994\n",
      "Epoch : 209, Loss : 0.01906050741672516\n",
      "Epoch : 210, Loss : 0.019037507474422455\n",
      "Epoch : 211, Loss : 0.01902657561004162\n",
      "Epoch : 212, Loss : 0.018998252227902412\n",
      "Epoch : 213, Loss : 0.01898127608001232\n",
      "Epoch : 214, Loss : 0.018970418721437454\n",
      "Epoch : 215, Loss : 0.01896774210035801\n",
      "Epoch : 216, Loss : 0.018971633166074753\n",
      "Epoch : 217, Loss : 0.018979176878929138\n",
      "Epoch : 218, Loss : 0.018989454954862595\n",
      "Epoch : 219, Loss : 0.01899615116417408\n",
      "Epoch : 220, Loss : 0.019006092101335526\n",
      "Epoch : 221, Loss : 0.019005827605724335\n",
      "Epoch : 222, Loss : 0.019009191542863846\n",
      "Epoch : 223, Loss : 0.01900286041200161\n",
      "Epoch : 224, Loss : 0.0190009456127882\n",
      "Epoch : 225, Loss : 0.0189924668520689\n",
      "Epoch : 226, Loss : 0.01898825168609619\n",
      "Epoch : 227, Loss : 0.018981946632266045\n",
      "Epoch : 228, Loss : 0.01897820271551609\n",
      "Epoch : 229, Loss : 0.01897459477186203\n",
      "Epoch : 230, Loss : 0.01897253468632698\n",
      "Epoch : 231, Loss : 0.01897077076137066\n",
      "Epoch : 232, Loss : 0.018969768658280373\n",
      "Epoch : 233, Loss : 0.018969090655446053\n",
      "Epoch : 234, Loss : 0.01896870695054531\n",
      "Epoch : 235, Loss : 0.018968459218740463\n",
      "Epoch : 236, Loss : 0.01896841451525688\n",
      "Epoch : 237, Loss : 0.018968431279063225\n",
      "Epoch : 238, Loss : 0.018968623131513596\n",
      "Epoch : 239, Loss : 0.018969010561704636\n",
      "Epoch : 240, Loss : 0.018969763070344925\n",
      "Epoch : 241, Loss : 0.018971025943756104\n",
      "Epoch : 242, Loss : 0.01897362619638443\n",
      "Epoch : 243, Loss : 0.018977783620357513\n",
      "Epoch : 244, Loss : 0.018987154588103294\n",
      "Epoch : 245, Loss : 0.019002234563231468\n",
      "Epoch : 246, Loss : 0.01903817616403103\n",
      "Epoch : 247, Loss : 0.01908247359097004\n",
      "Epoch : 248, Loss : 0.019200634211301804\n",
      "Epoch : 249, Loss : 0.019257569685578346\n",
      "Epoch : 250, Loss : 0.01944088190793991\n",
      "Epoch : 251, Loss : 0.019311808049678802\n",
      "Epoch : 252, Loss : 0.019243884831666946\n",
      "Epoch : 253, Loss : 0.019074933603405952\n",
      "Epoch : 254, Loss : 0.01899867318570614\n",
      "Epoch : 255, Loss : 0.01896796189248562\n",
      "Epoch : 256, Loss : 0.01898847334086895\n",
      "Epoch : 257, Loss : 0.019046716392040253\n",
      "Epoch : 258, Loss : 0.019084623083472252\n",
      "Epoch : 259, Loss : 0.019133158028125763\n",
      "Epoch : 260, Loss : 0.019096896052360535\n",
      "Epoch : 261, Loss : 0.019063325598835945\n",
      "Epoch : 262, Loss : 0.019002534449100494\n",
      "Epoch : 263, Loss : 0.018972469493746758\n",
      "Epoch : 264, Loss : 0.018969707190990448\n",
      "Epoch : 265, Loss : 0.0189876239746809\n",
      "Epoch : 266, Loss : 0.019016757607460022\n",
      "Epoch : 267, Loss : 0.019030485302209854\n",
      "Epoch : 268, Loss : 0.01903684251010418\n",
      "Epoch : 269, Loss : 0.019014568999409676\n",
      "Epoch : 270, Loss : 0.018996454775333405\n",
      "Epoch : 271, Loss : 0.018976639956235886\n",
      "Epoch : 272, Loss : 0.01896826922893524\n",
      "Epoch : 273, Loss : 0.018969934433698654\n",
      "Epoch : 274, Loss : 0.018978074193000793\n",
      "Epoch : 275, Loss : 0.01898930035531521\n",
      "Epoch : 276, Loss : 0.0189956221729517\n",
      "Epoch : 277, Loss : 0.01899976283311844\n",
      "Epoch : 278, Loss : 0.0189936812967062\n",
      "Epoch : 279, Loss : 0.018987931311130524\n",
      "Epoch : 280, Loss : 0.018993211910128593\n",
      "Epoch : 281, Loss : 0.019021999090909958\n",
      "Epoch : 282, Loss : 0.019048485904932022\n",
      "Epoch : 283, Loss : 0.01909976452589035\n",
      "Epoch : 284, Loss : 0.019125433638691902\n",
      "Epoch : 285, Loss : 0.019188087433576584\n",
      "Epoch : 286, Loss : 0.019135933369398117\n",
      "Epoch : 287, Loss : 0.01908920891582966\n",
      "Epoch : 288, Loss : 0.01902238093316555\n",
      "Epoch : 289, Loss : 0.01899394765496254\n",
      "Epoch : 290, Loss : 0.018970971927046776\n",
      "Epoch : 291, Loss : 0.018975576385855675\n",
      "Epoch : 292, Loss : 0.019003018736839294\n",
      "Epoch : 293, Loss : 0.019017135724425316\n",
      "Epoch : 294, Loss : 0.01904146745800972\n",
      "Epoch : 295, Loss : 0.01903984695672989\n",
      "Epoch : 296, Loss : 0.019031893461942673\n",
      "Epoch : 297, Loss : 0.019007517024874687\n",
      "Epoch : 298, Loss : 0.018989527598023415\n",
      "Epoch : 299, Loss : 0.01897444576025009\n",
      "Epoch : 300, Loss : 0.01897002011537552\n",
      "Epoch : 301, Loss : 0.018970398232340813\n",
      "Epoch : 302, Loss : 0.018979959189891815\n",
      "Epoch : 303, Loss : 0.018988849595189095\n",
      "Epoch : 304, Loss : 0.018994681537151337\n",
      "Epoch : 305, Loss : 0.019002826884388924\n",
      "Epoch : 306, Loss : 0.018996993079781532\n",
      "Epoch : 307, Loss : 0.018993251025676727\n",
      "Epoch : 308, Loss : 0.01898578181862831\n",
      "Epoch : 309, Loss : 0.018978316336870193\n",
      "Epoch : 310, Loss : 0.018973277881741524\n",
      "Epoch : 311, Loss : 0.018969520926475525\n",
      "Epoch : 312, Loss : 0.01896783336997032\n",
      "Epoch : 313, Loss : 0.018968626856803894\n",
      "Epoch : 314, Loss : 0.0189695842564106\n",
      "Epoch : 315, Loss : 0.01897210255265236\n",
      "Epoch : 316, Loss : 0.018974849954247475\n",
      "Epoch : 317, Loss : 0.01897711306810379\n",
      "Epoch : 318, Loss : 0.01898088864982128\n",
      "Epoch : 319, Loss : 0.018982574343681335\n",
      "Epoch : 320, Loss : 0.01898585632443428\n",
      "Epoch : 321, Loss : 0.018988387659192085\n",
      "Epoch : 322, Loss : 0.01899300329387188\n",
      "Epoch : 323, Loss : 0.018997810781002045\n",
      "Epoch : 324, Loss : 0.019007647410035133\n",
      "Epoch : 325, Loss : 0.019012436270713806\n",
      "Epoch : 326, Loss : 0.019026504829525948\n",
      "Epoch : 327, Loss : 0.01903435029089451\n",
      "Epoch : 328, Loss : 0.019054418429732323\n",
      "Epoch : 329, Loss : 0.01905744895339012\n",
      "Epoch : 330, Loss : 0.019082827493548393\n",
      "Epoch : 331, Loss : 0.019080739468336105\n",
      "Epoch : 332, Loss : 0.019099406898021698\n",
      "Epoch : 333, Loss : 0.019082574173808098\n",
      "Epoch : 334, Loss : 0.01908939704298973\n",
      "Epoch : 335, Loss : 0.019061431288719177\n",
      "Epoch : 336, Loss : 0.019051704555749893\n",
      "Epoch : 337, Loss : 0.01902531459927559\n",
      "Epoch : 338, Loss : 0.019010700285434723\n",
      "Epoch : 339, Loss : 0.018993524834513664\n",
      "Epoch : 340, Loss : 0.01898333802819252\n",
      "Epoch : 341, Loss : 0.01897478848695755\n",
      "Epoch : 342, Loss : 0.018970143049955368\n",
      "Epoch : 343, Loss : 0.01896781474351883\n",
      "Epoch : 344, Loss : 0.018967589363455772\n",
      "Epoch : 345, Loss : 0.018968701362609863\n",
      "Epoch : 346, Loss : 0.018970860168337822\n",
      "Epoch : 347, Loss : 0.018974008038640022\n",
      "Epoch : 348, Loss : 0.018977336585521698\n",
      "Epoch : 349, Loss : 0.018981948494911194\n",
      "Epoch : 350, Loss : 0.018986228853464127\n",
      "Epoch : 351, Loss : 0.018993061035871506\n",
      "Epoch : 352, Loss : 0.018998293206095695\n",
      "Epoch : 353, Loss : 0.019008832052350044\n",
      "Epoch : 354, Loss : 0.019014986231923103\n",
      "Epoch : 355, Loss : 0.019030576571822166\n",
      "Epoch : 356, Loss : 0.01903785951435566\n",
      "Epoch : 357, Loss : 0.019059520214796066\n",
      "Epoch : 358, Loss : 0.019062435254454613\n",
      "Epoch : 359, Loss : 0.01908585987985134\n",
      "Epoch : 360, Loss : 0.019079629331827164\n",
      "Epoch : 361, Loss : 0.019094226881861687\n",
      "Epoch : 362, Loss : 0.019074290990829468\n",
      "Epoch : 363, Loss : 0.019074706360697746\n",
      "Epoch : 364, Loss : 0.019048089161515236\n",
      "Epoch : 365, Loss : 0.019036520272493362\n",
      "Epoch : 366, Loss : 0.019013743847608566\n",
      "Epoch : 367, Loss : 0.019000716507434845\n",
      "Epoch : 368, Loss : 0.018986664712429047\n",
      "Epoch : 369, Loss : 0.01897810399532318\n",
      "Epoch : 370, Loss : 0.018971851095557213\n",
      "Epoch : 371, Loss : 0.018968570977449417\n",
      "Epoch : 372, Loss : 0.018967406824231148\n",
      "Epoch : 373, Loss : 0.018967872485518456\n",
      "Epoch : 374, Loss : 0.01896951161324978\n",
      "Epoch : 375, Loss : 0.018971914425492287\n",
      "Epoch : 376, Loss : 0.01897517405450344\n",
      "Epoch : 377, Loss : 0.018978392705321312\n",
      "Epoch : 378, Loss : 0.018982859328389168\n",
      "Epoch : 379, Loss : 0.018986567854881287\n",
      "Epoch : 380, Loss : 0.01899276301264763\n",
      "Epoch : 381, Loss : 0.018997084349393845\n",
      "Epoch : 382, Loss : 0.019006025046110153\n",
      "Epoch : 383, Loss : 0.01901109144091606\n",
      "Epoch : 384, Loss : 0.019023897126317024\n",
      "Epoch : 385, Loss : 0.01902935840189457\n",
      "Epoch : 386, Loss : 0.019046787172555923\n",
      "Epoch : 387, Loss : 0.019050268456339836\n",
      "Epoch : 388, Loss : 0.019070375710725784\n",
      "Epoch : 389, Loss : 0.01906817965209484\n",
      "Epoch : 390, Loss : 0.01908506080508232\n",
      "Epoch : 391, Loss : 0.019072480499744415\n",
      "Epoch : 392, Loss : 0.019078955054283142\n",
      "Epoch : 393, Loss : 0.01905735582113266\n",
      "Epoch : 394, Loss : 0.019051315262913704\n",
      "Epoch : 395, Loss : 0.01902841031551361\n",
      "Epoch : 396, Loss : 0.019016651436686516\n",
      "Epoch : 397, Loss : 0.018999377265572548\n",
      "Epoch : 398, Loss : 0.018989035859704018\n",
      "Epoch : 399, Loss : 0.018979335203766823\n",
      "Epoch : 400, Loss : 0.018973369151353836\n",
      "Epoch : 401, Loss : 0.018969476222991943\n",
      "Epoch : 402, Loss : 0.01896766945719719\n",
      "Epoch : 403, Loss : 0.018967408686876297\n",
      "Epoch : 404, Loss : 0.018968285992741585\n",
      "Epoch : 405, Loss : 0.018970007076859474\n",
      "Epoch : 406, Loss : 0.01897219382226467\n",
      "Epoch : 407, Loss : 0.018975025042891502\n",
      "Epoch : 408, Loss : 0.018977750092744827\n",
      "Epoch : 409, Loss : 0.01898154430091381\n",
      "Epoch : 410, Loss : 0.018984608352184296\n",
      "Epoch : 411, Loss : 0.018989747390151024\n",
      "Epoch : 412, Loss : 0.018993403762578964\n",
      "Epoch : 413, Loss : 0.019000835716724396\n",
      "Epoch : 414, Loss : 0.01900550164282322\n",
      "Epoch : 415, Loss : 0.01901666261255741\n",
      "Epoch : 416, Loss : 0.01902242936193943\n",
      "Epoch : 417, Loss : 0.019038623198866844\n",
      "Epoch : 418, Loss : 0.019044168293476105\n",
      "Epoch : 419, Loss : 0.01907460391521454\n",
      "Epoch : 420, Loss : 0.01909133978188038\n",
      "Epoch : 421, Loss : 0.019155403599143028\n",
      "Epoch : 422, Loss : 0.01916736736893654\n",
      "Epoch : 423, Loss : 0.019229257479310036\n",
      "Epoch : 424, Loss : 0.01918702758848667\n",
      "Epoch : 425, Loss : 0.019190331920981407\n",
      "Epoch : 426, Loss : 0.019103851169347763\n",
      "Epoch : 427, Loss : 0.019055338576436043\n",
      "Epoch : 428, Loss : 0.019003363326191902\n",
      "Epoch : 429, Loss : 0.01897653378546238\n",
      "Epoch : 430, Loss : 0.01896752417087555\n",
      "Epoch : 431, Loss : 0.018973875790834427\n",
      "Epoch : 432, Loss : 0.01899089477956295\n",
      "Epoch : 433, Loss : 0.01900768093764782\n",
      "Epoch : 434, Loss : 0.019026705995202065\n",
      "Epoch : 435, Loss : 0.01902741938829422\n",
      "Epoch : 436, Loss : 0.01902659982442856\n",
      "Epoch : 437, Loss : 0.019008418545126915\n",
      "Epoch : 438, Loss : 0.018994027748703957\n",
      "Epoch : 439, Loss : 0.018978595733642578\n",
      "Epoch : 440, Loss : 0.018969833850860596\n",
      "Epoch : 441, Loss : 0.01896752044558525\n",
      "Epoch : 442, Loss : 0.018970660865306854\n",
      "Epoch : 443, Loss : 0.018976937979459763\n",
      "Epoch : 444, Loss : 0.018982911482453346\n",
      "Epoch : 445, Loss : 0.018988464027643204\n",
      "Epoch : 446, Loss : 0.018988724797964096\n",
      "Epoch : 447, Loss : 0.018987858667969704\n",
      "Epoch : 448, Loss : 0.01898282952606678\n",
      "Epoch : 449, Loss : 0.018978213891386986\n",
      "Epoch : 450, Loss : 0.018973130732774734\n",
      "Epoch : 451, Loss : 0.018969686701893806\n",
      "Epoch : 452, Loss : 0.018967799842357635\n",
      "Epoch : 453, Loss : 0.018967458978295326\n",
      "Epoch : 454, Loss : 0.018968336284160614\n",
      "Epoch : 455, Loss : 0.018969910219311714\n",
      "Epoch : 456, Loss : 0.018971817567944527\n",
      "Epoch : 457, Loss : 0.01897342875599861\n",
      "Epoch : 458, Loss : 0.01897505111992359\n",
      "Epoch : 459, Loss : 0.01897577941417694\n",
      "Epoch : 460, Loss : 0.018976552411913872\n",
      "Epoch : 461, Loss : 0.01897641085088253\n",
      "Epoch : 462, Loss : 0.018976625055074692\n",
      "Epoch : 463, Loss : 0.018976042047142982\n",
      "Epoch : 464, Loss : 0.018975984305143356\n",
      "Epoch : 465, Loss : 0.018975459039211273\n",
      "Epoch : 466, Loss : 0.018975531682372093\n",
      "Epoch : 467, Loss : 0.018975278362631798\n",
      "Epoch : 468, Loss : 0.01897576078772545\n",
      "Epoch : 469, Loss : 0.01897601969540119\n",
      "Epoch : 470, Loss : 0.018977226689457893\n",
      "Epoch : 471, Loss : 0.018978316336870193\n",
      "Epoch : 472, Loss : 0.018980922177433968\n",
      "Epoch : 473, Loss : 0.018983421847224236\n",
      "Epoch : 474, Loss : 0.018988769501447678\n",
      "Epoch : 475, Loss : 0.018994037061929703\n",
      "Epoch : 476, Loss : 0.019005049020051956\n",
      "Epoch : 477, Loss : 0.019015274941921234\n",
      "Epoch : 478, Loss : 0.01903805322945118\n",
      "Epoch : 479, Loss : 0.01905498467385769\n",
      "Epoch : 480, Loss : 0.019096316769719124\n",
      "Epoch : 481, Loss : 0.019114237278699875\n",
      "Epoch : 482, Loss : 0.019170701503753662\n",
      "Epoch : 483, Loss : 0.01916521228849888\n",
      "Epoch : 484, Loss : 0.019201284274458885\n",
      "Epoch : 485, Loss : 0.019153673201799393\n",
      "Epoch : 486, Loss : 0.019140278920531273\n",
      "Epoch : 487, Loss : 0.01907586120069027\n",
      "Epoch : 488, Loss : 0.01903882995247841\n",
      "Epoch : 489, Loss : 0.019002899527549744\n",
      "Epoch : 490, Loss : 0.01898285001516342\n",
      "Epoch : 491, Loss : 0.018971504643559456\n",
      "Epoch : 492, Loss : 0.01896757259964943\n",
      "Epoch : 493, Loss : 0.018969297409057617\n",
      "Epoch : 494, Loss : 0.018974699079990387\n",
      "Epoch : 495, Loss : 0.01898312382400036\n",
      "Epoch : 496, Loss : 0.018990926444530487\n",
      "Epoch : 497, Loss : 0.018999813124537468\n",
      "Epoch : 498, Loss : 0.019003164023160934\n",
      "Epoch : 499, Loss : 0.019007807597517967\n",
      "Epoch : 500, Loss : 0.01900445483624935\n",
      "Epoch : 501, Loss : 0.01900220476090908\n",
      "Epoch : 502, Loss : 0.018994111567735672\n",
      "Epoch : 503, Loss : 0.01898798532783985\n",
      "Epoch : 504, Loss : 0.018980585038661957\n",
      "Epoch : 505, Loss : 0.01897510327398777\n",
      "Epoch : 506, Loss : 0.018970781937241554\n",
      "Epoch : 507, Loss : 0.018968287855386734\n",
      "Epoch : 508, Loss : 0.018967391923069954\n",
      "Epoch : 509, Loss : 0.01896766945719719\n",
      "Epoch : 510, Loss : 0.018968898802995682\n",
      "Epoch : 511, Loss : 0.018970536068081856\n",
      "Epoch : 512, Loss : 0.018972624093294144\n",
      "Epoch : 513, Loss : 0.018974317237734795\n",
      "Epoch : 514, Loss : 0.018976230174303055\n",
      "Epoch : 515, Loss : 0.01897738315165043\n",
      "Epoch : 516, Loss : 0.018978890031576157\n",
      "Epoch : 517, Loss : 0.01897924579679966\n",
      "Epoch : 518, Loss : 0.018980244174599648\n",
      "Epoch : 519, Loss : 0.018980303779244423\n",
      "Epoch : 520, Loss : 0.01898125559091568\n",
      "Epoch : 521, Loss : 0.018981223925948143\n",
      "Epoch : 522, Loss : 0.01898239180445671\n",
      "Epoch : 523, Loss : 0.01898275688290596\n",
      "Epoch : 524, Loss : 0.018984688445925713\n",
      "Epoch : 525, Loss : 0.01898573711514473\n",
      "Epoch : 526, Loss : 0.018988925963640213\n",
      "Epoch : 527, Loss : 0.018991149961948395\n",
      "Epoch : 528, Loss : 0.01899673603475094\n",
      "Epoch : 529, Loss : 0.019000647589564323\n",
      "Epoch : 530, Loss : 0.019009863957762718\n",
      "Epoch : 531, Loss : 0.019016273319721222\n",
      "Epoch : 532, Loss : 0.019031966105103493\n",
      "Epoch : 533, Loss : 0.019040435552597046\n",
      "Epoch : 534, Loss : 0.019062833860516548\n",
      "Epoch : 535, Loss : 0.0190705806016922\n",
      "Epoch : 536, Loss : 0.01909724250435829\n",
      "Epoch : 537, Loss : 0.01909426413476467\n",
      "Epoch : 538, Loss : 0.019112221896648407\n",
      "Epoch : 539, Loss : 0.019092107191681862\n",
      "Epoch : 540, Loss : 0.019089441746473312\n",
      "Epoch : 541, Loss : 0.019056687131524086\n",
      "Epoch : 542, Loss : 0.019037818536162376\n",
      "Epoch : 543, Loss : 0.019009429961442947\n",
      "Epoch : 544, Loss : 0.018991047516465187\n",
      "Epoch : 545, Loss : 0.018976442515850067\n",
      "Epoch : 546, Loss : 0.018969006836414337\n",
      "Epoch : 547, Loss : 0.018967317417263985\n",
      "Epoch : 548, Loss : 0.01897013559937477\n",
      "Epoch : 549, Loss : 0.0189758762717247\n",
      "Epoch : 550, Loss : 0.018982041627168655\n",
      "Epoch : 551, Loss : 0.018988536670804024\n",
      "Epoch : 552, Loss : 0.018991295248270035\n",
      "Epoch : 553, Loss : 0.018993601202964783\n",
      "Epoch : 554, Loss : 0.018990805372595787\n",
      "Epoch : 555, Loss : 0.01898815482854843\n",
      "Epoch : 556, Loss : 0.01898266188800335\n",
      "Epoch : 557, Loss : 0.018978141248226166\n",
      "Epoch : 558, Loss : 0.01897350698709488\n",
      "Epoch : 559, Loss : 0.018970265984535217\n",
      "Epoch : 560, Loss : 0.018968163058161736\n",
      "Epoch : 561, Loss : 0.01896727830171585\n",
      "Epoch : 562, Loss : 0.018967386335134506\n",
      "Epoch : 563, Loss : 0.01896820031106472\n",
      "Epoch : 564, Loss : 0.0189694594591856\n",
      "Epoch : 565, Loss : 0.01897084154188633\n",
      "Epoch : 566, Loss : 0.018972372636198997\n",
      "Epoch : 567, Loss : 0.01897358149290085\n",
      "Epoch : 568, Loss : 0.018974928185343742\n",
      "Epoch : 569, Loss : 0.018975719809532166\n",
      "Epoch : 570, Loss : 0.01897682622075081\n",
      "Epoch : 571, Loss : 0.01897728629410267\n",
      "Epoch : 572, Loss : 0.01897832192480564\n",
      "Epoch : 573, Loss : 0.01897873915731907\n",
      "Epoch : 574, Loss : 0.018980029970407486\n",
      "Epoch : 575, Loss : 0.01898072101175785\n",
      "Epoch : 576, Loss : 0.01898270472884178\n",
      "Epoch : 577, Loss : 0.01898406445980072\n",
      "Epoch : 578, Loss : 0.018987374380230904\n",
      "Epoch : 579, Loss : 0.01898992247879505\n",
      "Epoch : 580, Loss : 0.018995655700564384\n",
      "Epoch : 581, Loss : 0.019000137224793434\n",
      "Epoch : 582, Loss : 0.019010048359632492\n",
      "Epoch : 583, Loss : 0.019017290323972702\n",
      "Epoch : 584, Loss : 0.0190338846296072\n",
      "Epoch : 585, Loss : 0.019043797627091408\n",
      "Epoch : 586, Loss : 0.01906883716583252\n",
      "Epoch : 587, Loss : 0.01907827891409397\n",
      "Epoch : 588, Loss : 0.019108779728412628\n",
      "Epoch : 589, Loss : 0.01910828799009323\n",
      "Epoch : 590, Loss : 0.019130544736981392\n",
      "Epoch : 591, Loss : 0.019105875864624977\n",
      "Epoch : 592, Loss : 0.01909848302602768\n",
      "Epoch : 593, Loss : 0.019059251993894577\n",
      "Epoch : 594, Loss : 0.019034938886761665\n",
      "Epoch : 595, Loss : 0.019003447145223618\n",
      "Epoch : 596, Loss : 0.01898353174328804\n",
      "Epoch : 597, Loss : 0.018970996141433716\n",
      "Epoch : 598, Loss : 0.018967168405652046\n",
      "Epoch : 599, Loss : 0.018970204517245293\n",
      "Epoch : 600, Loss : 0.018977414816617966\n",
      "Epoch : 601, Loss : 0.018986638635396957\n",
      "Epoch : 602, Loss : 0.0189927089959383\n",
      "Epoch : 603, Loss : 0.018997538834810257\n",
      "Epoch : 604, Loss : 0.018995221704244614\n",
      "Epoch : 605, Loss : 0.018991777673363686\n",
      "Epoch : 606, Loss : 0.018984051421284676\n",
      "Epoch : 607, Loss : 0.018977472558617592\n",
      "Epoch : 608, Loss : 0.018971644341945648\n",
      "Epoch : 609, Loss : 0.018968230113387108\n",
      "Epoch : 610, Loss : 0.018967188894748688\n",
      "Epoch : 611, Loss : 0.01896812953054905\n",
      "Epoch : 612, Loss : 0.0189705528318882\n",
      "Epoch : 613, Loss : 0.01897387020289898\n",
      "Epoch : 614, Loss : 0.01897771842777729\n",
      "Epoch : 615, Loss : 0.01898067444562912\n",
      "Epoch : 616, Loss : 0.01898353174328804\n",
      "Epoch : 617, Loss : 0.018984120339155197\n",
      "Epoch : 618, Loss : 0.018984545022249222\n",
      "Epoch : 619, Loss : 0.018982673063874245\n",
      "Epoch : 620, Loss : 0.018981104716658592\n",
      "Epoch : 621, Loss : 0.018978245556354523\n",
      "Epoch : 622, Loss : 0.01897590607404709\n",
      "Epoch : 623, Loss : 0.018973320722579956\n",
      "Epoch : 624, Loss : 0.018971439450979233\n",
      "Epoch : 625, Loss : 0.01896989531815052\n",
      "Epoch : 626, Loss : 0.018968811258673668\n",
      "Epoch : 627, Loss : 0.01896800845861435\n",
      "Epoch : 628, Loss : 0.018967492505908012\n",
      "Epoch : 629, Loss : 0.018967196345329285\n",
      "Epoch : 630, Loss : 0.01896706409752369\n",
      "Epoch : 631, Loss : 0.01896706223487854\n",
      "Epoch : 632, Loss : 0.018967144191265106\n",
      "Epoch : 633, Loss : 0.01896730065345764\n",
      "Epoch : 634, Loss : 0.018967516720294952\n",
      "Epoch : 635, Loss : 0.018967803567647934\n",
      "Epoch : 636, Loss : 0.018968161195516586\n",
      "Epoch : 637, Loss : 0.01896865852177143\n",
      "Epoch : 638, Loss : 0.018969282507896423\n",
      "Epoch : 639, Loss : 0.018970219418406487\n",
      "Epoch : 640, Loss : 0.01897144876420498\n",
      "Epoch : 641, Loss : 0.018973447382450104\n",
      "Epoch : 642, Loss : 0.01897607371211052\n",
      "Epoch : 643, Loss : 0.018980588763952255\n",
      "Epoch : 644, Loss : 0.018987365067005157\n",
      "Epoch : 645, Loss : 0.01900159940123558\n",
      "Epoch : 646, Loss : 0.019019868224859238\n",
      "Epoch : 647, Loss : 0.019053621217608452\n",
      "Epoch : 648, Loss : 0.019084975123405457\n",
      "Epoch : 649, Loss : 0.01915113627910614\n",
      "Epoch : 650, Loss : 0.019186992198228836\n",
      "Epoch : 651, Loss : 0.019276006147265434\n",
      "Epoch : 652, Loss : 0.019252697005867958\n",
      "Epoch : 653, Loss : 0.01926683634519577\n",
      "Epoch : 654, Loss : 0.019161703065037727\n",
      "Epoch : 655, Loss : 0.019093452021479607\n",
      "Epoch : 656, Loss : 0.019015759229660034\n",
      "Epoch : 657, Loss : 0.018976086750626564\n",
      "Epoch : 658, Loss : 0.018967850133776665\n",
      "Epoch : 659, Loss : 0.01898527331650257\n",
      "Epoch : 660, Loss : 0.019016467034816742\n",
      "Epoch : 661, Loss : 0.019035760313272476\n",
      "Epoch : 662, Loss : 0.01904565840959549\n",
      "Epoch : 663, Loss : 0.019023800268769264\n",
      "Epoch : 664, Loss : 0.01900145411491394\n",
      "Epoch : 665, Loss : 0.01898018643260002\n",
      "Epoch : 666, Loss : 0.018968988209962845\n",
      "Epoch : 667, Loss : 0.01896814815700054\n",
      "Epoch : 668, Loss : 0.018975187093019485\n",
      "Epoch : 669, Loss : 0.018984241411089897\n",
      "Epoch : 670, Loss : 0.018987473100423813\n",
      "Epoch : 671, Loss : 0.018985144793987274\n",
      "Epoch : 672, Loss : 0.018977021798491478\n",
      "Epoch : 673, Loss : 0.018970107659697533\n",
      "Epoch : 674, Loss : 0.018967293202877045\n",
      "Epoch : 675, Loss : 0.018969133496284485\n",
      "Epoch : 676, Loss : 0.018973300233483315\n",
      "Epoch : 677, Loss : 0.018976187333464622\n",
      "Epoch : 678, Loss : 0.018976479768753052\n",
      "Epoch : 679, Loss : 0.018973516300320625\n",
      "Epoch : 680, Loss : 0.01897002011537552\n",
      "Epoch : 681, Loss : 0.018967628479003906\n",
      "Epoch : 682, Loss : 0.018967395648360252\n",
      "Epoch : 683, Loss : 0.018968846648931503\n",
      "Epoch : 684, Loss : 0.01897064410150051\n",
      "Epoch : 685, Loss : 0.018971478566527367\n",
      "Epoch : 686, Loss : 0.018974432721734047\n",
      "Epoch : 687, Loss : 0.018982183188199997\n",
      "Epoch : 688, Loss : 0.018982846289873123\n",
      "Epoch : 689, Loss : 0.018989713862538338\n",
      "Epoch : 690, Loss : 0.018984206020832062\n",
      "Epoch : 691, Loss : 0.018983090296387672\n",
      "Epoch : 692, Loss : 0.01897536590695381\n",
      "Epoch : 693, Loss : 0.018972890451550484\n",
      "Epoch : 694, Loss : 0.018968088552355766\n",
      "Epoch : 695, Loss : 0.0189687330275774\n",
      "Epoch : 696, Loss : 0.018968578428030014\n",
      "Epoch : 697, Loss : 0.018971474841237068\n",
      "Epoch : 698, Loss : 0.018972523510456085\n",
      "Epoch : 699, Loss : 0.018974225968122482\n",
      "Epoch : 700, Loss : 0.018974149599671364\n",
      "Epoch : 701, Loss : 0.018973445519804955\n",
      "Epoch : 702, Loss : 0.01897163689136505\n",
      "Epoch : 703, Loss : 0.018970103934407234\n",
      "Epoch : 704, Loss : 0.018968621268868446\n",
      "Epoch : 705, Loss : 0.01896771229803562\n",
      "Epoch : 706, Loss : 0.018967315554618835\n",
      "Epoch : 707, Loss : 0.018967339769005775\n",
      "Epoch : 708, Loss : 0.018967773765325546\n",
      "Epoch : 709, Loss : 0.018968187272548676\n",
      "Epoch : 710, Loss : 0.018968762829899788\n",
      "Epoch : 711, Loss : 0.01896907016634941\n",
      "Epoch : 712, Loss : 0.018969444558024406\n",
      "Epoch : 713, Loss : 0.01896940916776657\n",
      "Epoch : 714, Loss : 0.01896943897008896\n",
      "Epoch : 715, Loss : 0.018969159573316574\n",
      "Epoch : 716, Loss : 0.018969068303704262\n",
      "Epoch : 717, Loss : 0.018968692049384117\n",
      "Epoch : 718, Loss : 0.018968528136610985\n",
      "Epoch : 719, Loss : 0.018968189135193825\n",
      "Epoch : 720, Loss : 0.018968092277646065\n",
      "Epoch : 721, Loss : 0.018967825919389725\n",
      "Epoch : 722, Loss : 0.018967770040035248\n",
      "Epoch : 723, Loss : 0.01896759308874607\n",
      "Epoch : 724, Loss : 0.018967611715197563\n",
      "Epoch : 725, Loss : 0.018967512995004654\n",
      "Epoch : 726, Loss : 0.018967578187584877\n",
      "Epoch : 727, Loss : 0.018967576324939728\n",
      "Epoch : 728, Loss : 0.018967732787132263\n",
      "Epoch : 729, Loss : 0.01896785758435726\n",
      "Epoch : 730, Loss : 0.018968159332871437\n",
      "Epoch : 731, Loss : 0.01896856352686882\n",
      "Epoch : 732, Loss : 0.018969213590025902\n",
      "Epoch : 733, Loss : 0.0189701896160841\n",
      "Epoch : 734, Loss : 0.0189716424793005\n",
      "Epoch : 735, Loss : 0.018974093720316887\n",
      "Epoch : 736, Loss : 0.01897759735584259\n",
      "Epoch : 737, Loss : 0.018983831629157066\n",
      "Epoch : 738, Loss : 0.01900198496878147\n",
      "Epoch : 739, Loss : 0.019054030999541283\n",
      "Epoch : 740, Loss : 0.019147930666804314\n",
      "Epoch : 741, Loss : 0.019342565909028053\n",
      "Epoch : 742, Loss : 0.019450144842267036\n",
      "Epoch : 743, Loss : 0.01964556984603405\n",
      "Epoch : 744, Loss : 0.019452208653092384\n",
      "Epoch : 745, Loss : 0.01924937404692173\n",
      "Epoch : 746, Loss : 0.01902781054377556\n",
      "Epoch : 747, Loss : 0.018972421064972878\n",
      "Epoch : 748, Loss : 0.019049901515245438\n",
      "Epoch : 749, Loss : 0.019168928265571594\n",
      "Epoch : 750, Loss : 0.019268130883574486\n",
      "Epoch : 751, Loss : 0.01917359232902527\n",
      "Epoch : 752, Loss : 0.01905418001115322\n",
      "Epoch : 753, Loss : 0.018971331417560577\n",
      "Epoch : 754, Loss : 0.019031202420592308\n",
      "Epoch : 755, Loss : 0.0191337950527668\n",
      "Epoch : 756, Loss : 0.019107725471258163\n",
      "Epoch : 757, Loss : 0.019012920558452606\n",
      "Epoch : 758, Loss : 0.01897527277469635\n",
      "Epoch : 759, Loss : 0.019025372341275215\n",
      "Epoch : 760, Loss : 0.01907634176313877\n",
      "Epoch : 761, Loss : 0.019025983288884163\n",
      "Epoch : 762, Loss : 0.01897316426038742\n",
      "Epoch : 763, Loss : 0.018991224467754364\n",
      "Epoch : 764, Loss : 0.019024787470698357\n",
      "Epoch : 765, Loss : 0.019021539017558098\n",
      "Epoch : 766, Loss : 0.018976859748363495\n",
      "Epoch : 767, Loss : 0.01897907815873623\n",
      "Epoch : 768, Loss : 0.019004952162504196\n",
      "Epoch : 769, Loss : 0.019006825983524323\n",
      "Epoch : 770, Loss : 0.018977776169776917\n",
      "Epoch : 771, Loss : 0.018972987309098244\n",
      "Epoch : 772, Loss : 0.018990522250533104\n",
      "Epoch : 773, Loss : 0.018998660147190094\n",
      "Epoch : 774, Loss : 0.018978368490934372\n",
      "Epoch : 775, Loss : 0.018969912081956863\n",
      "Epoch : 776, Loss : 0.018984025344252586\n",
      "Epoch : 777, Loss : 0.018987925723195076\n",
      "Epoch : 778, Loss : 0.018978144973516464\n",
      "Epoch : 779, Loss : 0.01896831952035427\n",
      "Epoch : 780, Loss : 0.018978070467710495\n",
      "Epoch : 781, Loss : 0.01898263953626156\n",
      "Epoch : 782, Loss : 0.01897645741701126\n",
      "Epoch : 783, Loss : 0.018968278542160988\n",
      "Epoch : 784, Loss : 0.018973790109157562\n",
      "Epoch : 785, Loss : 0.018978223204612732\n",
      "Epoch : 786, Loss : 0.01897457428276539\n",
      "Epoch : 787, Loss : 0.018968822434544563\n",
      "Epoch : 788, Loss : 0.018970414996147156\n",
      "Epoch : 789, Loss : 0.018975302577018738\n",
      "Epoch : 790, Loss : 0.01897292770445347\n",
      "Epoch : 791, Loss : 0.018969211727380753\n",
      "Epoch : 792, Loss : 0.01896854303777218\n",
      "Epoch : 793, Loss : 0.01897221803665161\n",
      "Epoch : 794, Loss : 0.018971843644976616\n",
      "Epoch : 795, Loss : 0.018969282507896423\n",
      "Epoch : 796, Loss : 0.018967848271131516\n",
      "Epoch : 797, Loss : 0.018970070406794548\n",
      "Epoch : 798, Loss : 0.018970686942338943\n",
      "Epoch : 799, Loss : 0.018969260156154633\n",
      "Epoch : 800, Loss : 0.01896771974861622\n",
      "Epoch : 801, Loss : 0.01896851696074009\n",
      "Epoch : 802, Loss : 0.018969641998410225\n",
      "Epoch : 803, Loss : 0.018969092518091202\n",
      "Epoch : 804, Loss : 0.018967874348163605\n",
      "Epoch : 805, Loss : 0.018967673182487488\n",
      "Epoch : 806, Loss : 0.018968606367707253\n",
      "Epoch : 807, Loss : 0.018968725576996803\n",
      "Epoch : 808, Loss : 0.01896807737648487\n",
      "Epoch : 809, Loss : 0.01896739937365055\n",
      "Epoch : 810, Loss : 0.01896783523261547\n",
      "Epoch : 811, Loss : 0.01896820031106472\n",
      "Epoch : 812, Loss : 0.01896817609667778\n",
      "Epoch : 813, Loss : 0.018967511132359505\n",
      "Epoch : 814, Loss : 0.018967395648360252\n",
      "Epoch : 815, Loss : 0.01896762102842331\n",
      "Epoch : 816, Loss : 0.018967876210808754\n",
      "Epoch : 817, Loss : 0.01896762289106846\n",
      "Epoch : 818, Loss : 0.018968166783452034\n",
      "Epoch : 819, Loss : 0.018969738855957985\n",
      "Epoch : 820, Loss : 0.018970753997564316\n",
      "Epoch : 821, Loss : 0.018971845507621765\n",
      "Epoch : 822, Loss : 0.018971044570207596\n",
      "Epoch : 823, Loss : 0.018968986347317696\n",
      "Epoch : 824, Loss : 0.018967799842357635\n",
      "Epoch : 825, Loss : 0.01896720565855503\n",
      "Epoch : 826, Loss : 0.01896822080016136\n",
      "Epoch : 827, Loss : 0.018969060853123665\n",
      "Epoch : 828, Loss : 0.018969206139445305\n",
      "Epoch : 829, Loss : 0.018968624994158745\n",
      "Epoch : 830, Loss : 0.018967539072036743\n",
      "Epoch : 831, Loss : 0.01896718703210354\n",
      "Epoch : 832, Loss : 0.01896725222468376\n",
      "Epoch : 833, Loss : 0.018967775627970695\n",
      "Epoch : 834, Loss : 0.018968142569065094\n",
      "Epoch : 835, Loss : 0.018967971205711365\n",
      "Epoch : 836, Loss : 0.01896762289106846\n",
      "Epoch : 837, Loss : 0.01896711252629757\n",
      "Epoch : 838, Loss : 0.018966995179653168\n",
      "Epoch : 839, Loss : 0.018967103213071823\n",
      "Epoch : 840, Loss : 0.01896732859313488\n",
      "Epoch : 841, Loss : 0.018967512995004654\n",
      "Epoch : 842, Loss : 0.0189674012362957\n",
      "Epoch : 843, Loss : 0.01896723173558712\n",
      "Epoch : 844, Loss : 0.018966984003782272\n",
      "Epoch : 845, Loss : 0.01896687224507332\n",
      "Epoch : 846, Loss : 0.018966905772686005\n",
      "Epoch : 847, Loss : 0.018966984003782272\n",
      "Epoch : 848, Loss : 0.018967099487781525\n",
      "Epoch : 849, Loss : 0.018967095762491226\n",
      "Epoch : 850, Loss : 0.01896703615784645\n",
      "Epoch : 851, Loss : 0.018966931849718094\n",
      "Epoch : 852, Loss : 0.01896682195365429\n",
      "Epoch : 853, Loss : 0.018966786563396454\n",
      "Epoch : 854, Loss : 0.01896677166223526\n",
      "Epoch : 855, Loss : 0.018966814503073692\n",
      "Epoch : 856, Loss : 0.01896684616804123\n",
      "Epoch : 857, Loss : 0.018966855481266975\n",
      "Epoch : 858, Loss : 0.01896684803068638\n",
      "Epoch : 859, Loss : 0.018966801464557648\n",
      "Epoch : 860, Loss : 0.018966760486364365\n",
      "Epoch : 861, Loss : 0.018966712057590485\n",
      "Epoch : 862, Loss : 0.018966685980558395\n",
      "Epoch : 863, Loss : 0.018966680392622948\n",
      "Epoch : 864, Loss : 0.01896667294204235\n",
      "Epoch : 865, Loss : 0.018966684117913246\n",
      "Epoch : 866, Loss : 0.018966689705848694\n",
      "Epoch : 867, Loss : 0.018966689705848694\n",
      "Epoch : 868, Loss : 0.018966682255268097\n",
      "Epoch : 869, Loss : 0.018966669216752052\n",
      "Epoch : 870, Loss : 0.01896665245294571\n",
      "Epoch : 871, Loss : 0.01896663010120392\n",
      "Epoch : 872, Loss : 0.018966609612107277\n",
      "Epoch : 873, Loss : 0.018966592848300934\n",
      "Epoch : 874, Loss : 0.01896657980978489\n",
      "Epoch : 875, Loss : 0.018966570496559143\n",
      "Epoch : 876, Loss : 0.018966561183333397\n",
      "Epoch : 877, Loss : 0.0189665537327528\n",
      "Epoch : 878, Loss : 0.0189665500074625\n",
      "Epoch : 879, Loss : 0.018966546282172203\n",
      "Epoch : 880, Loss : 0.018966542556881905\n",
      "Epoch : 881, Loss : 0.018966538831591606\n",
      "Epoch : 882, Loss : 0.01896653324365616\n",
      "Epoch : 883, Loss : 0.01896653324365616\n",
      "Epoch : 884, Loss : 0.018966523930430412\n",
      "Epoch : 885, Loss : 0.018966520205140114\n",
      "Epoch : 886, Loss : 0.018966518342494965\n",
      "Epoch : 887, Loss : 0.018966516479849815\n",
      "Epoch : 888, Loss : 0.018966514617204666\n",
      "Epoch : 889, Loss : 0.018966516479849815\n",
      "Epoch : 890, Loss : 0.018966520205140114\n",
      "Epoch : 891, Loss : 0.01896653324365616\n",
      "Epoch : 892, Loss : 0.0189665500074625\n",
      "Epoch : 893, Loss : 0.018966583535075188\n",
      "Epoch : 894, Loss : 0.01896663010120392\n",
      "Epoch : 895, Loss : 0.01896670274436474\n",
      "Epoch : 896, Loss : 0.01896681822836399\n",
      "Epoch : 897, Loss : 0.01896698586642742\n",
      "Epoch : 898, Loss : 0.018967250362038612\n",
      "Epoch : 899, Loss : 0.018967652693390846\n",
      "Epoch : 900, Loss : 0.01896829716861248\n",
      "Epoch : 901, Loss : 0.01896926388144493\n",
      "Epoch : 902, Loss : 0.01897083781659603\n",
      "Epoch : 903, Loss : 0.018973123282194138\n",
      "Epoch : 904, Loss : 0.01897687464952469\n",
      "Epoch : 905, Loss : 0.018981942906975746\n",
      "Epoch : 906, Loss : 0.018990270793437958\n",
      "Epoch : 907, Loss : 0.019000060856342316\n",
      "Epoch : 908, Loss : 0.019016195088624954\n",
      "Epoch : 909, Loss : 0.019031038507819176\n",
      "Epoch : 910, Loss : 0.019055744633078575\n",
      "Epoch : 911, Loss : 0.019067833200097084\n",
      "Epoch : 912, Loss : 0.019090425223112106\n",
      "Epoch : 913, Loss : 0.019085237756371498\n",
      "Epoch : 914, Loss : 0.019088592380285263\n",
      "Epoch : 915, Loss : 0.019061468541622162\n",
      "Epoch : 916, Loss : 0.019041528925299644\n",
      "Epoch : 917, Loss : 0.01901046559214592\n",
      "Epoch : 918, Loss : 0.01898839697241783\n",
      "Epoch : 919, Loss : 0.018972504884004593\n",
      "Epoch : 920, Loss : 0.018966520205140114\n",
      "Epoch : 921, Loss : 0.018968971446156502\n",
      "Epoch : 922, Loss : 0.018976647406816483\n",
      "Epoch : 923, Loss : 0.018986068665981293\n",
      "Epoch : 924, Loss : 0.01899142935872078\n",
      "Epoch : 925, Loss : 0.018993500620126724\n",
      "Epoch : 926, Loss : 0.018988173454999924\n",
      "Epoch : 927, Loss : 0.0189812108874321\n",
      "Epoch : 928, Loss : 0.01897319033741951\n",
      "Epoch : 929, Loss : 0.018967993557453156\n",
      "Epoch : 930, Loss : 0.01896645687520504\n",
      "Epoch : 931, Loss : 0.01896819844841957\n",
      "Epoch : 932, Loss : 0.018971653655171394\n",
      "Epoch : 933, Loss : 0.018974624574184418\n",
      "Epoch : 934, Loss : 0.018976127728819847\n",
      "Epoch : 935, Loss : 0.01899343729019165\n",
      "Epoch : 936, Loss : 0.019054070115089417\n",
      "Epoch : 937, Loss : 0.019117847084999084\n",
      "Epoch : 938, Loss : 0.019223608076572418\n",
      "Epoch : 939, Loss : 0.019229579716920853\n",
      "Epoch : 940, Loss : 0.019231989979743958\n",
      "Epoch : 941, Loss : 0.01911344565451145\n",
      "Epoch : 942, Loss : 0.019010942429304123\n",
      "Epoch : 943, Loss : 0.018971307203173637\n",
      "Epoch : 944, Loss : 0.018991846591234207\n",
      "Epoch : 945, Loss : 0.019041435793042183\n",
      "Epoch : 946, Loss : 0.019060928374528885\n",
      "Epoch : 947, Loss : 0.019052835181355476\n",
      "Epoch : 948, Loss : 0.01900727115571499\n",
      "Epoch : 949, Loss : 0.018970254808664322\n",
      "Epoch : 950, Loss : 0.018975334241986275\n",
      "Epoch : 951, Loss : 0.01900123991072178\n",
      "Epoch : 952, Loss : 0.01902000606060028\n",
      "Epoch : 953, Loss : 0.019000917673110962\n",
      "Epoch : 954, Loss : 0.018977023661136627\n",
      "Epoch : 955, Loss : 0.01896725781261921\n",
      "Epoch : 956, Loss : 0.01897723414003849\n",
      "Epoch : 957, Loss : 0.018990758806467056\n",
      "Epoch : 958, Loss : 0.018989134579896927\n",
      "Epoch : 959, Loss : 0.018977290019392967\n",
      "Epoch : 960, Loss : 0.018967634066939354\n",
      "Epoch : 961, Loss : 0.01897001452744007\n",
      "Epoch : 962, Loss : 0.01897868700325489\n",
      "Epoch : 963, Loss : 0.018980534747242928\n",
      "Epoch : 964, Loss : 0.01897491328418255\n",
      "Epoch : 965, Loss : 0.018967894837260246\n",
      "Epoch : 966, Loss : 0.018967868760228157\n",
      "Epoch : 967, Loss : 0.01897329092025757\n",
      "Epoch : 968, Loss : 0.018979454413056374\n",
      "Epoch : 969, Loss : 0.01897905208170414\n",
      "Epoch : 970, Loss : 0.018971789628267288\n",
      "Epoch : 971, Loss : 0.01896717958152294\n",
      "Epoch : 972, Loss : 0.018969183787703514\n",
      "Epoch : 973, Loss : 0.018973998725414276\n",
      "Epoch : 974, Loss : 0.018974382430315018\n",
      "Epoch : 975, Loss : 0.018970755860209465\n",
      "Epoch : 976, Loss : 0.018967095762491226\n",
      "Epoch : 977, Loss : 0.018967589363455772\n",
      "Epoch : 978, Loss : 0.018969912081956863\n",
      "Epoch : 979, Loss : 0.018971318379044533\n",
      "Epoch : 980, Loss : 0.018969818949699402\n",
      "Epoch : 981, Loss : 0.018967527896165848\n",
      "Epoch : 982, Loss : 0.018966831266880035\n",
      "Epoch : 983, Loss : 0.01896791346371174\n",
      "Epoch : 984, Loss : 0.018969273194670677\n",
      "Epoch : 985, Loss : 0.0189689751714468\n",
      "Epoch : 986, Loss : 0.018967753276228905\n",
      "Epoch : 987, Loss : 0.01896674372255802\n",
      "Epoch : 988, Loss : 0.018966948613524437\n",
      "Epoch : 989, Loss : 0.018967758864164352\n",
      "Epoch : 990, Loss : 0.0189681313931942\n",
      "Epoch : 991, Loss : 0.018967745825648308\n",
      "Epoch : 992, Loss : 0.018966957926750183\n",
      "Epoch : 993, Loss : 0.01896664686501026\n",
      "Epoch : 994, Loss : 0.018966862931847572\n",
      "Epoch : 995, Loss : 0.01896732486784458\n",
      "Epoch : 996, Loss : 0.01896742545068264\n",
      "Epoch : 997, Loss : 0.01896713115274906\n",
      "Epoch : 998, Loss : 0.018966739997267723\n",
      "Epoch : 999, Loss : 0.0189665574580431\n"
     ]
    }
   ],
   "source": [
    "bce_loss_list = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    sgd_optim.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    #print(outputs.shape, y_train.shape)\n",
    "    bce_loss = criterion(outputs, y_train)\n",
    "\n",
    "    bce_loss.backward()\n",
    "    bce_loss_list.append(bce_loss)\n",
    "\n",
    "    sgd_optim.step()\n",
    "\n",
    "    print(f\"Epoch : {epoch}, Loss : {bce_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\"The movie is awfull\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = vectorizer.transform(sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(sample).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2015, -1.7011]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentiment = model(torch.from_numpy(sample).float())\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = [\"Good tasty and the texture was just great\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = vectorizer.transform(sample2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.1774,   0.0000]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentiment2 = model(torch.from_numpy(sample2).float())\n",
    "sentiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.0140,  0.0438, -0.0215,  ..., -0.0223, -0.0427, -0.0273],\n",
       "                      [ 0.0257,  0.0284, -0.0206,  ..., -0.0341, -0.0254,  0.0125],\n",
       "                      [-0.0207, -0.0313,  0.0252,  ..., -0.0390, -0.0141,  0.0239],\n",
       "                      ...,\n",
       "                      [-0.0138, -0.0421,  0.0385,  ...,  0.0416,  0.0229,  0.0187],\n",
       "                      [-0.0006,  0.0053,  0.0435,  ...,  0.0412, -0.0046, -0.0010],\n",
       "                      [ 0.0065,  0.0186, -0.0217,  ..., -0.0166, -0.0119,  0.0348]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.0447, -0.0213, -0.0443, -0.0424, -0.0018,  0.0274,  0.0059,  0.0445,\n",
       "                       0.0135,  0.0178, -0.0021, -0.0185,  0.0354,  0.0260,  0.0241,  0.0336,\n",
       "                      -0.0451,  0.0270,  0.0330,  0.0310, -0.0261,  0.0157,  0.0026, -0.0302,\n",
       "                      -0.0195,  0.0439, -0.0316, -0.0039,  0.0402, -0.0097,  0.0191, -0.0244,\n",
       "                       0.0250,  0.0357, -0.0012,  0.0199,  0.0128, -0.0423, -0.0357,  0.0331,\n",
       "                       0.0091,  0.0102,  0.0455, -0.0158,  0.0319,  0.0113, -0.0017, -0.0436,\n",
       "                      -0.0047, -0.0375, -0.0297,  0.0235, -0.0405, -0.0242, -0.0158,  0.0141,\n",
       "                      -0.0461,  0.0097,  0.0248, -0.0104,  0.0067, -0.0451,  0.0193, -0.0028,\n",
       "                       0.0108, -0.0361,  0.0390,  0.0177, -0.0325, -0.0253, -0.0419,  0.0228,\n",
       "                      -0.0029,  0.0246,  0.0179,  0.0081, -0.0258, -0.0213, -0.0171,  0.0121,\n",
       "                       0.0115,  0.0288, -0.0402,  0.0200,  0.0064,  0.0230,  0.0273, -0.0330,\n",
       "                       0.0453,  0.0128,  0.0137, -0.0130,  0.0084, -0.0178,  0.0150, -0.0366,\n",
       "                       0.0174, -0.0357, -0.0027, -0.0224,  0.0074, -0.0462,  0.0411, -0.0218,\n",
       "                      -0.0227,  0.0445, -0.0424, -0.0423,  0.0039, -0.0404, -0.0414, -0.0442,\n",
       "                       0.0418, -0.0051,  0.0419, -0.0191, -0.0126, -0.0082, -0.0181, -0.0331,\n",
       "                       0.0189, -0.0265,  0.0312,  0.0106, -0.0361, -0.0238, -0.0086, -0.0239,\n",
       "                      -0.0124,  0.0132, -0.0252, -0.0357, -0.0363, -0.0222,  0.0260,  0.0085,\n",
       "                      -0.0004,  0.0269,  0.0138,  0.0022, -0.0067,  0.0283, -0.0053, -0.0428,\n",
       "                       0.0111, -0.0047, -0.0090,  0.0174,  0.0379, -0.0140,  0.0219,  0.0258,\n",
       "                       0.0041,  0.0060,  0.0412, -0.0319, -0.0420, -0.0111, -0.0005, -0.0192,\n",
       "                       0.0087,  0.0206,  0.0292, -0.0026,  0.0088, -0.0096, -0.0126, -0.0360,\n",
       "                       0.0298, -0.0090,  0.0241,  0.0131,  0.0071,  0.0109, -0.0356,  0.0081,\n",
       "                      -0.0187, -0.0336,  0.0081,  0.0068,  0.0304, -0.0309,  0.0119,  0.0087,\n",
       "                      -0.0240,  0.0448, -0.0352,  0.0327, -0.0312, -0.0157,  0.0272,  0.0137,\n",
       "                      -0.0155,  0.0401,  0.0029, -0.0285,  0.0461, -0.0140,  0.0068, -0.0231,\n",
       "                       0.0195,  0.0303, -0.0268,  0.0246,  0.0147,  0.0036, -0.0444,  0.0407,\n",
       "                       0.0189, -0.0079,  0.0415, -0.0313, -0.0181,  0.0331,  0.0017,  0.0249,\n",
       "                       0.0458, -0.0070, -0.0404, -0.0179,  0.0337,  0.0171,  0.0375, -0.0157,\n",
       "                       0.0223, -0.0405, -0.0007,  0.0106, -0.0146, -0.0417,  0.0190,  0.0355,\n",
       "                      -0.0431, -0.0252, -0.0199,  0.0007,  0.0132, -0.0114, -0.0019, -0.0445,\n",
       "                       0.0421,  0.0146, -0.0049,  0.0170, -0.0062, -0.0460, -0.0047,  0.0111,\n",
       "                       0.0169, -0.0068, -0.0014,  0.0343, -0.0173,  0.0002,  0.0256, -0.0110,\n",
       "                       0.0398, -0.0061,  0.0260, -0.0392,  0.0254,  0.0067, -0.0224, -0.0048,\n",
       "                       0.0401, -0.0048, -0.0403,  0.0185, -0.0179, -0.0317, -0.0343, -0.0144,\n",
       "                      -0.0141,  0.0086, -0.0077, -0.0079,  0.0094, -0.0110, -0.0442, -0.0073,\n",
       "                       0.0283,  0.0246, -0.0260,  0.0346, -0.0332, -0.0205, -0.0454,  0.0375,\n",
       "                       0.0152, -0.0015,  0.0178, -0.0247, -0.0396, -0.0183,  0.0066,  0.0212,\n",
       "                      -0.0408,  0.0178, -0.0264, -0.0172, -0.0217,  0.0200,  0.0086,  0.0436,\n",
       "                      -0.0064,  0.0378, -0.0384, -0.0458, -0.0395, -0.0059,  0.0121, -0.0064,\n",
       "                       0.0398, -0.0275, -0.0317, -0.0101,  0.0316, -0.0022,  0.0454, -0.0148,\n",
       "                       0.0333,  0.0272,  0.0156,  0.0212,  0.0271,  0.0215,  0.0165,  0.0245,\n",
       "                      -0.0281,  0.0274, -0.0301,  0.0022, -0.0367, -0.0300,  0.0051,  0.0279,\n",
       "                      -0.0209,  0.0296,  0.0106,  0.0126, -0.0397,  0.0211, -0.0366,  0.0270,\n",
       "                      -0.0069, -0.0276, -0.0169, -0.0316, -0.0146,  0.0170, -0.0151,  0.0042,\n",
       "                       0.0457,  0.0411,  0.0145, -0.0028,  0.0228,  0.0212, -0.0227, -0.0192,\n",
       "                      -0.0395,  0.0375,  0.0307,  0.0131, -0.0182,  0.0454,  0.0138,  0.0219,\n",
       "                      -0.0417,  0.0022,  0.0048,  0.0301, -0.0268,  0.0148,  0.0110,  0.0169,\n",
       "                      -0.0343, -0.0217, -0.0337,  0.0452, -0.0421,  0.0046,  0.0312,  0.0119,\n",
       "                       0.0030,  0.0024, -0.0202,  0.0434,  0.0151, -0.0362,  0.0269,  0.0049,\n",
       "                      -0.0234,  0.0048, -0.0236, -0.0405,  0.0221,  0.0242,  0.0264, -0.0241,\n",
       "                      -0.0231, -0.0248, -0.0034,  0.0340,  0.0122,  0.0353, -0.0170, -0.0430,\n",
       "                      -0.0036,  0.0051, -0.0241, -0.0017,  0.0282, -0.0264,  0.0129, -0.0391,\n",
       "                      -0.0014, -0.0129, -0.0009,  0.0183,  0.0334,  0.0113, -0.0257, -0.0454,\n",
       "                      -0.0415, -0.0002, -0.0131, -0.0090,  0.0124, -0.0054,  0.0101, -0.0273,\n",
       "                      -0.0429,  0.0057, -0.0089, -0.0105,  0.0424,  0.0338,  0.0368, -0.0317,\n",
       "                      -0.0078,  0.0213, -0.0411,  0.0337,  0.0394,  0.0378, -0.0135, -0.0041,\n",
       "                      -0.0310,  0.0389, -0.0331, -0.0236, -0.0239,  0.0281,  0.0462, -0.0114,\n",
       "                       0.0188,  0.0391, -0.0367, -0.0309, -0.0303,  0.0020,  0.0141,  0.0019,\n",
       "                       0.0268, -0.0345,  0.0328, -0.0147,  0.0127, -0.0138,  0.0044, -0.0119,\n",
       "                       0.0450,  0.0407,  0.0175, -0.0110, -0.0136,  0.0026, -0.0360,  0.0381,\n",
       "                      -0.0260,  0.0176,  0.0077,  0.0026,  0.0203,  0.0247, -0.0117, -0.0059,\n",
       "                      -0.0192,  0.0016,  0.0298, -0.0203, -0.0131, -0.0210,  0.0318, -0.0135,\n",
       "                       0.0340, -0.0098,  0.0418, -0.0237])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.0307,  0.0053,  0.0444,  ...,  0.0246,  0.0125,  0.0123],\n",
       "                      [-0.0283, -0.0333, -0.0413,  ..., -0.0108, -0.0029,  0.0264],\n",
       "                      [ 0.0122,  0.0026, -0.0133,  ..., -0.0152,  0.0112,  0.0101],\n",
       "                      ...,\n",
       "                      [-0.0153,  0.0393, -0.0241,  ..., -0.0431, -0.0299, -0.0131],\n",
       "                      [ 0.0133,  0.0294,  0.0074,  ...,  0.0304, -0.0250, -0.0416],\n",
       "                      [-0.0304,  0.0247,  0.0150,  ..., -0.0236,  0.0383, -0.0323]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 3.4273e-02, -5.3707e-03,  4.0339e-02, -2.1472e-02,  1.7118e-03,\n",
       "                       1.9835e-02,  4.2876e-02,  2.5599e-02,  3.5573e-02,  3.5563e-03,\n",
       "                       2.9415e-02,  1.2497e-02,  3.9763e-02, -3.7197e-02, -1.0849e-02,\n",
       "                       1.4894e-02, -1.4284e-02,  1.7397e-02, -4.0039e-02,  2.3648e-02,\n",
       "                       1.8963e-02,  4.3076e-03, -1.0927e-02,  5.3958e-03, -1.5149e-02,\n",
       "                       5.3555e-03, -3.8393e-02,  1.4554e-02, -3.5398e-02,  2.2639e-02,\n",
       "                       1.3423e-02,  3.3406e-02,  1.0424e-02, -1.6389e-02,  3.5123e-02,\n",
       "                       7.2009e-03,  2.6453e-02,  2.5715e-03, -3.9822e-04,  1.0496e-03,\n",
       "                       4.3202e-02,  1.1722e-02,  1.2410e-02, -1.5623e-02,  1.6634e-02,\n",
       "                       2.2539e-02, -2.3066e-02, -2.9174e-02, -3.8438e-02,  2.8413e-02,\n",
       "                      -4.0773e-02,  2.0099e-02, -1.6972e-02,  4.2828e-02,  1.2469e-02,\n",
       "                       2.6471e-02,  2.4811e-03, -2.2670e-02, -2.5398e-03, -2.3030e-02,\n",
       "                      -3.6866e-02, -1.3423e-02, -4.0222e-02,  1.1179e-02,  2.5165e-02,\n",
       "                      -2.2733e-02,  2.1162e-02,  1.6152e-02, -1.1230e-02,  1.5233e-02,\n",
       "                      -2.0845e-02,  8.8574e-03,  3.3661e-02, -6.5448e-03,  4.0582e-03,\n",
       "                       2.4099e-02, -2.8050e-03,  1.0014e-02,  2.1061e-02,  4.4811e-03,\n",
       "                      -3.5699e-02, -3.3225e-02,  3.1642e-02,  4.5188e-03, -1.7219e-02,\n",
       "                       1.3910e-03,  3.4725e-02, -3.9364e-02, -2.2291e-02, -2.5489e-02,\n",
       "                       8.8983e-03, -7.4967e-03,  9.8185e-03,  3.2868e-02, -3.7568e-02,\n",
       "                      -1.1149e-03,  1.6921e-02, -9.9530e-03,  1.6346e-02, -8.1397e-03,\n",
       "                      -3.4533e-02, -2.3544e-02,  2.6797e-02,  1.7333e-02, -3.9502e-02,\n",
       "                       1.7399e-02, -3.2343e-02, -2.1736e-02,  4.1983e-02, -1.6627e-02,\n",
       "                       3.5929e-02, -2.3641e-02,  3.1106e-02,  2.1522e-02, -1.7874e-02,\n",
       "                      -2.6238e-02, -1.9938e-02, -2.4469e-03, -1.1824e-02, -1.9155e-02,\n",
       "                       4.3648e-02, -1.5112e-02,  2.4860e-02,  1.0466e-02,  1.2723e-02,\n",
       "                       3.6261e-02, -1.3020e-03, -1.0937e-02,  2.0496e-02, -3.8480e-02,\n",
       "                       1.4815e-02,  2.2559e-02,  1.3623e-02, -1.6156e-02, -2.8038e-02,\n",
       "                      -1.2420e-02,  1.4797e-02, -2.3463e-02, -3.1973e-02,  3.1168e-02,\n",
       "                      -2.2890e-02,  1.7558e-02,  2.7794e-02,  3.4339e-02, -2.5831e-02,\n",
       "                      -1.2854e-02, -3.8845e-04, -4.2639e-02,  3.7481e-02,  6.1780e-03,\n",
       "                      -4.3204e-02,  2.7375e-02, -1.6556e-02,  4.3056e-02, -2.3405e-02,\n",
       "                       4.0430e-03,  4.3449e-02,  2.8485e-02,  2.2443e-02,  3.2851e-02,\n",
       "                      -2.1016e-02,  2.4309e-02, -1.8150e-02,  1.6644e-02, -8.4244e-03,\n",
       "                       2.9242e-02, -1.9173e-02,  2.4364e-02, -3.9725e-02,  1.1305e-02,\n",
       "                       2.9797e-02,  4.4138e-03,  5.2003e-03, -2.2567e-02, -2.8564e-02,\n",
       "                       1.2794e-02, -3.1673e-02,  1.9555e-02, -2.3959e-03, -1.1274e-02,\n",
       "                       1.8545e-02, -3.7425e-02, -4.7979e-03,  4.1376e-02, -1.6015e-02,\n",
       "                       1.5051e-02,  2.8288e-02,  4.1514e-02,  2.4969e-02,  3.0544e-02,\n",
       "                       2.1396e-03, -2.5318e-02, -2.5432e-02, -1.8496e-02,  3.8878e-02,\n",
       "                       2.3445e-02, -3.3438e-02, -5.0778e-03, -1.1317e-02, -4.4155e-03,\n",
       "                      -3.5446e-02, -1.8893e-02,  3.0442e-02, -2.8030e-02,  1.2092e-02,\n",
       "                       2.6639e-02,  1.8993e-02, -4.1023e-02, -3.8062e-02,  4.2259e-02,\n",
       "                       1.6758e-02,  1.9642e-02,  4.0694e-02, -4.2712e-02,  2.6409e-02,\n",
       "                      -1.8514e-02,  3.9177e-03, -1.8608e-02, -1.1827e-02,  2.1063e-02,\n",
       "                       3.4515e-02, -1.7029e-02,  3.3356e-02, -3.4825e-02, -3.8008e-02,\n",
       "                      -7.5299e-03,  3.1961e-02, -3.3839e-02,  6.9621e-04,  3.3270e-02,\n",
       "                       1.2375e-02,  1.4356e-02, -1.0714e-02, -1.9272e-02,  4.9008e-03,\n",
       "                      -2.8115e-02,  3.0598e-02,  1.3013e-03, -3.7231e-02,  2.8842e-03,\n",
       "                       1.3285e-02,  2.0765e-03,  3.7123e-02,  6.1296e-03,  7.6657e-03,\n",
       "                      -3.0243e-02,  2.7186e-02,  2.8599e-03, -1.0451e-02, -2.9718e-02,\n",
       "                       2.4319e-02, -3.7947e-02, -3.4555e-02,  1.1720e-03,  1.2683e-04,\n",
       "                      -2.4944e-02, -8.9065e-03,  1.6213e-02, -1.4682e-02,  5.9030e-04,\n",
       "                      -1.4793e-03, -4.4066e-03,  2.0831e-02, -2.6088e-03, -1.0146e-02,\n",
       "                      -4.4539e-02, -1.4309e-02,  8.2322e-04,  1.7977e-02,  2.7342e-02,\n",
       "                       1.9873e-02,  7.9080e-03,  6.7522e-03, -4.4946e-03, -1.9256e-02,\n",
       "                      -1.0999e-02, -8.6731e-03, -1.7986e-02,  5.0590e-03,  4.1584e-02,\n",
       "                      -3.3595e-02,  2.9390e-02,  4.3966e-02, -4.2828e-02,  4.4616e-02,\n",
       "                       2.3872e-02, -1.1317e-02,  3.1624e-02,  2.3487e-02,  3.9504e-02,\n",
       "                      -4.1695e-03, -6.7216e-03,  3.9452e-03, -2.2413e-03, -2.4252e-02,\n",
       "                      -2.5254e-02, -5.1146e-03, -4.3921e-02, -4.0774e-02, -4.2857e-02,\n",
       "                       2.8011e-02,  5.4613e-03,  2.1542e-02, -4.3243e-02,  1.0537e-02,\n",
       "                      -3.5309e-02,  1.9119e-02,  1.3619e-02,  1.6255e-02,  3.1986e-02,\n",
       "                      -2.7126e-02,  2.7616e-02, -3.9425e-02,  3.6880e-04,  3.2053e-02,\n",
       "                       3.9294e-02,  3.0900e-02, -1.1658e-02, -4.4196e-02, -4.4000e-03,\n",
       "                      -9.4722e-03,  8.5724e-03, -2.6098e-02,  2.7892e-03,  6.4919e-03,\n",
       "                      -5.0427e-03,  1.9672e-02,  1.0161e-02,  1.8453e-02, -3.1209e-02,\n",
       "                      -3.3762e-02,  3.6278e-02,  2.4991e-02,  1.6952e-02, -3.4992e-02,\n",
       "                       4.9080e-03, -4.2647e-02, -2.3202e-02,  2.1449e-02,  1.9564e-02,\n",
       "                      -3.7761e-02, -2.1748e-02,  3.1690e-02, -3.0303e-02,  3.9529e-02,\n",
       "                      -3.2594e-02,  3.5651e-02, -2.4972e-02,  7.0378e-05, -1.3497e-03,\n",
       "                       3.8018e-02,  1.9570e-02, -2.1456e-02,  1.8743e-02,  4.2290e-02,\n",
       "                       4.2230e-02, -7.1035e-03, -6.9383e-04,  4.2162e-02,  2.6359e-02,\n",
       "                       7.4248e-03,  4.1421e-02,  1.8352e-02, -1.3094e-02,  3.6575e-02,\n",
       "                      -3.8853e-02, -4.2088e-02, -1.4186e-03, -3.6379e-02, -6.5102e-03,\n",
       "                       3.0443e-02,  3.3706e-02,  2.5910e-02, -2.1039e-02,  5.3535e-03,\n",
       "                       1.9382e-02, -3.8682e-02,  2.6046e-02, -2.8436e-03,  1.0755e-02,\n",
       "                      -3.2121e-02,  4.1641e-02, -1.8999e-02,  1.8155e-02, -2.8686e-02,\n",
       "                      -2.4194e-02,  4.0992e-02, -2.6973e-02,  2.0712e-03,  2.8279e-02,\n",
       "                       2.8449e-02, -2.6878e-02,  7.0315e-03, -2.3738e-02,  9.9522e-03,\n",
       "                      -2.9868e-02, -1.4170e-02, -1.4894e-02, -3.3687e-02,  1.3418e-02,\n",
       "                       2.3525e-02,  8.2430e-03, -5.7166e-03, -3.9219e-02, -4.2762e-02,\n",
       "                      -4.4059e-02, -2.9092e-02,  1.9265e-02, -7.0922e-03,  1.2626e-02,\n",
       "                      -1.7245e-02,  2.6404e-02, -2.9872e-02,  2.6932e-02,  1.1570e-02,\n",
       "                      -2.2846e-03, -3.8296e-02, -1.3758e-02, -2.1763e-02, -2.6225e-02,\n",
       "                      -3.0896e-04,  1.8954e-02,  9.5408e-03, -5.1482e-03,  3.1222e-02,\n",
       "                       1.3217e-02, -3.0937e-02,  9.6086e-03, -1.3565e-02,  4.3107e-02,\n",
       "                       2.2845e-02, -1.0686e-02, -1.8761e-02,  3.7952e-02,  4.3368e-02,\n",
       "                      -6.3013e-03,  1.2495e-02, -4.3278e-02, -5.4800e-03,  1.1003e-02,\n",
       "                      -6.5975e-03, -1.9681e-03,  3.8950e-02,  9.5073e-03,  1.4216e-02,\n",
       "                       2.5080e-03, -4.1391e-02, -1.6527e-02,  1.1063e-02,  1.6632e-02,\n",
       "                      -1.7025e-02,  5.5060e-03,  3.9740e-03, -3.7710e-02,  1.5869e-02,\n",
       "                      -1.0006e-02,  5.9126e-04, -2.9470e-02, -2.4845e-02,  2.0447e-02,\n",
       "                       2.5858e-02,  3.8351e-02, -3.6677e-02, -4.4653e-03,  4.3525e-02,\n",
       "                       2.2699e-02,  1.4529e-02,  4.3452e-02,  1.0955e-02, -2.0298e-02,\n",
       "                       2.5226e-02,  2.5119e-02,  1.6094e-02, -1.3655e-02, -2.1856e-02,\n",
       "                       4.3762e-02, -3.5816e-02,  1.3392e-02, -2.3822e-02,  1.0084e-03,\n",
       "                       4.1098e-02, -3.0607e-02, -6.6140e-03,  1.9098e-02, -2.5219e-02,\n",
       "                       2.6528e-02,  4.3276e-02,  2.8687e-02, -3.6422e-02, -2.9934e-02,\n",
       "                       1.7070e-02, -2.6809e-02,  1.3905e-02, -4.3740e-02,  1.8582e-03,\n",
       "                       2.6358e-02,  1.6597e-02, -1.5854e-02, -4.2760e-02, -3.3884e-02])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[ 3.1891e-02,  3.9511e-02,  1.0271e-02, -3.7113e-02,  2.2404e-02,\n",
       "                        3.0970e-02,  1.8835e-03, -3.8793e-02,  3.3884e-02, -4.4691e-02,\n",
       "                       -4.6087e-03,  2.6998e-02,  3.0717e-02,  6.0040e-03,  1.1411e-02,\n",
       "                        3.2217e-02,  4.3932e-02,  3.9596e-02, -4.0244e-02, -2.8895e-02,\n",
       "                       -1.0847e-02, -3.5516e-02,  4.1766e-02,  3.3344e-02, -1.5326e-02,\n",
       "                        2.0654e-02, -3.0396e-02, -3.2938e-03,  3.2980e-02,  2.9122e-02,\n",
       "                        4.2312e-02, -1.4675e-02,  5.2660e-03,  3.1194e-02, -6.2608e-03,\n",
       "                        4.2920e-02, -4.1876e-02, -1.1086e-02, -2.1713e-02,  2.3523e-03,\n",
       "                       -8.3316e-03,  4.6239e-03,  1.9052e-02,  8.0853e-03, -3.6976e-02,\n",
       "                       -3.8280e-02, -7.2521e-03, -1.1326e-02, -3.5618e-02, -3.9021e-02,\n",
       "                       -3.7695e-02,  8.0884e-04,  2.8893e-02,  3.8093e-03, -4.2849e-02,\n",
       "                       -3.8176e-02,  4.4721e-02, -2.3328e-02,  4.0365e-02, -1.6706e-02,\n",
       "                        6.1896e-03, -3.4396e-02, -4.1295e-02,  1.5393e-03, -2.7061e-02,\n",
       "                       -2.5464e-02,  3.3056e-02, -1.4436e-02,  4.0792e-02, -2.2162e-02,\n",
       "                       -2.3413e-02, -4.3889e-02,  2.5874e-02, -2.1148e-02, -1.0306e-02,\n",
       "                        1.6638e-02,  2.2363e-02, -1.5817e-02,  1.9728e-02,  4.0153e-03,\n",
       "                        1.4970e-02, -4.1362e-02,  3.8418e-02,  1.2608e-03,  9.0163e-03,\n",
       "                       -4.8268e-03,  2.6803e-02,  3.6699e-02,  6.6030e-03,  4.0019e-02,\n",
       "                       -4.4029e-02,  3.2728e-02,  4.3954e-02,  3.4405e-02, -6.2498e-03,\n",
       "                       -3.7526e-02, -2.9578e-02, -2.4261e-02, -2.7064e-02,  3.6588e-02,\n",
       "                        2.3447e-02, -3.1727e-02, -4.0933e-02,  2.5608e-02, -6.5155e-03,\n",
       "                       -2.4452e-03,  1.1724e-02,  3.4629e-02,  1.9669e-02, -3.6388e-02,\n",
       "                       -1.7793e-02,  2.6912e-02, -5.9439e-03,  2.7696e-04, -4.1147e-02,\n",
       "                       -3.8124e-02,  6.2890e-03,  6.9041e-03,  2.3751e-02, -3.0435e-02,\n",
       "                        1.7307e-02,  7.9959e-03, -8.4985e-03, -2.0341e-03,  3.3763e-02,\n",
       "                        2.8594e-02,  3.8771e-02,  3.9281e-02,  1.6711e-02, -1.0754e-02,\n",
       "                       -4.4606e-02,  3.7273e-03,  3.9615e-02,  2.7744e-02, -3.6787e-02,\n",
       "                       -4.0389e-02, -2.1136e-02, -2.4696e-02, -3.8539e-02, -2.1413e-05,\n",
       "                        1.1773e-02,  1.7953e-02,  3.0632e-02,  2.7754e-02,  1.8612e-02,\n",
       "                       -4.1875e-02, -3.2374e-02, -2.4533e-02, -4.1835e-03, -4.1776e-02,\n",
       "                       -4.3267e-02, -1.9930e-02, -1.9853e-02, -2.0697e-02,  9.0559e-03,\n",
       "                        4.2501e-02,  7.1657e-04, -7.0171e-03,  2.9762e-02,  1.4976e-02,\n",
       "                        3.2567e-03,  1.6757e-02, -3.0807e-02,  3.5714e-03, -1.3023e-03,\n",
       "                       -3.4667e-02, -2.8043e-02,  2.7618e-02, -2.5205e-02, -3.3520e-03,\n",
       "                        3.8238e-02, -4.0279e-02,  3.5647e-02,  2.4310e-02,  3.5380e-02,\n",
       "                        4.1325e-02,  6.0070e-03, -2.6687e-02, -1.1439e-02,  9.4295e-03,\n",
       "                       -1.3429e-02,  3.2426e-02,  1.8201e-02,  2.3100e-02,  2.6620e-02,\n",
       "                       -7.9776e-03, -8.9915e-03,  6.3225e-03,  3.0876e-02, -7.0725e-03,\n",
       "                        7.9671e-03, -2.8631e-02,  8.0495e-03,  3.1074e-02,  4.2538e-02,\n",
       "                       -2.0405e-02, -1.9642e-02, -2.6629e-02,  1.9504e-02, -2.9482e-02,\n",
       "                        2.2598e-03, -2.3562e-02, -1.9573e-02,  1.3243e-02,  3.9068e-02,\n",
       "                       -3.3436e-02, -4.4346e-02,  3.8425e-02,  1.3631e-02, -5.5074e-03,\n",
       "                       -5.7544e-03,  2.9893e-02, -1.3072e-02,  2.7462e-02,  3.8932e-02,\n",
       "                        3.1342e-02,  3.0771e-03,  2.4711e-02, -1.0930e-02, -7.0440e-03,\n",
       "                        3.3875e-02, -3.6034e-03, -3.3005e-02, -3.2775e-03,  2.1461e-02,\n",
       "                       -1.4856e-02,  3.7706e-02, -2.9784e-02,  3.1489e-02,  6.5852e-04,\n",
       "                       -2.3912e-03,  1.8577e-02, -5.8294e-04,  1.7673e-02,  9.5539e-03,\n",
       "                        2.8803e-02,  3.0587e-02, -2.5317e-02, -4.3885e-02, -1.5572e-02,\n",
       "                       -9.6084e-03,  1.6802e-02, -1.5569e-02,  9.5877e-04,  5.5282e-03,\n",
       "                       -1.9552e-03, -2.2147e-02, -3.0300e-02,  2.1558e-02,  3.9712e-02,\n",
       "                        2.7361e-02, -3.3881e-02,  1.5290e-03,  2.5682e-02,  4.2328e-03,\n",
       "                        3.2691e-02,  1.8007e-02,  1.1896e-02,  3.0714e-02,  2.4761e-02,\n",
       "                       -1.3747e-02, -1.5908e-03,  1.3914e-02, -3.3756e-02, -2.2866e-02,\n",
       "                        2.1730e-02,  1.8249e-02, -3.8879e-02, -1.9738e-02,  3.2707e-02,\n",
       "                       -2.7067e-02, -2.3184e-02,  1.5696e-02, -2.7470e-02, -1.2913e-02,\n",
       "                       -3.2916e-02, -3.6410e-02, -1.4304e-02,  4.5472e-03, -2.1372e-02,\n",
       "                        2.4240e-03,  2.3239e-02,  7.1595e-03,  6.1782e-03,  3.2500e-02,\n",
       "                        3.0192e-02, -2.7623e-02,  3.0505e-02, -3.4765e-02, -1.1027e-02,\n",
       "                        4.2737e-02,  3.2713e-02,  4.4228e-02, -5.9427e-03, -2.0879e-02,\n",
       "                       -3.5447e-02, -5.2199e-03, -3.6663e-02, -4.1928e-02, -4.0769e-02,\n",
       "                       -2.4342e-02, -3.3386e-02,  1.2288e-02,  8.4276e-03,  3.3182e-02,\n",
       "                        3.0487e-03,  3.7003e-02, -4.3414e-02, -6.4653e-03,  2.6146e-02,\n",
       "                       -1.1689e-02, -2.8317e-02, -3.1582e-02, -1.8272e-02, -4.2813e-02,\n",
       "                       -1.1520e-02, -3.9103e-02,  1.4406e-02, -1.9059e-02, -4.4280e-02,\n",
       "                       -3.5221e-02,  2.9670e-02,  1.7503e-02, -3.9719e-02, -9.7126e-03,\n",
       "                       -4.4710e-02,  3.1715e-02,  2.5082e-02,  1.4094e-02, -7.4503e-03,\n",
       "                       -1.4538e-02, -3.9021e-02,  3.7186e-02,  5.4950e-04, -1.5562e-02,\n",
       "                        2.8906e-02,  3.2831e-02,  2.2478e-02,  3.7714e-02, -3.3888e-02,\n",
       "                       -2.3632e-02, -3.2962e-02,  2.1043e-02,  2.0928e-02,  3.9489e-02,\n",
       "                       -2.0687e-03,  1.3590e-02,  1.2287e-02,  1.8808e-02,  3.4766e-02,\n",
       "                       -4.5630e-03, -6.8016e-03,  3.5663e-02,  1.6535e-02,  4.0887e-02,\n",
       "                       -9.5343e-03, -1.3363e-02,  1.5109e-02,  5.2281e-03, -3.1251e-02,\n",
       "                        2.3605e-02,  2.9939e-02,  1.9096e-02, -3.2623e-02,  2.8627e-02,\n",
       "                       -2.5592e-02,  2.1506e-02,  6.7457e-03,  1.1273e-02, -2.5579e-02,\n",
       "                        4.2118e-02, -3.8160e-02, -3.3141e-02, -1.1545e-02, -2.3251e-02,\n",
       "                        1.1775e-02,  1.4342e-02,  3.0334e-05,  2.1827e-02,  7.6498e-03,\n",
       "                       -1.1896e-02, -2.5847e-02, -2.7186e-02, -1.6624e-02, -1.0145e-02,\n",
       "                        1.9393e-02,  1.3497e-02, -4.3015e-02,  3.7956e-02, -4.0880e-02,\n",
       "                        2.4695e-02, -2.3154e-02,  4.1894e-02, -2.3571e-02, -3.9601e-02,\n",
       "                        3.2226e-02, -1.8423e-02, -2.7544e-02,  5.9687e-03, -2.4288e-02,\n",
       "                        2.5493e-02, -8.7604e-04, -1.1133e-02,  3.2722e-02, -3.4445e-02,\n",
       "                        4.9856e-03,  3.1318e-02, -1.6789e-02,  1.2287e-02,  1.5394e-02,\n",
       "                       -3.0583e-02, -8.3453e-03, -2.4282e-02,  4.0296e-02,  3.4521e-02,\n",
       "                       -2.8309e-02,  4.2627e-03,  3.2842e-02,  1.2581e-02,  1.2944e-03,\n",
       "                       -1.5019e-02,  3.2996e-02,  9.9709e-03,  8.8171e-03, -3.4050e-02,\n",
       "                       -2.5230e-03, -3.2892e-02,  3.4751e-02,  1.6155e-02,  2.8748e-02,\n",
       "                        4.4489e-02, -3.3195e-02, -6.9722e-03,  2.0606e-02, -2.9857e-02,\n",
       "                        9.7435e-03, -1.1873e-02,  1.7117e-02, -9.5105e-03,  1.0744e-02,\n",
       "                        1.8414e-02,  3.8051e-02,  2.4595e-02,  2.3092e-02, -2.7792e-03,\n",
       "                       -2.8271e-02, -2.4850e-02, -1.6604e-02,  3.4231e-02, -1.0061e-02,\n",
       "                        2.1491e-03,  4.3621e-02, -3.6724e-02,  3.7037e-02, -1.4883e-02,\n",
       "                        1.6491e-02, -3.1391e-02, -7.7774e-03, -3.5195e-02, -2.0460e-02,\n",
       "                        2.0336e-02,  3.6987e-03,  4.3899e-02, -2.1134e-02, -2.1464e-02,\n",
       "                        2.9388e-02,  1.4831e-02,  2.9103e-02,  3.6742e-02, -5.7516e-04,\n",
       "                        4.4378e-03, -1.1794e-02,  4.1691e-02,  3.5263e-02, -1.1003e-02,\n",
       "                        3.0930e-02,  3.9772e-02,  2.5281e-02, -1.2768e-02,  1.7663e-02,\n",
       "                       -1.8435e-02, -2.2132e-02,  1.4307e-02,  1.8857e-02, -1.7162e-02,\n",
       "                        1.7338e-02,  2.2058e-02,  1.2466e-02, -4.2859e-03, -2.4081e-02,\n",
       "                       -2.9622e-03,  4.5892e-03,  1.6192e-02,  3.1031e-02,  1.1850e-02,\n",
       "                       -4.2681e-02, -4.8108e-03,  1.9658e-02, -3.5364e-02, -7.2006e-03],\n",
       "                      [ 1.2977e-02,  2.0827e-02, -1.9942e-02, -2.0392e-02, -1.3141e-02,\n",
       "                       -3.7466e-02,  2.3677e-02,  1.2237e-02,  5.6181e-03,  4.8915e-03,\n",
       "                        3.4596e-02,  1.2682e-02, -2.2241e-04,  3.8176e-02,  7.3922e-03,\n",
       "                       -1.4365e-02, -4.2809e-03, -3.2014e-02, -4.0285e-02, -3.2727e-02,\n",
       "                        3.9501e-03,  1.7955e-02, -8.9046e-03, -3.5780e-02,  1.3011e-02,\n",
       "                        2.7880e-02, -3.6464e-02, -2.5991e-02, -8.2655e-03,  8.2836e-03,\n",
       "                        2.9728e-02, -1.3797e-02, -1.8749e-02, -2.7817e-02, -2.5366e-02,\n",
       "                        3.0001e-03,  4.0300e-02, -8.6036e-03, -3.6362e-02,  2.9953e-02,\n",
       "                       -1.6517e-02,  2.0090e-02, -2.2336e-02, -1.9858e-02,  2.4918e-02,\n",
       "                        1.2596e-02,  1.0768e-02,  2.5885e-02,  9.8055e-03,  1.4255e-02,\n",
       "                        1.8389e-02, -2.0984e-02, -9.4301e-03,  2.6953e-02, -2.9152e-02,\n",
       "                       -2.7930e-02,  1.3167e-02,  3.9043e-02, -1.8214e-02,  8.7164e-03,\n",
       "                        1.5370e-02, -1.3895e-02,  2.6896e-03, -4.1414e-02, -3.6593e-02,\n",
       "                       -2.2652e-02,  1.3231e-03, -4.1862e-02,  2.3146e-02, -4.4667e-02,\n",
       "                        9.6508e-04,  8.3117e-03,  8.5935e-03, -2.1854e-02, -3.9279e-02,\n",
       "                       -4.2854e-02, -3.7200e-03, -3.1236e-02,  3.7081e-02,  1.2708e-02,\n",
       "                        3.7974e-02,  3.6129e-02,  2.5647e-02, -2.9933e-02, -2.3014e-02,\n",
       "                       -1.9276e-02, -3.2393e-02,  2.8564e-02, -3.0110e-03, -3.1892e-02,\n",
       "                        1.6091e-02, -3.1013e-02,  3.1209e-02,  4.2592e-02,  1.3586e-02,\n",
       "                       -1.3235e-02,  2.0399e-03,  4.3346e-02,  3.2218e-02,  3.9733e-02,\n",
       "                        2.9266e-02, -1.2100e-02, -6.5837e-03, -2.3365e-02,  1.8854e-02,\n",
       "                        1.4817e-03,  2.1836e-02,  1.3467e-02, -3.1629e-03,  9.4943e-03,\n",
       "                        4.2209e-02,  1.1076e-02, -3.5851e-02, -1.7010e-02,  8.0803e-03,\n",
       "                       -2.1187e-02, -2.9722e-02, -1.5371e-02, -4.5643e-04,  1.1846e-02,\n",
       "                        4.0625e-02, -8.3906e-03, -1.6992e-02, -7.2192e-04, -6.5843e-03,\n",
       "                        4.0055e-02, -4.3192e-02, -6.5860e-03,  2.6606e-02,  1.3440e-02,\n",
       "                       -3.1015e-02, -1.8213e-02,  8.3888e-03,  1.1363e-03, -1.7956e-02,\n",
       "                       -2.0520e-02, -2.0227e-02, -2.4710e-02, -2.6307e-02, -2.1628e-02,\n",
       "                       -1.3928e-02, -2.0846e-02,  2.8559e-02,  2.7232e-02, -3.6700e-02,\n",
       "                       -9.7804e-03, -2.2080e-02,  2.0607e-02, -3.6899e-02, -9.2834e-03,\n",
       "                        2.6562e-02, -3.7037e-02, -2.3300e-03, -6.1562e-03,  1.3834e-02,\n",
       "                        1.3204e-02,  3.8926e-02,  2.5457e-02, -2.1827e-03, -2.8374e-02,\n",
       "                        1.0821e-03,  3.8862e-02,  3.1113e-03,  1.2815e-02,  7.9825e-03,\n",
       "                       -1.1775e-02,  4.2358e-02, -1.0096e-02, -3.7003e-02,  1.3745e-02,\n",
       "                       -1.0629e-02,  4.0030e-02, -3.4828e-02, -2.6716e-02,  1.4986e-02,\n",
       "                       -1.8075e-02,  3.2955e-02,  8.1370e-03,  2.5537e-03,  2.8177e-02,\n",
       "                        3.1289e-02,  3.7452e-03, -1.1477e-03, -2.5585e-03, -1.7283e-03,\n",
       "                       -1.6151e-02,  4.1134e-02,  3.1180e-02, -2.7861e-02,  4.3099e-02,\n",
       "                        2.8867e-02,  7.7167e-03,  7.7403e-03,  3.5008e-02, -4.2729e-03,\n",
       "                       -4.1137e-02,  4.0593e-02, -3.1105e-02,  4.0649e-02,  2.5854e-02,\n",
       "                        3.7004e-02,  1.7845e-02, -2.8676e-02, -2.4962e-02,  1.8655e-02,\n",
       "                        1.6126e-02,  3.3025e-03, -3.0430e-02, -3.3548e-02, -3.8177e-03,\n",
       "                        2.9512e-02,  3.3128e-02, -3.3697e-02,  3.5426e-02,  3.5170e-02,\n",
       "                        3.1843e-02,  1.6288e-02, -1.6733e-02, -2.7592e-02,  5.3480e-03,\n",
       "                        2.2577e-03, -1.0249e-02, -5.1331e-03, -1.7750e-02, -5.6748e-03,\n",
       "                        3.1018e-02,  1.5783e-02, -3.2416e-02,  3.1328e-02,  3.4003e-02,\n",
       "                        2.0655e-02,  4.1049e-02,  2.9180e-02, -2.0098e-02, -6.3412e-03,\n",
       "                       -3.9080e-03,  3.2878e-02, -1.4772e-02,  3.5729e-02, -1.2818e-02,\n",
       "                       -3.7155e-02, -7.3948e-03, -2.0281e-02, -3.0967e-02, -1.7638e-02,\n",
       "                       -4.2127e-02, -1.3611e-02, -3.6522e-02, -4.2670e-02, -4.3146e-02,\n",
       "                        4.2403e-02, -4.3602e-02,  6.6334e-03, -4.8653e-03, -2.0178e-02,\n",
       "                        2.6425e-02,  4.4697e-02, -6.1490e-03, -3.1462e-02,  1.1145e-02,\n",
       "                        3.5011e-02,  7.1205e-04,  1.6222e-02, -7.1075e-03, -4.6597e-03,\n",
       "                       -2.9208e-02, -2.8533e-02, -3.2793e-02,  1.3216e-02, -2.1627e-02,\n",
       "                       -2.0488e-02, -1.6245e-02,  7.1216e-03,  1.6331e-02,  1.5026e-03,\n",
       "                       -2.1920e-02,  1.3475e-02, -3.9218e-02, -4.1171e-02, -4.2962e-03,\n",
       "                        6.6648e-03, -2.3676e-03,  1.7826e-02,  3.2497e-02, -4.1182e-02,\n",
       "                        3.1051e-02, -2.2242e-02, -1.2904e-02,  2.6602e-03, -3.7526e-02,\n",
       "                        2.1923e-02, -3.3883e-02,  2.3895e-02,  2.3289e-02, -2.7943e-02,\n",
       "                       -2.2686e-02,  2.2649e-02, -3.0435e-02,  3.0707e-02,  2.8152e-02,\n",
       "                       -2.0425e-02, -3.6547e-02,  1.6888e-02, -1.2581e-02,  3.8189e-02,\n",
       "                        4.9741e-03, -3.3342e-02, -3.7959e-02, -8.7089e-03,  6.1193e-03,\n",
       "                       -7.1118e-03, -2.9056e-02, -2.8359e-03, -3.3120e-02,  1.9882e-02,\n",
       "                        3.4649e-02, -1.1218e-02, -3.1829e-02, -1.5432e-03,  2.4574e-02,\n",
       "                       -2.0375e-02, -2.2323e-02, -3.5965e-03,  1.7286e-02,  3.7462e-02,\n",
       "                       -1.7257e-02,  4.3531e-02,  3.0371e-02, -1.2018e-04,  2.7221e-02,\n",
       "                        2.4233e-03, -3.6126e-02, -5.0062e-03,  1.6686e-02, -3.2874e-02,\n",
       "                       -3.3928e-02, -2.4848e-02,  1.7030e-02,  2.1420e-02,  2.5098e-02,\n",
       "                       -3.5789e-02,  5.5196e-03,  8.6041e-03,  2.3570e-02,  1.7237e-02,\n",
       "                        1.6575e-02,  4.3161e-02,  3.3727e-02,  3.9445e-02,  1.6202e-02,\n",
       "                       -8.4306e-03, -4.2201e-02, -4.4180e-02, -3.7827e-02,  9.4371e-04,\n",
       "                        2.5643e-02, -3.8334e-02,  2.2290e-02, -2.9375e-02,  2.0205e-02,\n",
       "                        2.2138e-02,  3.6305e-02,  3.4689e-02,  4.1522e-02,  1.2716e-02,\n",
       "                       -1.5616e-02,  2.8787e-02,  4.5730e-04,  1.6185e-02,  2.6685e-02,\n",
       "                        1.7649e-02,  1.4758e-03,  1.4740e-02,  4.2032e-03,  2.9439e-02,\n",
       "                       -6.4641e-03,  4.7878e-03,  8.8607e-03,  2.6541e-02,  3.0240e-02,\n",
       "                        3.2934e-02, -3.6070e-02, -1.9528e-02, -4.0945e-02, -3.9783e-02,\n",
       "                        1.8443e-02,  3.8968e-02, -4.2111e-02,  3.4376e-02,  7.4893e-03,\n",
       "                       -5.8551e-03,  1.4466e-02,  9.9755e-03, -3.1866e-02, -2.3111e-02,\n",
       "                       -3.4447e-02, -4.4729e-03, -9.3898e-03, -1.0773e-02,  3.1608e-02,\n",
       "                        5.4660e-03, -3.5157e-02, -2.5769e-02, -4.2058e-02,  2.2916e-02,\n",
       "                       -3.7743e-02, -2.2501e-02, -1.9578e-02, -1.4632e-03,  3.5382e-02,\n",
       "                        1.6017e-02, -3.8425e-02, -3.8393e-02,  9.0115e-03,  1.7227e-03,\n",
       "                        3.6731e-02, -9.4053e-03, -3.8987e-02, -1.9603e-03,  3.6606e-02,\n",
       "                       -1.6100e-02,  2.9189e-02, -2.4967e-02,  3.8220e-02, -1.0070e-02,\n",
       "                        1.1688e-02,  4.4650e-02,  6.3067e-03, -2.3207e-02, -3.2326e-02,\n",
       "                       -3.1625e-02,  2.9498e-02, -4.3574e-02,  4.2283e-02,  2.5660e-02,\n",
       "                        2.2739e-02,  2.5961e-02, -2.3619e-02, -3.0919e-02, -3.1863e-02,\n",
       "                        1.4976e-03,  9.6736e-04, -1.8790e-03,  8.4780e-03, -1.5521e-02,\n",
       "                       -3.2128e-02,  5.3093e-03,  2.6918e-02, -7.5158e-04, -2.3256e-02,\n",
       "                        2.2370e-02,  1.8397e-02, -5.3126e-03,  4.9145e-03,  2.8616e-02,\n",
       "                        3.6852e-02, -2.1423e-02, -3.8096e-02,  2.5069e-02, -4.4096e-02,\n",
       "                       -1.9044e-02, -1.3090e-02,  1.7831e-02,  2.0803e-02,  1.2618e-02,\n",
       "                       -4.1994e-02,  5.3634e-03, -4.2266e-02,  4.1741e-02, -3.2524e-02,\n",
       "                       -3.5313e-02,  2.0112e-02,  3.6710e-02,  4.2442e-02,  7.4338e-03,\n",
       "                       -3.9183e-02, -8.0490e-03, -2.8864e-03,  2.6984e-02,  3.8682e-03,\n",
       "                        2.5709e-02,  2.1454e-02,  1.4458e-02,  3.4996e-02, -2.7091e-02,\n",
       "                        3.0618e-02, -2.4992e-03,  2.4031e-02, -2.2586e-02, -3.9517e-02,\n",
       "                        3.7335e-02, -2.4732e-02, -2.1598e-02,  1.6087e-02,  2.7904e-02,\n",
       "                       -3.4977e-02,  7.5868e-03,  2.5935e-02,  2.7192e-02,  6.9470e-03]])),\n",
       "             ('layer3.bias', tensor([ 0.0379, -0.0140]))])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
